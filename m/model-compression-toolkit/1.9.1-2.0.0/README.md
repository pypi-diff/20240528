# Comparing `tmp/model_compression_toolkit-1.9.1.tar.gz` & `tmp/model_compression_toolkit-2.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "model_compression_toolkit-1.9.1.tar", last modified: Tue Aug  1 12:41:37 2023, max compression
+gzip compressed data, was "model_compression_toolkit-2.0.0.tar", last modified: Tue Apr  2 13:41:43 2024, max compression
```

## Comparing `model_compression_toolkit-1.9.1.tar` & `model_compression_toolkit-2.0.0.tar`

### file list

```diff
@@ -1,529 +1,593 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/
--rw-r--r--   0 runner    (1001) docker     (123)    10174 2023-08-01 12:40:49.000000 model_compression_toolkit-1.9.1/LICENSE.md
--rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    10054 2023-08-01 12:40:49.000000 model_compression_toolkit-1.9.1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.051166 model_compression_toolkit-1.9.1/model_compression_toolkit/
--rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3949 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.055166 model_compression_toolkit-1.9.1/model_compression_toolkit/core/
--rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2975 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.055166 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.055166 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/back2framework/base_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/base_substitutions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.059167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2576 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/base_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     6864 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/histogram_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     3888 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/mean_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     5207 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     7929 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/statistics_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/defaultdict.py
--rw-r--r--   0 runner    (1001) docker     (123)    22391 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/framework_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6424 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.059167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/fusion/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/fusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/fusion/layer_fusing.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.059167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      773 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    28860 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/base_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    20579 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/base_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     3733 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/edge.py
--rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/functional_node.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4732 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/graph_matchers.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5128 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/graph_searches.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.059167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3880 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     2612 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
--rw-r--r--   0 runner    (1001) docker     (123)     2470 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/cut.py
--rw-r--r--   0 runner    (1001) docker     (123)    17045 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
--rw-r--r--   0 runner    (1001) docker     (123)     3961 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
--rw-r--r--   0 runner    (1001) docker     (123)     7175 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     9265 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.059167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/
--rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3091 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/base_graph_filter.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2210 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/base_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3706 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/edge_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1773 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/function.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2745 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/node_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1111 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/walk_matcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/memory_computation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.063167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6704 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.063167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4297 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
--rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
--rw-r--r--   0 runner    (1001) docker     (123)     7046 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     1602 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)    19323 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
--rw-r--r--   0 runner    (1001) docker     (123)     8041 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     6822 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)    34622 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.063167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/search_methods/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15453 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
--rw-r--r--   0 runner    (1001) docker     (123)    25266 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6326 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
--rw-r--r--   0 runner    (1001) docker     (123)     1324 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/model_builder_mode.py
--rw-r--r--   0 runner    (1001) docker     (123)     5005 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/model_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/model_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.063167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17994 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/actions.py
--rw-r--r--   0 runner    (1001) docker     (123)     1756 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/edit_network.py
--rw-r--r--   0 runner    (1001) docker     (123)     3149 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/node_filters.py
--rw-r--r--   0 runner    (1001) docker     (123)     1769 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/node_prior_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.067167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/core_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1482 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/debug_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4352 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
--rw-r--r--   0 runner    (1001) docker     (123)    16271 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3462 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_analyzer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7247 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2352 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4090 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.067167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/
--rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16505 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     7270 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     8484 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4558 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)     4315 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)    41685 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
--rw-r--r--   0 runner    (1001) docker     (123)     5090 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)     9689 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7879 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2939 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
--rw-r--r--   0 runner    (1001) docker     (123)     3614 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantize_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.067167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2350 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2774 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    14210 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     5486 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)    10683 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     7450 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/similarity_analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.067167 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3434 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5956 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    10228 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5584 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.071168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
--rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     5892 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     9962 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     9093 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     2250 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
--rw-r--r--   0 runner    (1001) docker     (123)     4866 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    10978 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)    26835 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2625 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     3400 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     4228 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/user_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.071168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6371 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5955 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/nn_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    20099 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4233 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.071168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.071168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      808 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2226 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2444 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    16452 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15316 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (123)     2481 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4999 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.071168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3940 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3655 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5598 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
--rw-r--r--   0 runner    (1001) docker     (123)     5925 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    26778 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (123)     2387 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
--rw-r--r--   0 runner    (1001) docker     (123)     3176 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5542 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7714 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)    10781 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1814 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (123)    27197 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_model_validation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_node_prior_info.py
--rw-r--r--   0 runner    (1001) docker     (123)     8600 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1900 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6151 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     4507 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2837 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     6854 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10902 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     7621 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2627 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    11418 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/connectivity_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7906 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     2760 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2107 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     2408 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     5951 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/node_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8109 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3060 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/tf_tensor_numpy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.075168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/visualization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.079168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.079168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3419 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4976 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    18214 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (123)    16443 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.079168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5773 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2472 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4224 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.079168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.079168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3038 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2162 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     4804 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     5797 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    38353 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1953 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     5601 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (123)     4148 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2899 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     9838 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     1375 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (123)     8482 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11345 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1678 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py
--rw-r--r--   0 runner    (1001) docker     (123)    25698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/pytorch_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3250 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6464 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4452 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12113 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/graph_builders.py
--rw-r--r--   0 runner    (1001) docker     (123)     1789 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/node_holders.py
--rw-r--r--   0 runner    (1001) docker     (123)     5801 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2959 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    23009 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/core/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/
--rw-r--r--   0 runner    (1001) docker     (123)     1189 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2017 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)      966 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
--rw-r--r--   0 runner    (1001) docker     (123)    11100 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3043 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     8048 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     6016 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)      967 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
--rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     5866 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)     1187 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.083168 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4287 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8318 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2077 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3543 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3921 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2086 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3449 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/
--rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9508 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_config.py
--rw-r--r--   0 runner    (1001) docker     (123)      611 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     1266 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_framework_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    15167 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_training.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    17319 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     4564 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/graph_info.py
--rw-r--r--   0 runner    (1001) docker     (123)    14791 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      963 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4751 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5055 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4136 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3962 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
--rw-r--r--   0 runner    (1001) docker     (123)    12163 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10381 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.087169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8370 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2719 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1268 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)    14668 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     3955 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/graph_info.py
--rw-r--r--   0 runner    (1001) docker     (123)    12768 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      968 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4171 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3893 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3996 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2089 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4132 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
--rw-r--r--   0 runner    (1001) docker     (123)    12351 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9103 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8782 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)     5534 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18116 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/keras_quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)    17664 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/pytorch_quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)     4863 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/
--rw-r--r--   0 runner    (1001) docker     (123)      930 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9868 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8580 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/pytorch/quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/
--rw-r--r--   0 runner    (1001) docker     (123)     1091 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/common/
--rw-r--r--   0 runner    (1001) docker     (123)      829 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/common/qat_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.091169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16051 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      856 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2138 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2123 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5635 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13436 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)    10708 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12461 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      859 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2213 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5491 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5004 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9629 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)     8651 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      920 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     1723 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/immutable.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/
--rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2353 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     8538 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3108 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
--rw-r--r--   0 runner    (1001) docker     (123)     9206 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1392 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/
--rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8759 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2046 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
--rw-r--r--   0 runner    (1001) docker     (123)     4019 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     6040 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     8713 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
--rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.095169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4625 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6310 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3845 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6846 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4321 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4618 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8332 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4631 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5372 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8332 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5477 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4727 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8145 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5373 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3526 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.099169 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8140 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5379 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4721 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2090 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6204 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     2942 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2085 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8048 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     6131 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     5002 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/
--rw-r--r--   0 runner    (1001) docker     (123)     1104 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7564 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)      875 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     6351 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3558 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4791 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4212 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3991 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
--rw-r--r--   0 runner    (1001) docker     (123)     3480 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/load_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1797 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.103170 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3083 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-01 12:41:37.051166 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-08-01 12:41:36.000000 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    32878 2023-08-01 12:41:36.000000 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-01 12:41:36.000000 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      147 2023-08-01 12:41:36.000000 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       26 2023-08-01 12:41:36.000000 model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-08-01 12:41:37.107170 model_compression_toolkit-1.9.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     2040 2023-08-01 12:40:50.000000 model_compression_toolkit-1.9.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/
+-rw-r--r--   0 runner    (1001) docker     (127)    10174 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/LICENSE.md
+-rw-r--r--   0 runner    (1001) docker     (127)    19994 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    17988 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.518365 model_compression_toolkit-2.0.0/model_compression_toolkit/
+-rw-r--r--   0 runner    (1001) docker     (127)     1557 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3648 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1982 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2975 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/
+-rw-r--r--   0 runner    (1001) docker     (127)     1447 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2023 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1666 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/base_substitutions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2591 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/base_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6882 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/histogram_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3414 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/mean_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7930 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/statistics_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21152 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6337 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/fusion/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/fusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5548 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/fusion/layer_fusing.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.526365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      773 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38178 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/base_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28492 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/base_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3784 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/edge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3173 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/functional_node.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4744 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/graph_matchers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5128 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/graph_searches.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.526365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3800 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17045 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3961 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7175 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9803 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.526365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9721 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/hessian_info_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1325 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/hessian_info_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/trace_hessian_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3321 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/hessian/trace_hessian_request.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.526365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/
+-rwxr-xr-x   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3091 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2210 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/base_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3706 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/edge_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1773 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/function.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2745 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/node_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1111 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/walk_matcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1205 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/memory_computation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.530365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7536 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      882 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5177 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/configurable_quantizer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2812 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4573 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7550 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37578 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.530365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4745 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7922 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4217 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1682 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21473 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.530365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/search_methods/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16592 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28519 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2846 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7901 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1324 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/model_builder_mode.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8352 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/model_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/model_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.530365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/
+-rw-r--r--   0 runner    (1001) docker     (127)     1307 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19594 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/actions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1748 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/edit_network.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3149 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/node_filters.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2832 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.534365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3892 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/channels_grouping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7928 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/greedy_mask_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.534365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1988 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/base_importance_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1999 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/importance_metric_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14027 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/lfh_importance_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.534365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/mask/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/mask/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/mask/per_channel_mask.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5958 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/mask/per_simd_group_mask.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19523 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/memory_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3323 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/prune_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7516 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/pruner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3681 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/pruning_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6734 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/pruning_framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3781 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/pruning_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5721 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/pruning_section.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.534365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4900 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2257 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/core_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1482 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/debug_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7136 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26737 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6699 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3847 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.538365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/
+-rw-r--r--   0 runner    (1001) docker     (127)     1486 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18165 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7410 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1772 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8538 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4558 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41524 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3882 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9743 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7933 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2854 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantize_node.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.538365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2761 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14245 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5521 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11503 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8515 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/similarity_analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.538365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4669 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5641 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10145 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5519 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.542365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1390 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13392 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10028 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12367 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2406 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4767 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10966 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29865 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2625 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3477 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4737 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1648 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/user_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.542365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6371 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5921 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21951 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4115 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10100 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/graph_prep_runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.542365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.542365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      808 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2233 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2444 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4517 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16001 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15567 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2481 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1192 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/custom_layer_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4961 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.542365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.546365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5140 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8158 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3168 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2478 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5794 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/dwconv_to_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5940 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8143 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4112 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/matmul_substitution.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3872 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2387 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3181 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5542 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7941 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11149 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1462 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.546365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9656 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/activation_trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3913 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10525 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/weights_trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29399 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_model_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3879 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5137 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6729 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12710 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/pruning/pruning_keras_implementation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1628 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6274 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4475 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2484 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11303 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7906 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2760 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2107 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2408 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9666 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/node_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8109 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4851 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/resource_utilization_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3060 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2491 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.550365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/visualization/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/visualization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.554365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.554365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      813 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2279 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3419 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1848 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15060 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.554365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5773 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1640 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3456 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4175 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.554365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8292 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2822 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2162 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3919 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3935 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_batch_norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3799 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5815 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38459 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1953 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5667 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4919 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2902 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3303 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10668 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1375 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1616 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8147 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/activation_trace_hessian_calculator_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3425 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/trace_hessian_calculator_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6974 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/weights_trace_hessian_calculator_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4871 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6337 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14648 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pruning/pruning_pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4330 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pytorch_device_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27082 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3250 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4420 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12626 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/node_holders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6015 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4983 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/resource_utilization_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3261 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2880 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6153 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/quantization_prep_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11836 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/core/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.558365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/
+-rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1018 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6415 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4564 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/data_generation_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/enums.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6046 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/model_info_exctractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19572 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/optimization_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7623 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21539 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/keras_data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8610 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/model_info_exctractors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1983 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4119 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5325 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4913 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21146 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1179 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6617 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9690 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1968 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2597 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4719 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5015 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2587 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19085 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/optimization_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20937 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2277 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/defaultdict.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/
+-rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.562365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2017 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/quantization_format.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11481 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3514 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8048 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5750 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1976 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/mctq_keras_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4021 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5137 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2897 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6351 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)     1187 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2297 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5130 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.566365 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4915 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9348 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3464 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)     1228 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5288 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)      611 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1266 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3039 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17705 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_training.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1248 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6241 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18438 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4639 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14057 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4782 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5055 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4016 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12159 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10377 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.570365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8356 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2719 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1268 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15808 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3967 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12495 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      968 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4137 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3893 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4566 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1908 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4132 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12347 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9099 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8768 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5924 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4567 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8612 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/keras/pruning_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9398 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/pytorch/pruning_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      904 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9057 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7583 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/
+-rw-r--r--   0 runner    (1001) docker     (127)     1143 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.574365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      829 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3394 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/common/qat_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17026 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      996 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2065 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/lsq/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/lsq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12022 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/lsq/symmetric_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11223 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/lsq/uniform_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2543 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5872 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13517 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10789 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13374 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1003 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/lsq/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/lsq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10712 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/lsq/symmetric_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10355 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/lsq/uniform_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5721 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5335 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9657 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8778 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.578365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1519 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1827 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/immutable.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/
+-rw-r--r--   0 runner    (1001) docker     (127)     1724 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3924 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14248 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3108 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1392 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)     1513 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8776 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4019 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6720 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9837 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3337 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2959 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10883 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6484 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5731 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/
+-rw-r--r--   0 runner    (1001) docker     (127)      721 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10616 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6493 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5739 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/
+-rw-r--r--   0 runner    (1001) docker     (127)      721 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10369 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6505 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.582365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1510 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2090 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8106 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3814 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1505 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2085 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9864 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6830 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/
+-rw-r--r--   0 runner    (1001) docker     (127)     1224 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7706 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      875 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6983 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3428 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1505 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4791 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4148 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4360 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3656 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/load_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5468 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1797 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.586365 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3047 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 13:41:43.522365 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    19994 2024-04-02 13:41:43.000000 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    36440 2024-04-02 13:41:43.000000 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-02 13:41:43.000000 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      137 2024-04-02 13:41:43.000000 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       26 2024-04-02 13:41:43.000000 model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      133 2024-04-02 13:41:43.590365 model_compression_toolkit-2.0.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     2040 2024-04-02 13:41:14.000000 model_compression_toolkit-2.0.0/setup.py
```

### Comparing `model_compression_toolkit-1.9.1/LICENSE.md` & `model_compression_toolkit-2.0.0/LICENSE.md`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/constants.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/constants.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,39 +14,41 @@
 # ==============================================================================
 
 import importlib
 
 # Supported frameworks in MCT:
 TENSORFLOW = 'tensorflow'
 PYTORCH = 'pytorch'
-FOUND_TF = importlib.util.find_spec(TENSORFLOW) is not None and importlib.util.find_spec(
-    "tensorflow_model_optimization") is not None
+FOUND_TF = importlib.util.find_spec(TENSORFLOW) is not None
 FOUND_TORCH = importlib.util.find_spec("torch") is not None
 FOUND_ONNX = importlib.util.find_spec("onnx") is not None
 FOUND_ONNXRUNTIME = importlib.util.find_spec("onnxruntime") is not None
+FOUND_SONY_CUSTOM_LAYERS = importlib.util.find_spec('sony_custom_layers') is not None
 
 WEIGHTS_SIGNED = True
 # Minimal threshold to use for quantization ranges:
 MIN_THRESHOLD = (2 ** -16)
 EPS = 1e-8
-MULTIPLIER_N_BITS = 8
+LUT_VALUES_BITWIDTH = 8
+FP32_BYTES_PER_PARAMETER = 4.
 
 # Quantization attributes:
 OUTPUT_SCALE = 'output_scale'
 THRESHOLD = 'threshold'
 SIGNED = 'is_signed'
-CLUSTER_CENTERS = 'cluster_centers'
+LUT_VALUES = 'lut_values'
 SCALE_PER_CHANNEL = 'scale_per_channel'
 RANGE_MIN = 'range_min'
 RANGE_MAX = 'range_max'
 
 # BaseNode attributes
 REUSE = 'reuse'
 REUSE_GROUP = 'reuse_group'
 LAST_AXIS = -1
+AXIS = 'axis'
 
 # Data types:
 DATA_TYPE = 'dtype'
 FLOAT_32 = 'float32'
 
 # Number of Tensorboard cosine-similarity plots to add:
 NUM_SAMPLES_DISTANCE_TENSORBOARD = 20
@@ -87,40 +89,34 @@
 DEFAULT_DEC_FACTOR = (1.02, 0.98)
 DEFAULT_TOL = 1e-11
 BOTTOM_FACTOR = 0.7
 UPPER_FACTOR = 1.2
 DEC_RANGE_BOTTOM = 0.97
 DEC_RANGE_UPPER = 1.03
 
-# KPI computation parameters
+# Resource utilization computation parameters
 BITS_TO_BYTES = 8.0
 
 # Default threshold for Softmax layer
 SOFTMAX_THRESHOLD = 1
 
 # Substitutions node names
 VIRTUAL_WEIGHTS_SUFFIX = '_v_weights'
 VIRTUAL_ACTIVATION_SUFFIX = '_v_activation'
 VIRTUAL_ACTIVATION_WEIGHTS_NODE_PREFIX = 'virtual'
 
-# Quantization config candidate initialization
-ACTIVATION_QUANTIZATION_CFG = 'activation_quantization_cfg'
-WEIGHTS_QUANTIZATION_CFG = 'weights_quantization_cfg'
-QC = 'qc'
-OP_CFG = 'op_cfg'
-ACTIVATION_QUANTIZATION_FN = 'activation_quantization_fn'
-WEIGHTS_QUANTIZATION_FN = 'weights_quantization_fn'
-ACTIVATION_QUANT_PARAMS_FN = 'activation_quantization_params_fn'
-WEIGHTS_QUANT_PARAMS_FN = 'weights_quantization_params_fn'
-WEIGHTS_CHANNELS_AXIS = 'weights_channels_axis'
-
 # Memory graph constants
 DUMMY_NODE = 'dummy_node'
 DUMMY_TENSOR = 'dummy_tensor'
 
+# Hessian scores constants
+MIN_HESSIAN_ITER = 10
+HESSIAN_COMP_TOLERANCE = 1e-3
+
 
-# TF Input node base name
-INPUT_BASE_NAME = 'base_input'
+# Hessian configuration default constants
+HESSIAN_OUTPUT_ALPHA = 0.3
+HESSIAN_NUM_ITERATIONS = 50
+HESSIAN_EPS = 1e-6
 
-# Jacobian-weights constants
-MIN_JACOBIANS_ITER = 10
-JACOBIANS_COMP_TOLERANCE = 1e-3
+# Pruning constants
+PRUNING_NUM_SCORE_APPROXIMATIONS = 32
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,20 +9,20 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.data_loader import FolderImageLoader
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
 from model_compression_toolkit.core.common import network_editors as network_editor
 from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
 from model_compression_toolkit.core.common.quantization import quantization_config
 from model_compression_toolkit.core.common.mixed_precision import mixed_precision_quantization_config
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, QuantizationErrorMethod, DEFAULTCONFIG
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.keras.kpi_data_facade import keras_kpi_data, keras_kpi_data_experimental
-from model_compression_toolkit.core.pytorch.kpi_data_facade import pytorch_kpi_data, pytorch_kpi_data_experimental
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
+from model_compression_toolkit.core.keras.resource_utilization_data_facade import keras_resource_utilization_data
+from model_compression_toolkit.core.pytorch.resource_utilization_data_facade import pytorch_resource_utilization_data
+from model_compression_toolkit.core.common.mixed_precision.distance_weighting import MpDistanceWeighting
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/analyzer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/analyzer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/back2framework/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/back2framework/base_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/base_substitutions.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/base_substitutions.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/base_collector.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/base_collector.py`

 * *Files 4% similar despite different names*

```diff
@@ -62,9 +62,9 @@
     def validate_data_correctness(self):
         """
         Verify the collector's statistics were manipulated in a granularity they were collected by.
         If the statistics are invalid, an exception is raised.
         """
 
         if not self.is_legal:
-            Logger.exception(f'{self.__class__.__name__} was manipulated per-channel,'
-                             'but collected per-tensor. Data is invalid.')  # pragma: no cover
+            Logger.critical('The data is invalid.'
+                            f'{self.__class__.__name__} was collected per-tensor but received data manipulated per-channel.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/histogram_collector.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/histogram_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -69,15 +69,15 @@
             bins_stack = np.vstack([hist[1] for hist in self.__histogram_per_iteration])
 
             # The combined histogram will be computed between new min/max (which is the min/max of all histograms).
             # The bin width of the merged histogram is the minimal bin width among all histograms (to lose as less
             # information as possible during the merge).
             merged_histogram_min = np.min(bins_stack)
             merged_histogram_max = np.max(bins_stack)
-            merged_bin_width = np.min(bins_stack[:, 1] - bins_stack[:, 0])
+            merged_bin_width = (merged_histogram_max - merged_histogram_min) / self.__n_bins
             merged_histogram_bins = np.arange(merged_histogram_min, merged_histogram_max+merged_bin_width, merged_bin_width)
 
             merged_histogram_counts = None
             for histogram in self.__histogram_per_iteration:  # Iterate all collected histograms and merge them
                 if merged_histogram_counts is None:  # First histogram to consider
                     merged_histogram_counts = interpolate_histogram(merged_histogram_bins, histogram[1], histogram[0])
                 else:  # Merge rest of histograms into existing final histogram
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/mean_collector.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/mean_collector.py`

 * *Files 22% similar despite different names*

```diff
@@ -23,82 +23,75 @@
 class MeanCollector(BaseCollector):
     """
         Class to collect observed per channel mean values of tensors that goes through it (passed to update).
         The mean is calculated using a exponential moving average with bias correction.
     """
 
     def __init__(self,
-                 axis: int,
-                 beta: float = 0.99):
+                 axis: int):
         """
         Instantiate a per channel mean collector using a exponential moving average with bias correction.
 
         Args:
             axis: Compute the mean with regard to this axis.
-            beta: Parameter for mean smoothing by EMA.
         """
         super().__init__()
         self.axis = axis
-        self.__state_internal = np.array([0.0])  # mean per-channel
-        self.__state_internal_correction = None
-        self.beta = beta
-        self.i = 0.0
+        self.current_mean = 0
+        self.current_sum = 0
+        self.i = 0
 
     def scale(self, scale_factor: np.ndarray):
         """
         Scale all statistics in collector by some factor.
         Since mean was collected per-channel, it can be scaled either by a single factor or a factor
         per-channel.
         The scaling is done using the corrected mean.
 
         Args:
             scale_factor: Factor to scale all collector's statistics by.
 
         """
 
-        self.__state_internal_correction *= scale_factor
+        self.current_mean *= scale_factor
 
     def shift(self, shift_value: np.ndarray):
         """
         Shift all statistics in collector by some value.
         Since mean was collected per-channel, it can be shifted either by a single value or a
         shifting value per-channel.
         The shifting is done using the corrected mean.
 
         Args:
             shift_value: Value to shift all collector's statistics by.
 
         """
 
-        self.__state_internal_correction += shift_value
+        self.current_mean += shift_value
 
     @property
     def state(self):
         """
         The mean is kept internal and corrected when accessed from outside the collector.
 
         Returns: Mean of the collector after bias correction.
         """
         self.validate_data_correctness()
-        return self.__state_internal_correction
+        return self.current_mean
 
     def update(self,
                x: np.ndarray):
         """
         Update the mean using a new tensor x to consider.
 
         Args:
             x: Tensor that goes through the mean collector and needs to be considered in the mean computation.
         """
-
         self.i += 1  # Update the iteration index
         axis = (len(x.shape) - 1) if self.axis == LAST_AXIS else self.axis
         n = x.shape[axis]
         transpose_index = [axis, *[i for i in range(len(x.shape)) if i != axis]]
-        mu = np.mean(np.reshape(np.transpose(x, transpose_index), [n, -1]), axis=-1)  # compute mean per channel
-        update_state = self.beta * self.__state_internal + (1 - self.beta) * mu
-        self.__state_internal = update_state
-
-        # Since we use a weighted mean, initial values can be distorted,
-        # so use bias correction to compensate it.
-        bias_correction = 1 - (self.beta ** self.i)
-        self.__state_internal_correction = self.__state_internal / bias_correction
+        mu = np.mean(np.reshape(np.transpose(x, transpose_index), [n, -1]), axis=-1) # mean per channel for a batch
+        self.current_sum += mu # sum of all batches
+        self.current_mean = self.current_sum / self.i # mean of all batches
+
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/collectors/statistics_collector.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/collectors/statistics_collector.py`

 * *Files 0% similar despite different names*

```diff
@@ -98,15 +98,15 @@
         """
 
         return self.mc.state
 
     def get_min_max_values(self) -> Tuple[float, float]:
         """
         Get min/max from collector.
-        When its accessed from outside the tensor, the scale and shift come into consideration.
+        When it's accessed from outside the tensor, the scale and shift come into consideration.
 
         Returns: Min/max from collector.
         """
 
         min_value = self.mpcc.min
         max_value = self.mpcc.max
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/defaultdict.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/defaultdict.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,36 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from typing import Callable, Dict, Any
+from typing import Dict, Any
+from copy import deepcopy
 
 
-class DefaultDict(object):
+class DefaultDict:
     """
     Default dictionary. It wraps a dictionary given at initialization and return its
     values when requested. If the requested key is not presented at initial dictionary,
-    it returns the returned value a default factory (that is passed at initialization) generates.
+    it returns the returned value a default value (that is passed at initialization) generates.
     """
 
     def __init__(self,
-                 known_dict: Dict[Any, Any],
-                 default_factory: Callable = None):
+                 known_dict: Dict[Any, Any] = None,
+                 default_value: Any = None):
         """
 
         Args:
-            known_dict: Dictionary to wrap.
-            default_factory: Callable to get default values when requested key is not in known_dict.
+            known_dict: Dictionary to wrap. If None is provided, initializes an empty dictionary.
+            default_value: default value when requested key is not in known_dict.
         """
 
-        self.default_factory = default_factory
-        self.known_dict = known_dict
+        self.default_value = default_value
+        self.known_dict = known_dict if known_dict is not None else {}
 
     def get(self, key: Any) -> Any:
         """
         Get the value of the inner dictionary by the given key, If key is not in dictionary,
         it uses the default_factory to return a default value.
 
         Args:
@@ -47,19 +48,17 @@
 
         Returns:
             Value of the inner dictionary by the given key, or a default value if not exist.
             If default_factory was not passed at initialization, it returns None.
         """
 
         if key in self.known_dict:
-            return self.known_dict.get(key)
+            return self.known_dict[key]
         else:
-            if self.default_factory is not None:
-                return self.default_factory()
-            return None
+            return deepcopy(self.default_value)
 
     def keys(self):
         """
         Get keys of known_dict
         Returns: keys of known_dict
         """
         return self.known_dict.keys()
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/framework_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/framework_implementation.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,20 +13,22 @@
 # limitations under the License.
 # ==============================================================================
 from abc import ABC, abstractmethod
 from typing import Callable, Any, List, Tuple, Dict
 
 import numpy as np
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianInfoService
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.user_info import UserInformation
 
@@ -43,14 +45,33 @@
 
         Returns: Module of the framework constants.
 
         """
         raise NotImplemented(f'{self.__class__.__name__} did not supply a constants module.')  # pragma: no cover
 
     @abstractmethod
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get framework trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculator to use for the trace hessian approximation computation for this request.
+        """
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_trace_hessian_calculator method.')  # pragma: no cover
+
+    @abstractmethod
     def to_numpy(self, tensor: Any) -> np.ndarray:
         """
         Convert framework's tensor to a Numpy array.
         Args:
             tensor: Framework's tensor.
 
         Returns:
@@ -90,29 +111,29 @@
 
     @abstractmethod
     def model_builder(self,
                       graph: Graph,
                       mode: ModelBuilderMode,
                       append2output: List[Any],
                       fw_info: FrameworkInfo,
-                      return_float_outputs: bool = False) -> Tuple[Any, UserInformation]:
+                      return_float_outputs: bool = False) -> Tuple:
         """
         Build a framework model from a graph.
         The mode determines how the model should be build. append2output is a list of Nodes
         to set as the model outputs.
 
         Args:
             graph: Graph to build the model from it.
             mode: Mode for how to build the model.
             append2output: List of Nodes to set as the model's outputs.
             fw_info: FrameworkInfo object with information about the specific framework's model
             return_float_outputs (bool): whether to return outputs before or after quantization nodes (default)
 
         Returns:
-            A tuple of the model that was built and an UserInformation object.
+            A tuple with the model and additional relevant supporting objects.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s model_builder method.')  # pragma: no cover
 
     @abstractmethod
     def run_model_inference(self,
                             model: Any,
@@ -146,30 +167,14 @@
         Returns:
             Graph after SNC.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s apply_shift_negative_correction method.')  # pragma: no cover
 
     @abstractmethod
-    def attach_sc_to_node(self, node: BaseNode, fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel).
-
-        Returns:
-            Statistics collector for the node.
-        """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s attach_sc_to_node method.')  # pragma: no cover
-
-    @abstractmethod
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
         Args:
@@ -211,14 +216,22 @@
         """
         Returns: linear collapsing substitution
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_linear_collapsing_substitution method.')  # pragma: no cover
 
     @abstractmethod
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: conv2d add const collapsing substitution
+        """
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_op2d_add_const_collapsing_substitution method.')  # pragma: no cover
+
+    @abstractmethod
     def get_substitutions_statistics_correction(self, quant_config: QuantizationConfig) -> \
             List[common.BaseSubstitution]:
         """
         Returns A list of the framework substitutions used for statistics correction.
 
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
@@ -284,32 +297,33 @@
         Returns:
             A list of the framework substitutions used after we apply second moment statistics.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_substitutions_after_second_moment_correction '
                              f'method.')  # pragma: no cover
 
-
     @abstractmethod
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
+                                  hessian_info_service: HessianInfoService = None,
                                   disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
 
         Returns:
             A function that computes the metric.
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_sensitivity_evaluator method.')  # pragma: no cover
@@ -341,115 +355,49 @@
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s count_node_for_mixed_precision_interest_points method.')  # pragma: no cover
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_node_distance_fn method.')  # pragma: no cover
 
-    @abstractmethod
-    def get_model_layers_names(self,
-                               model: Any) -> List[str]:
-        """
-        Returns a list of the given model's layers names.
-
-        Args:
-            model: A model.
-
-        Returns: List of layers' names.
-
-        """
-
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s get_model_layers_names method.')  # pragma: no cover
-
-    @abstractmethod
-    def get_model_layer_by_name(self,
-                                model: Any,
-                                layer_name: str) -> Any:
-        """
-        Returns a model's layer by its name.
-
-        Args:
-            model: A model to retrieve a layer from.
-            layer_name: The requested layer's name.
-
-        Returns: A layer object.
-
-        """
-
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s get_model_layer_by_name method.')  # pragma: no cover
-
-    @abstractmethod
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, np.ndarray],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a framework specific model gradient calculation function, which computes the jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list.
-            alpha: A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
-        """
-
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s model_grad method.')  # pragma: no cover
 
     @abstractmethod
-    def is_node_compatible_for_metric_outputs(self,
-                                                 node: BaseNode) -> bool:
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> bool:
         """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s is_node_compatible_for_metric_outputs method.')  # pragma: no cover
+                             f'framework\'s is_output_node_compatible_for_hessian_score_computation method.')  # pragma: no cover
 
     @abstractmethod
     def get_node_mac_operations(self,
                                 node: BaseNode,
                                 fw_info: FrameworkInfo) -> float:
         """
         Gets the MAC operation count for a given operation.
@@ -497,7 +445,23 @@
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s sensitivity_eval_inference method.')  # pragma: no cover
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of framework compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_inferable_quantizers method.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/framework_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/framework_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,16 +16,15 @@
 
 from collections.abc import Callable
 from enum import Enum
 from typing import Dict, Any, List
 
 
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 class ChannelAxis(Enum):
     """
 
     Index of output channels axis:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/fusion/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/fusion/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/fusion/layer_fusing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/fusion/layer_fusing.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,41 +27,43 @@
         fusing_patterns: supported fusings
         node: node to decide if it can be a part of fusion
         idx: index of layer in the fusion
     Returns:
         fusing_patterns after filtering non-relevant fusions
     """
     valid_fusing_patterns = []
-    for i,fusing_pattern in enumerate(fusing_patterns):
+    for i, fusing_pattern in enumerate(fusing_patterns):
         if idx < len(fusing_pattern):
-            if (type(fusing_pattern[idx]) == LayerFilterParams and node.is_match_filter_params(fusing_pattern[idx])) or fusing_pattern[idx] == node.type:
+            if (type(fusing_pattern[idx]) == LayerFilterParams and node.is_match_filter_params(fusing_pattern[idx])) or \
+                    node.is_match_type(fusing_pattern[idx]):
                 valid_fusing_patterns.append(fusing_pattern)
 
     # Return only valid patterns for this node
     return valid_fusing_patterns
 
 
 def is_valid_fusion(fusing_patterns: List[List[Any]], nodes: List[BaseNode]) -> bool:
     """
     Check if the fusion is valid: exist in fusing_patterns
     Args:
-        fusing_patterns: supported fusings
+        fusing_patterns: supported fusing patterns
         nodes: nodes which are participating in fusion
     Returns:
         whether the fusion in valid
     """
     fusion_depth = len(nodes)
     if fusion_depth <= 1:
         return False
     for fusing_pattern in fusing_patterns:
         if fusion_depth != len(fusing_pattern):
             continue
         counter = 0
-        for i,layer in enumerate(fusing_pattern):
-            if (type(layer) == LayerFilterParams and nodes[i].is_match_filter_params(layer)) or layer == nodes[i].type:
+        for i, layer in enumerate(fusing_pattern):
+            if (type(layer) == LayerFilterParams and nodes[i].is_match_filter_params(layer)) or \
+                    nodes[i].is_match_type(layer):
                 counter += 1
         if counter == fusion_depth:
             return True
     return False
 
 
 def disable_nodes_activation_quantization(nodes: List[BaseNode]):
@@ -103,15 +105,15 @@
     nodes = fused_graph.get_topo_sorted_nodes()
     fused_nodes = []  # nodes that are participating in fusing
     for node in nodes:
         # Skip if already in fusing
         if node in fused_nodes:
             continue
         # Start fusing search
-        fusing_nodes = [] # nodes that are candidates for participating in fusing
+        fusing_nodes = []  # nodes that are candidates for participating in fusing
         patterns = copy.deepcopy(fusing_patterns)
         next_nodes = [node]
         for i in range(max_layers_fusing):
             patterns = filter_fusing_patterns(patterns, next_nodes[0], i)
             if len(patterns) == 0: # Give up if no more fusion pattern
                 break
             fusing_nodes.append(next_nodes[0])
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/base_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/base_graph.py`

 * *Files 23% similar despite different names*

```diff
@@ -25,17 +25,19 @@
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX, EDGE_SOURCE_INDEX
 from model_compression_toolkit.core.common.graph.edge import Edge, convert_to_edge
 from model_compression_toolkit.core.common.graph.graph_searches import GraphSearches
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.collectors.statistics_collector import scale_statistics, shift_statistics
+from model_compression_toolkit.core.common.pruning.pruning_section import PruningSection
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities, LayerFilterParams
 
 OutTensor = namedtuple('OutTensor', 'node node_out_index')
 
 
 class Graph(nx.MultiDiGraph, GraphSearches):
     """
     Base graph representing a model to be optimized.
@@ -87,14 +89,28 @@
     def set_tpc(self,
                 tpc: TargetPlatformCapabilities):
         """
         Set the graph's TPC.
         Args:
             tpc: TargetPlatformCapabilities object.
         """
+        # validate graph nodes are either from the framework or a custom layer defined in the TPC
+        # Validate graph nodes are either built-in layers from the framework or custom layers defined in the TPC
+        tpc_layers = tpc.op_sets_to_layers.get_layers()
+        tpc_filtered_layers = [layer for layer in tpc_layers if isinstance(layer, LayerFilterParams)]
+        for n in self.nodes:
+            is_node_in_tpc = n.type in tpc_layers or any([n.is_match_filter_params(filtered_layer)
+                                                          for filtered_layer in tpc_filtered_layers])
+            if n.is_custom:
+                if not is_node_in_tpc:
+                    Logger.critical(f'MCT does not support optimizing Keras custom layers. Found a layer of type {n.type}. '
+                                 f' Please add the custom layer to Target Platform Capabilities (TPC), or file a feature request or an issue if you believe this should be supported.')
+                if any([qc.default_weight_attr_config.enable_weights_quantization for qc in n.get_qco(tpc).quantization_config_list]):
+                    Logger.critical(f'Layer identified: {n.type}. MCT does not support weight quantization for Keras custom layers.')
+
         self.tpc = tpc
 
     def get_topo_sorted_nodes(self):
         """
         Returns: a list of toposorted nodes.
         """
 
@@ -194,15 +210,15 @@
                                 n: BaseNode) -> BaseStatsCollector:
         """
         Get the output statistics collector of a node containing output statistics of the node.
         Args:
             n: Node to get its output statistics collector.
 
         Returns:
-            Tensor containing output statistics of the node.
+            BaseStatsCollector object of the node.
         """
         return self.node_to_out_stats_collector.get(n)
 
     def get_in_stats_collector(self,
                                n: BaseNode) -> BaseStatsCollector:
         """
         Get the input statistics collector of a node containing input statistics of the node.
@@ -211,15 +227,15 @@
 
         Returns:
             Statistics collector containing input statistics of the node.
         """
 
         sc = self.node_to_in_stats_collector.get(n)
         if sc is None:
-            Logger.error(f'Input statistics collector of node {n.name} is None')  # pragma: no cover
+            Logger.critical(f'No input statistics collector found for node {n.name}.')  # pragma: no cover
         return sc
 
     def scale_stats_collector(self,
                               node: BaseNode,
                               scale_factor: np.ndarray):
         """
         Scale the output statistics of a node in the graph by a given scaling factor.
@@ -280,27 +296,32 @@
             List of output nodes objects.
 
         """
 
         return [edges_list.sink_node for edges_list in self.out_edges(node_obj)]
 
     def get_prev_nodes(self,
-                       node_obj: BaseNode) -> List[BaseNode]:
+                       node_obj: BaseNode,
+                       sink_index_sorted: bool = False) -> List[BaseNode]:
         """
         Get previous nodes (in a topological order) of a node.
 
         Args:
             node_obj: Node to get its previous nodes.
+            sink_index_sorted: Whether to sort the returned list by the sink_index of the edges.
 
         Returns:
             List of input nodes objects.
 
         """
-
-        return [edges_list.source_node for edges_list in self.incoming_edges(node_obj)]
+        if sink_index_sorted:
+            sort_attr = 'sink_index'
+        else:
+            sort_attr = None
+        return [edges_list.source_node for edges_list in self.incoming_edges(node_obj, sort_by_attr=sort_attr)]
 
     def reconnect_out_edges(self,
                             current_node: BaseNode,
                             new_node: BaseNode):
         """
         Connect all outgoing edges of a node to be outgoing edges of a different node
         (useful when replacing a node during substitutions).
@@ -345,16 +366,15 @@
              are zero
         """
 
         if len(input_nodes_output_index) == 0:
             input_nodes_output_index = [0] * len(input_nodes)
 
         if len(input_nodes_output_index) != len(input_nodes):
-            Logger.error('Graph.add_node_with_in_edges: input_nodes & input_nodes_output_index must be the same '
-                         'length')  # pragma: no cover
+            Logger.critical('The number of input nodes and their corresponding output indices must be equal. Found mismatched lengths.')  # pragma: no cover
 
         self.add_node(new_node)
         for sink_index, (in_node, source_index) in enumerate(zip(input_nodes, input_nodes_output_index)):
             self.add_edge(in_node, new_node, source_index=source_index, sink_index=sink_index)
 
     def replace_output_node(self,
                             current_node: BaseNode,
@@ -389,15 +409,15 @@
 
         Args:
             current_node: Node that (possibly) is an input node.
             new_node: New node to set as an input node if the current node is an input node.
 
         """
         if new_node is None:
-            Logger.error("Graph received a None value as a new input node.")
+            Logger.critical("Cannot replace input node with a None value; new input node is required.")
 
         graph_inputs = self.get_inputs()
         new_graph_inputs = copy(graph_inputs)
         if current_node in graph_inputs:
             new_graph_inputs.remove(current_node)
             new_graph_inputs.append(new_node)
         self.set_inputs(new_graph_inputs)
@@ -417,21 +437,21 @@
 
         """
 
         output_nodes = [ot.node for ot in self.get_outputs()]  # get output nodes from namedtuples
         if node_to_remove in output_nodes:  # If node is in the graph's outputs, the outputs should be updated
             if new_graph_outputs is None:
                 Logger.critical(
-                    f'{node_to_remove.name} is in graph outputs, but new outputs were not given.')  # pragma: no cover
+                    f"{node_to_remove.name} is among the graph outputs; however, it cannot be removed without providing a new output.")  # pragma: no cover
             self.set_outputs(new_graph_outputs)
 
         if node_to_remove in self.get_inputs():  # If node is in the graph's inputs, the inputs should be updated
             if new_graph_inputs is None:
                 Logger.critical(
-                    f'{node_to_remove.name} is in graph inputs, but new inputs were not given.')  # pragma: no cover
+                    f'{node_to_remove.name} s among the graph inputs; however, it cannot be removed without providing a new input.')  # pragma: no cover
             self.set_inputs(new_graph_inputs)
 
         # Make sure there are no connected edges left to the node before removing it.
         assert len(
             self.incoming_edges(node_to_remove)) == 0, f'There are {len(self.incoming_edges(node_to_remove))} ' \
                                                        f'incoming ' \
                                                        f'edges to node {node_to_remove}, and they should be removed ' \
@@ -504,60 +524,69 @@
         """
         memory = 0
         for n in self.nodes:
             memory += n.get_float_memory_bytes(self.fw_info)
         return memory
 
     def get_configurable_sorted_nodes_names(self,
+                                            fw_info: FrameworkInfo,
                                             include_reused_nodes: bool = False) -> List[str]:
         """
         Get a list of nodes' names that can be configured (namely, has one or
         more weight qc candidate). The names are sorted according to the topological
         order of the graph.
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether or not to include reused nodes (False by default).
 
         Returns: List of nodes' names that can be configured (namely, has one or
         more weight qc candidate) sorted topology.
 
         """
-        sorted_names = [n.name for n in self.get_configurable_sorted_nodes(include_reused_nodes=include_reused_nodes)]
+        sorted_names = [n.name for n in self.get_configurable_sorted_nodes(fw_info=fw_info,
+                                                                           include_reused_nodes=include_reused_nodes)]
         return sorted_names
 
     def get_weights_configurable_nodes(self,
+                                       fw_info: FrameworkInfo,
                                        include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of nodes that their weights can be configured (namely, has one or
         more weight qc candidate and their weights should be quantized).
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether to include reused nodes (False by default).
 
         Returns:
             A list of nodes that their weights can be configured (namely, has one or more weight qc candidate).
         """
-        return list(filter(lambda n: n.is_weights_quantization_enabled()
-                                     and not n.is_all_weights_candidates_equal()
-                                     and (not n.reuse or include_reused_nodes), list(self)))
+        # configurability is only relevant for kernel attribute quantization
+        potential_conf_nodes = [n for n in list(self) if fw_info.is_kernel_op(n.type)]
+        return list(filter(lambda n: n.is_weights_quantization_enabled(fw_info.get_kernel_op_attributes(n.type)[0])
+                                     and not n.is_all_weights_candidates_equal(fw_info.get_kernel_op_attributes(n.type)[0])
+                                     and (not n.reuse or include_reused_nodes), potential_conf_nodes))
 
     def get_sorted_weights_configurable_nodes(self,
+                                              fw_info: FrameworkInfo,
                                               include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of sorted nodes that their weights can be configured (namely, has one or
         more weight qc candidate and their weights should be quantized).
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether to include reused nodes (False by default).
 
         Returns:
             A list of nodes that their weights can be configured (namely, has one or more weight qc candidate)
             sorted topologically.
         """
-        return self._sort_nodes_in_list(self.get_weights_configurable_nodes(include_reused_nodes))
+        return self._sort_nodes_in_list(self.get_weights_configurable_nodes(fw_info, include_reused_nodes))
 
     def get_activation_configurable_nodes(self) -> List[BaseNode]:
         """
         Get a list of nodes that their activation can be configured (namely, has one or
         more activation qc candidate and their activation should be quantized).
 
         Returns:
@@ -574,28 +603,30 @@
         Returns:
             A list of nodes that their activation can be configured (namely, has one or more activation qc candidate)
             sorted topologically.
         """
         return self._sort_nodes_in_list(self.get_activation_configurable_nodes())
 
     def get_configurable_sorted_nodes(self,
+                                      fw_info: FrameworkInfo,
                                       include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of nodes that can be configured (namely, has one or
         more qc candidate and their weights or activations should be quantized).
         The nodes are sorted according to the topological order of the graph.
 
         Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether or not to include reused nodes (False by default).
 
         Returns:
              A list of nodes that can be configured (namely, has one or more qc candidate) sorted topology.
 
         """
-        weights_configurable_nodes = self.get_weights_configurable_nodes(include_reused_nodes)
+        weights_configurable_nodes = self.get_weights_configurable_nodes(fw_info, include_reused_nodes)
         activation_configurable_nodes = self.get_activation_configurable_nodes()
 
         # combine and remove duplications
         configurable_nodes = list(set(weights_configurable_nodes + activation_configurable_nodes))
 
         return self._sort_nodes_in_list(configurable_nodes)
 
@@ -612,59 +643,70 @@
         sorted_configurable_nodes = []
         sorted_nodes = list(topological_sort(self))
         for n in sorted_nodes:
             if n in nodes_list:
                 sorted_configurable_nodes.append(n)
         return sorted_configurable_nodes
 
-    def get_min_candidates_config(self) -> List[int]:
+    def get_min_candidates_config(self, fw_info: FrameworkInfo) -> List[int]:
         """
         Builds a minimal configuration.
         Note: we assume that a minimal configuration exists, i.e., each configurable node has exactly one candidate
             with minimal n_bits (in both weight and activation if both are quantized, or in the relevant one if only
             one of them is quantized)
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of candidate for each node (list on indices)
         """
 
-        conf_sorted_nodes = self.get_configurable_sorted_nodes()
+        conf_sorted_nodes = self.get_configurable_sorted_nodes(fw_info)
         min_cfg_candidates = [n.find_min_candidates_indices() for n in conf_sorted_nodes]  # list of lists of indices
 
         assert all([len(lst) == 1 for lst in min_cfg_candidates]), \
             f"A minimal config candidate must be defined, but some node have multiple potential minimal candidates"
 
         return [lst[0] for lst in min_cfg_candidates]
 
-    def get_max_candidates_config(self) -> List[int]:
+    def get_max_candidates_config(self, fw_info: FrameworkInfo) -> List[int]:
         """
         Builds a maximal configuration.
         Note: we assume that a maximal configuration exists, i.e., each configurable node has exactly one candidate
             with maximal n_bits (in both weight and activation if both are quantized, or in the relevant one if only
             one of them is quantized)
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of candidate for each node (list on indices)
         """
 
-        conf_sorted_nodes = self.get_configurable_sorted_nodes()
+        conf_sorted_nodes = self.get_configurable_sorted_nodes(fw_info)
         max_cfg_candidates = [n.find_max_candidates_indices() for n in conf_sorted_nodes]  # list of lists of indices
 
         assert all([len(lst) == 1 for lst in max_cfg_candidates]), \
             f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
 
         return [lst[0] for lst in max_cfg_candidates]
 
-    def get_final_weights_config(self) -> List[Tuple[BaseNode, int]]:
+    def get_final_weights_config(self, fw_info: FrameworkInfo) -> List[Tuple[BaseNode, int]]:
         """
         Gets the final number of bits for quantization of each weights' configurable layer.
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of pairs of (node type, node's weights quantization bitwidth).
 
         """
-        sorted_conf_weights = self.get_sorted_weights_configurable_nodes()
-        return [(n, n.final_weights_quantization_cfg.weights_n_bits) for n in sorted_conf_weights]
+        sorted_conf_weights = self.get_sorted_weights_configurable_nodes(fw_info)
+        # a configurable node by definition has a kernel op
+        return [(n, n.final_weights_quantization_cfg.get_attr_config(self.fw_info.get_kernel_op_attributes(n.type)[0]).weights_n_bits)
+                for n in sorted_conf_weights]
 
     def get_final_activation_config(self) -> List[Tuple[BaseNode, int]]:
         """
         Gets the final number of bits for quantization of each activation configurable layer.
 
         Returns: A list of pairs of (node type, nod's activation quantization bitwidth).
 
@@ -686,7 +728,134 @@
         """
         Checks whether all nodes in the graph that have activation quantization are quantized with the same bit-width.
 
         Returns: True if all quantization config candidates of all nodes have the same activation quantization bit-width.
 
         """
         return all([n.is_all_activation_candidates_equal() for n in self.nodes])
+
+    def replace_node(self, node_to_replace: BaseNode, new_node: BaseNode):
+        """
+        Replaces a node in the graph with a new node.
+
+        Args:
+            node_to_replace: The node to replace.
+            new_node: The new node to replace with.
+
+        """
+        self.add_node(new_node)
+        self.reconnect_out_edges(node_to_replace, new_node)
+        self.reconnect_in_edges(node_to_replace, new_node)
+        self.replace_output_node(node_to_replace, new_node)
+        self.replace_input_node(node_to_replace, new_node)
+        self.remove_node(node_to_replace)
+
+    def get_pruning_sections(self,
+                             fw_impl: Any) -> List[PruningSection]:
+        """
+        Constructs pruning sections for a given computational graph.
+        Each section is created starting from an entry node and includes intermediate and exit nodes.
+
+        Args:
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: List of PruningSection in the graph.
+        """
+        entry_nodes = self.get_pruning_sections_entry_nodes(fw_impl)
+        return [self._create_pruning_section(entry_node,  fw_impl) for entry_node in entry_nodes]
+
+    def get_pruning_sections_entry_nodes(self, fw_impl: Any) -> List[BaseNode]:
+        """
+        Identifies entry nodes for pruning sections within the graph.
+        Traverses the graph in a topological order, checking each node for prunability criteria.
+        Returns a list of nodes that mark the beginning of a prunable section in the graph.
+
+        Args:
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: List of nodes that are entry nodes in the pruning sections of the graph.
+
+        """
+        prunable_nodes = []
+        for n in list(topological_sort(self)):
+            if fw_impl.is_node_entry_node(n) and self._is_node_topology_prunable(n, fw_impl):
+                prunable_nodes.append(n)
+        return prunable_nodes
+
+    def _is_node_topology_prunable(self, entry_node: BaseNode, fw_impl: Any) -> bool:
+        """
+        Determines if the topology starting from a given entry node is suitable for pruning.
+        Iteratively examines the graph structure, focusing on node connectivity and pruning criteria.
+        Returns True if the topology is prunable, False otherwise.
+
+        Args:
+            entry_node (BaseNode): The node to start the topology check from.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: Whether this node is a start of a pruning section according to the graph topology or not.
+
+        """
+        next_node = entry_node
+
+        # Continue iterating until the conditions for prunability are no longer met
+        while len(self.out_edges(next_node)) == 1:
+            next_node = self.out_edges(next_node)[0].sink_node
+
+            # If next_node is an exit node and has only one incoming edge, the topology is prunable.
+            if fw_impl.is_node_exit_node(next_node, entry_node, self.fw_info) and len(self.in_edges(next_node)) == 1:
+                return True
+
+            # If the next node is not an intermediate node or has more than one incoming/outgoing edge,
+            # stop the check.
+            if not fw_impl.is_node_intermediate_pruning_section(next_node) or len(self.in_edges(next_node)) != 1 or len(self.out_edges(next_node)) != 1:
+                return False
+
+        # If the loop exits normally, it implies that the topology is not prunable
+        return False
+
+
+    def _create_pruning_section(self, entry_node: BaseNode, fw_impl: Any) -> PruningSection:
+        """
+        Creates a PruningSection object starting from a given entry node.
+        Includes logic to find intermediate and exit nodes to complete the section.
+        Ensures the provided entry node is a valid starting point for pruning.
+
+        Args:
+            entry_node (BaseNode): The entry node to create the section it starts.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: The pruning section that starts with node entry_node.
+
+        """
+        if not fw_impl.is_node_entry_node(entry_node):
+            Logger.critical(f"Node {entry_node} is not a valid entry node for creating a pruning section")
+
+        intermediate_nodes, exit_node = self._find_intermediate_and_exit_nodes(entry_node, fw_impl)
+
+        if not fw_impl.is_node_exit_node(exit_node, entry_node, self.fw_info):
+            Logger.critical(f"Node {exit_node} is not a valid exit node for the pruning section starting with {entry_node}.")
+
+        return PruningSection(entry_node=entry_node,
+                              intermediate_nodes=intermediate_nodes,
+                              exit_node=exit_node)
+
+    def _find_intermediate_and_exit_nodes(self, entry_node: BaseNode, fw_impl: Any) -> Tuple[List[BaseNode], BaseNode]:
+        """
+        Identifies intermediate and exit nodes for a pruning section starting from an entry node.
+        Iterates through connected nodes to build the complete structure of the pruning section.
+
+        Args:
+            entry_node (BaseNode): An entry node to find the intermediate and exit nodes of its section.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: A tuple containing a list of intermediate nodes and the exit node.
+
+        """
+        intermediate_nodes = []
+        next_node = self.out_edges(entry_node)[0].sink_node
+        while not fw_impl.is_node_exit_node(next_node, entry_node, self.fw_info):
+            intermediate_nodes.append(next_node)
+            next_node = self.out_edges(next_node)[0].sink_node
+
+        return intermediate_nodes, next_node
+
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/base_node.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/base_node.py`

 * *Files 25% similar despite different names*

```diff
@@ -10,20 +10,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import copy
-from typing import Dict, Any, Tuple, List
+from typing import Dict, Any, Tuple, List, Type
 
 import numpy as np
 
 from model_compression_toolkit.constants import WEIGHTS_NBITS_ATTRIBUTE, CORRECTED_BIAS_ATTRIBUTE, \
-    ACTIVATION_NBITS_ATTRIBUTE
+    ACTIVATION_NBITS_ATTRIBUTE, FP32_BYTES_PER_PARAMETER
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationConfigOptions, \
     TargetPlatformCapabilities, LayerFilterParams
 
 
 class BaseNode:
     """
@@ -36,30 +37,34 @@
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
-                 has_activation: bool = True
+                 has_activation: bool = True,
+                 is_custom: bool = False
                  ):
         """
         Init a Node object.
 
         Args:
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
             output_shape: Input tensor shape of the node.
             weights: Dictionary from a variable name to the weights with that name in the layer the node represents.
+                     Constant inputs to a node are also saved in the weights (AKA positional weights) dictionary and
+                     their key is their position (an integer) in the node's call_args.
             layer_class: Class path of the layer this node represents.
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             has_activation: Whether the node has activations that we might want to quantize.
+            is_custom: Whether the node is custom layer or not.
         """
         self.name = name
         self.framework_attr = framework_attr
         self.quantization_attr = quantization_attr if quantization_attr is not None else dict()
         self.input_shape = input_shape
         self.output_shape = output_shape
         self.weights = weights
@@ -67,20 +72,22 @@
         self.reuse = reuse
         self.reuse_group = reuse_group
         self.final_weights_quantization_cfg = None
         self.final_activation_quantization_cfg = None
         self.candidates_quantization_cfg = None
         self.prior_info = None
         self.has_activation = has_activation
+        self.is_custom = is_custom
 
     @property
     def type(self):
         """
         A function to get the node's layer_class op for convenient comparison
-        :return: the node's layer_class
+        Returns:
+            the node's layer_class
         """
         return self.layer_class
 
     def get_has_activation(self):
         """
         Returns has_activation attribute.
 
@@ -100,37 +107,54 @@
             return self.final_activation_quantization_cfg.enable_activation_quantization
 
         for qc in self.candidates_quantization_cfg:
             assert self.candidates_quantization_cfg[0].activation_quantization_cfg.enable_activation_quantization == \
                    qc.activation_quantization_cfg.enable_activation_quantization
         return self.candidates_quantization_cfg[0].activation_quantization_cfg.enable_activation_quantization
 
-    def is_weights_quantization_enabled(self) -> bool:
+    def is_weights_quantization_enabled(self, attr_name: str) -> bool:
         """
+        Checks whether a node's weights attribute quantization is enabled.
+
+        Args:
+            attr_name: An attribute to check if its quantization is enabled.
 
         Returns: Whether node weights quantization is enabled or not.
 
         """
         if self.final_weights_quantization_cfg:
             # if we have a final configuration, then we only care to check if it enables weights quantization
-            return self.final_weights_quantization_cfg.enable_weights_quantization
+            return self.final_weights_quantization_cfg.get_attr_config(attr_name).enable_weights_quantization
 
-        for qc in self.candidates_quantization_cfg:
-            assert self.candidates_quantization_cfg[0].weights_quantization_cfg.enable_weights_quantization == \
-                   qc.weights_quantization_cfg.enable_weights_quantization
-        return self.candidates_quantization_cfg[0].weights_quantization_cfg.enable_weights_quantization
+        attr_candidates = self.get_all_weights_attr_candidates(attr_name)
+        candidates_enable_quantization = [c.enable_weights_quantization for c in attr_candidates]
+        if len(candidates_enable_quantization) > 0 and len(set(candidates_enable_quantization)) > 1:
+            Logger.error(f"Weights attribute {attr_name} in node {self.name} has multiple quantization candidates "
+                         f"configuration with incompatible values.")
+        if all(candidates_enable_quantization):
+            return True
+
+        return False
 
     def __repr__(self):
         """
 
         Returns: String that represents the node.
 
         """
         return f'{self.type.__name__}:{self.name}'
 
+    def is_reused(self) -> bool:
+        """
+        Check whether the node is reused or not
+        Returns:
+            True if node is reused, else False
+        """
+        return self.reuse or self.reuse_group is not None
+
     def get_weights_by_keys(self, name: str) -> np.ndarray:
         """
         Get a node's weight by its name.
         Args:
             name: Name of the variable for a node's weight.
 
         Returns:
@@ -161,18 +185,43 @@
         else:  # Add if not exist
             self.weights[name] = tensor
             self.weights_keys = list(self.weights.keys())  # update keys
 
     def get_weights_list(self):
         """
 
-        Returns: A list of all weights the node holds.
+        Returns: A list of all non-positional weights the node holds.
 
         """
-        return [self.weights[k] for k in self.weights.keys() if self.weights[k] is not None]
+        return [self.weights[k] for k in self.weights.keys() if not isinstance(k, int)]
+
+    def get_node_weights_attributes(self) -> List[str]:
+        """
+
+        Returns: A list of all weights attributes that the node holds.
+
+        """
+        return list(self.weights.keys())
+
+    def insert_positional_weights_to_input_list(self, input_tensors: List) -> List:
+        """
+        Insert node's positional weights to input tensors list. The positional weights are inserted
+        in the node's list of inputs according to their keys in the weights dictionary.
+
+        Args:
+            input_tensors: activation input tensors to node.
+        Returns:
+            Activation input tensors list with positional weights
+        """
+        for pos, weight in sorted((pos, weight) for pos, weight in self.weights.items()
+                                  if isinstance(pos, int)):
+            assert pos <= len(input_tensors), 'Positional weight index mismatch'
+            input_tensors.insert(pos, weight)
+
+        return input_tensors
 
     def get_num_parameters(self, fw_info) -> Tuple[int,int]:
         """
         Compute the number of parameters the node holds.
         It returns a tuple: Number of quantized parameters, number of float parameters.
 
         Args:
@@ -204,116 +253,155 @@
 
         Args:
             fw_info: Framework info to decide which attributes should be quantized.
 
         Returns: Number of bytes the node's memory requires.
 
         """
+        # TODO: this method is used for tensorboard only. If we want to enable logging of other attributes memory
+        #  then it needs to be modified. But, it might be better to remove this method from the BaseNode completely.
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        if kernel_attr is None:
+            return 0
         q_params, f_params = self.get_num_parameters(fw_info)
         if self.final_weights_quantization_cfg is None:  # float coefficients
-            memory = (f_params+q_params) * 4
+            memory = (f_params+q_params) * FP32_BYTES_PER_PARAMETER
         else:
-            memory = (f_params*4)+ (q_params * self.final_weights_quantization_cfg.weights_n_bits / 8)  # in bytes
+            memory = ((f_params * FP32_BYTES_PER_PARAMETER) +
+                      (q_params * self.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                       / 8))  # in bytes
 
         return memory
 
     def get_float_memory_bytes(self, fw_info) -> float:
         """
         Compute the number of bytes the node's memory requires.
 
         Args:
             fw_info: Framework info to decide which attributes should be quantized.
 
         Returns: Number of bytes the node's memory requires when in floating point (32 bit).
 
         """
         q_params, f_params = self.get_num_parameters(fw_info)
-        return (f_params + q_params) * 32 / 8 # in bytes
+        return (f_params + q_params) * FP32_BYTES_PER_PARAMETER
 
-    def get_unified_weights_candidates_dict(self):
+    def get_unified_weights_candidates_dict(self, fw_info) -> Dict[str, Any]:
         """
-        In Mixed-Precision, a node can have multiple candidates for weights quantization configuration.
+        In Mixed-Precision, a node's kernel can have multiple candidates for weights quantization configuration.
         In order to display a single view of a node (for example, for logging in TensorBoard) we need a way
         to create a single dictionary from all candidates.
         This method is aimed to build such an unified dictionary for a node.
 
+        Args:
+            fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
         Returns: A dictionary containing information from node's weight quantization configuration candidates.
 
         """
-        shared_attributes = [CORRECTED_BIAS_ATTRIBUTE, WEIGHTS_NBITS_ATTRIBUTE]
-        attr = dict()
-        if self.is_weights_quantization_enabled():
-            attr = copy.deepcopy(self.candidates_quantization_cfg[0].weights_quantization_cfg.__dict__)
-            for shared_attr in shared_attributes:
-                if shared_attr in attr:
-                    unified_attr = []
-                    for candidate in self.candidates_quantization_cfg:
-                        unified_attr.append(getattr(candidate.weights_quantization_cfg, shared_attr))
-                    attr[shared_attr] = unified_attr
-        return attr
+        shared_parameters = [CORRECTED_BIAS_ATTRIBUTE, WEIGHTS_NBITS_ATTRIBUTE]
+        parameters_dict = dict()
+        # We assume that only the kernel attribute have more than one candidate, since we only allow to
+        # quantize the kernel using mixed precision
+        # TODO: need to modify if we want to present a unified config for other attributes
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        if kernel_attr is None:
+            # This node doesn't have a kernel attribute
+            return {}
+
+        if self.is_weights_quantization_enabled(kernel_attr):
+            parameters_dict = copy.deepcopy(self.candidates_quantization_cfg[0].weights_quantization_cfg.
+                                            get_attr_config(kernel_attr).__dict__)
+            for shared_parameter in shared_parameters:
+                if shared_parameter in parameters_dict:
+                    unified_param = []
+                    attr_candidates = self.get_all_weights_attr_candidates(kernel_attr)
+                    for attr_candidate in attr_candidates:
+                        unified_param.append(getattr(attr_candidate, shared_parameter))
+                    parameters_dict[shared_parameter] = unified_param
+        return parameters_dict
 
-    def get_unified_activation_candidates_dict(self):
+    def get_unified_activation_candidates_dict(self) -> Dict[str, Any]:
         """
         In Mixed-Precision, a node can have multiple candidates for activation quantization configuration.
         In order to display a single view of a node (for example, for logging in TensorBoard) we need a way
         to create a single dictionary from all candidates.
         This method is aimed to build such an unified dictionary for a node.
 
         Returns: A dictionary containing information from node's activation quantization configuration candidates.
 
         """
         shared_attributes = [ACTIVATION_NBITS_ATTRIBUTE]
         attr = dict()
-        if self.is_weights_quantization_enabled():
+        if self.is_activation_quantization_enabled():
             attr = copy.deepcopy(self.candidates_quantization_cfg[0].activation_quantization_cfg.__dict__)
             for shared_attr in shared_attributes:
                 if shared_attr in attr:
                     unified_attr = []
                     for candidate in self.candidates_quantization_cfg:
                         unified_attr.append(getattr(candidate.activation_quantization_cfg, shared_attr))
                     attr[shared_attr] = unified_attr
         return attr
 
-    def is_all_activation_candidates_equal(self):
+    def is_all_activation_candidates_equal(self) -> bool:
         """
         Checks whether all candidates' quantization configuration have the same activation configuration,
         using the self-implemented __eq__ method of class NodeActivationQuantizationConfig.
 
         Returns: True if all candidates have same activation configuration, False otherwise.
 
         """
         return all(candidate.activation_quantization_cfg ==
                    self.candidates_quantization_cfg[0].activation_quantization_cfg
                    for candidate in self.candidates_quantization_cfg)
 
-    def is_all_weights_candidates_equal(self):
+    def is_all_weights_candidates_equal(self, attr: str) -> bool:
         """
-        Checks whether all candidates' quantization configuration have the same weights configuration,
+        Checks whether all candidates' quantization configuration of a given weights attribute
+        have the same weights configuration,
         using the self-implemented __eq__ method of class NodeWeightsQuantizationConfig.
 
-        Returns: True if all candidates have same weights configuration, False otherwise.
+        Args:
+            attr: The attribute name to check if all its quantization configuration candidates are equal.
+
+        Returns: True if all the weights attribute candidates have same configuration, False otherwise.
 
         """
-        return all(candidate.weights_quantization_cfg ==
-                   self.candidates_quantization_cfg[0].weights_quantization_cfg
-                   for candidate in self.candidates_quantization_cfg)
+        # note that if the given attribute name does not exist in the node's attributes mapping,
+        # the inner method would log an exception.
+        return all(attr_candidate ==
+                   self.candidates_quantization_cfg[0].weights_quantization_cfg.get_attr_config(attr)
+                   for attr_candidate in self.get_all_weights_attr_candidates(attr))
 
-    def has_weights_to_quantize(self, fw_info):
+    def has_kernel_weight_to_quantize(self, fw_info):
         """
-        Checks whether the node has weights that need to be quantized according to the framework info.
+        Checks whether the node has kernel attribute that need to be quantized according to the framework info.
+
         Args:
             fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
         Returns: Whether the node has weights that need to be quantized.
         """
         attrs = fw_info.get_kernel_op_attributes(self.type)
         for attr in attrs:
             if attr and self.get_weights_by_keys(attr) is not None:
                 return True
         return False
 
+    def has_any_weight_attr_to_quantize(self) -> bool:
+        """
+        Checks whether the node has any weights attribute that is supposed to be quantized, based on its provided
+        quantization configuration candidates.
+
+        Returns: True if the is at least one weights attribute in the node that is supposed to be quantized.
+
+        """
+
+        return any([self.is_weights_quantization_enabled(attr) for attr in self.get_node_weights_attributes()])
+
     def get_total_output_params(self) -> float:
         """
         Calculates the output size of the node.
 
         Returns: Output size.
         """
         output_shapes = self.output_shape if isinstance(self.output_shape, List) else [self.output_shape]
@@ -337,15 +425,15 @@
 
         return sum([np.prod([x for x in input_shape if x is not None]) for input_shape in input_shapes])
 
     def find_min_candidates_indices(self) -> List[int]:
         """
         Returns a list with potential minimal candidates.
         A potential minimal candidate is a candidate which its weights_n_bits and activation_n_bits pair is
-        on the Pareto Front, i.e., there is no other candidates that its n_bits pair exceeds in both entries.
+        on the Pareto Front, i.e., there is no other candidate that its n_bits pair exceeds in both entries.
 
         Returns: A list of indices of potential minimal candidates.
 
         """
 
         # We assume that the candidates are sorted according to weights_n_bits first and activation_n_bits second
         # First, we add the last candidate to the set of minimal candidates (candidate, index)
@@ -377,28 +465,37 @@
         # Iterate over all other candidates, and add ones with higher weights_n_bits but smaller activation_n_bits
         for i, c in enumerate(self.candidates_quantization_cfg):
             if c.activation_quantization_cfg.activation_n_bits > first_max[1]:
                 max_candidates.append((i, c))
 
         return [i for i, a_n_bits in max_candidates]
 
-    def get_unique_weights_candidates(self) -> List[Any]:
+    def get_unique_weights_candidates(self, attr: str) -> List[Any]:
         """
-        Returns a list with node's candidates of unique weights bit-width value.
-        If the node have multiple candidates with the same weights bit-width,
+        Returns a list with node's candidates of unique weights bit-width value for the given attribute.
+        If the node have multiple candidates with the same weights bit-width for this attribute,
         the first candidate in the list is returned.
 
-        Returns: A list with node's candidates of unique weights bit-width value.
+        Args:
+            attr: A weights attribute name to get its unique candidates list.
+
+        Returns: A list with node's candidates of unique weights bit-width value for the given attribute.
         """
 
+        if attr is None or len(self.get_all_weights_attr_candidates(attr)) == 0:
+            Logger.warning(f"Trying to retrieve quantization configuration candidates for attribute '{attr}', "
+                           f"but such attribute can't be found in node {self.name}."
+                           f"An empty list of candidates is returned.")
+            return []
+
         unique_candidates = copy.deepcopy(self.candidates_quantization_cfg)
         seen_candidates = set()
         unique_candidates = [candidate for candidate in unique_candidates if
-                             candidate.weights_quantization_cfg not in seen_candidates
-                             and not seen_candidates.add(candidate.weights_quantization_cfg)]
+                             candidate.weights_quantization_cfg.get_attr_config(attr) not in seen_candidates
+                             and not seen_candidates.add(candidate.weights_quantization_cfg.get_attr_config(attr))]
         return unique_candidates
 
     def get_unique_activation_candidates(self) -> List[Any]:
         """
         Returns a list with node's candidates of unique activation bit-width value.
         If the node have multiple candidates with the same activation bit-width,
         the first candidate in the list is returned.
@@ -409,70 +506,90 @@
         unique_candidates = copy.deepcopy(self.candidates_quantization_cfg)
         seen_candidates = set()
         unique_candidates = [candidate for candidate in unique_candidates if
                              candidate.activation_quantization_cfg not in seen_candidates
                              and not seen_candidates.add(candidate.activation_quantization_cfg)]
         return unique_candidates
 
-    def has_weights_quantization_enabled_candidate(self) -> bool:
-        """
-        Checks whether the node has quantization configuration candidates that enable weights quantization.
-
-        Returns: True if the node has at list one quantization configuration candidate with weights quantization enabled.
-        """
-
-        return len(self.candidates_quantization_cfg) > 0 and \
-               any([c.weights_quantization_cfg.enable_weights_quantization for c in self.candidates_quantization_cfg])
-
     def has_activation_quantization_enabled_candidate(self) -> bool:
         """
         Checks whether the node has quantization configuration candidates that enable activation quantization.
 
         Returns: True if the node has at list one quantization configuration candidate with activation quantization enabled.
         """
 
         return len(self.candidates_quantization_cfg) > 0 and \
                any([c.activation_quantization_cfg.enable_activation_quantization for c in self.candidates_quantization_cfg])
 
+    def get_all_weights_attr_candidates(self, attr: str) -> List[WeightsAttrQuantizationConfig]:
+        """
+        Returns all WeightsAttrQuantizationConfig configuration of the given attribute of the node.
+
+        Args:
+            attr: The attribute name to get its configurations.
+
+        Returns: A list of the attribute's quantization configurations.
+
+        """
+        # note that if the given attribute name does not exist in the node's attributes mapping,
+        # the inner method would log an exception.
+        return [c.weights_quantization_cfg.get_attr_config(attr) for c in self.candidates_quantization_cfg]
+
     def get_qco(self, tpc: TargetPlatformCapabilities) -> QuantizationConfigOptions:
         """
         Get the QuantizationConfigOptions of the node according
         to the mappings from layers/LayerFilterParams to the OperatorsSet in the TargetPlatformModel.
 
         Args:
             tpc: TPC to extract the QuantizationConfigOptions for the node
 
         Returns:
             QuantizationConfigOptions of the node.
         """
 
         if tpc is None:
-            Logger.error(f'Can not retrieve QC options for None TPC')  # pragma: no cover
+            Logger.critical(f'Can not retrieve QC options for None TPC')  # pragma: no cover
 
         for fl, qco in tpc.filterlayer2qco.items():
             if self.is_match_filter_params(fl):
                 return qco
         if self.type in tpc.layer2qco:
             return tpc.layer2qco.get(self.type)
         return tpc.tp_model.default_qco
 
+    def is_match_type(self, _type: Type) -> bool:
+        """
+        Check if input type matches the node type, either in instance type or in type name. Checking the
+        name string is required because of function types changes that occurred in TF 2.15.
+
+        Args:
+            _type: other node type
+        Returns:
+            Whether _type matches the self node type
+
+        """
+        return _type == self.type or _type.__name__ == self.type.__name__
 
     def is_match_filter_params(self, layer_filter_params: LayerFilterParams) -> bool:
         """
         Check if the node matches a LayerFilterParams according to its
         layer, conditions and keyword-arguments.
 
         Args:
             layer_filter_params: LayerFilterParams to check if the node matches its properties.
 
         Returns:
             Whether the node matches to the LayerFilterParams properties.
         """
+        # check if provided argument is of type LayerFilterParams
+        if not isinstance(layer_filter_params, LayerFilterParams):
+            return False
+
         # Check the node has the same type as the layer in LayerFilterParams
-        if layer_filter_params.layer != self.type:
+        if not self.is_match_type(layer_filter_params.layer):
             return False
 
         # Get attributes from node to filter
         layer_config = self.framework_attr
         if hasattr(self, "op_call_kwargs"):
             layer_config.update(self.op_call_kwargs)
 
@@ -480,8 +597,52 @@
             if layer_config.get(attr) != value:
                 return False
 
         for c in layer_filter_params.conditions:
             if not c.match(layer_config):
                 return False
 
-        return True
+        return True
+
+    def get_simd(self) -> int:
+        """
+        Retrieves the SIMD size used for this node. It collects the SIMD sizes from all candidate
+        configurations and returns the minimum SIMD size.
+
+        Returns:
+            int: The node's SIMD size.
+
+        """
+        simd_list = [qc.weights_quantization_cfg.simd_size for qc in self.candidates_quantization_cfg]
+        if len(simd_list) > 1:
+            Logger.warning(f"More than one pruning SIMD option is available."
+                           f" Min SIMD is used: {min(simd_list)}")
+        if len(simd_list) == 0:
+            Logger.critical(f"No SIMD option is available for {self}")
+        _simd = min(simd_list)
+        if _simd <= 0 or int(_simd) != _simd:
+            Logger.critical(f"SIMD is expected to be a non-positive integer but found: {_simd}")
+        return _simd
+
+    def sort_node_candidates(self, fw_info):
+        """
+        Sorts the node candidates.
+        We assume that the candidates are ordered in the following way (for mixed precision purposes):
+            - If the node has a kernel attribute, then we use the kernel weights number of bits to sort the candidates
+            (in descending order). We use the candidate activation number of bits as a secondary order.
+            - If the node doesn't have a kernel we only consider the candidate activation number of bits to sort
+            the candidates in descending order.
+        The operation is done inplace.
+
+        Args:
+            fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
+        """
+        if self.candidates_quantization_cfg is not None:
+            kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+            if kernel_attr is not None:
+                self.candidates_quantization_cfg.sort(
+                    key=lambda c: (c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
+                                   c.activation_quantization_cfg.activation_n_bits), reverse=True)
+            else:
+                self.candidates_quantization_cfg.sort(key=lambda c: c.activation_quantization_cfg.activation_n_bits,
+                                                      reverse=True)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/edge.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/edge.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from typing import Any, Dict
 
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.logger import Logger
 
 # Edge attributes:
 EDGE_SOURCE_INDEX = 'source_index'
 EDGE_SINK_INDEX = 'sink_index'
 
 
 class Edge(object):
@@ -104,8 +105,8 @@
                     dst_node,
                     edge_data[EDGE_SOURCE_INDEX],
                     edge_data[EDGE_SINK_INDEX])
 
     elif isinstance(edge, Edge):  # it's already an Edge and no change need to be done
         return edge
 
-    raise Exception('Edges list contains an object that is not a known edge format.')
+    Logger.critical('Edge conversion failed: unrecognized edge format encountered.')
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/functional_node.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/functional_node.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,22 +12,23 @@
     def __init__(self,
                  name: str,
                  framework_attr: Dict[str, Any],
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
-                 op_call_args: List[Any] = None,
+                 op_call_args: Tuple[Any] = None,
                  op_call_kwargs: Dict[str, Any] = None,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
                  functional_op: Any = None,
                  inputs_as_list: bool = False,
-                 has_activation: bool = True):
+                 has_activation: bool = True,
+                 tensor_input_indices = None):
         """
         Init a FunctionalNode object.
 
         Args:
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
@@ -38,14 +39,15 @@
             op_call_kwargs: Key-Word Arguments dictionary with values to pass when calling the layer.
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             functional_op: The op the node implements.
             inputs_as_list: Whether to pass the node its input tensors as a list or not when calling the layer.
             has_activation: Whether the node has activations that we might want to quantize.
+            tensor_input_indices: A list of indices for activation tensors in the node's input tensor list
 
         """
 
         super().__init__(name,
                          framework_attr,
                          input_shape,
                          output_shape,
@@ -56,14 +58,15 @@
                          quantization_attr,
                          has_activation=has_activation)
 
         self.op_call_kwargs = op_call_kwargs
         self.op_call_args = op_call_args
         self.functional_op = functional_op
         self.inputs_as_list = inputs_as_list
+        self.tensor_input_indices = [] if tensor_input_indices is None else tensor_input_indices
 
     @property
     def type(self):
         """
         A function to get the node's function op for convenient comparison (instead of the layer_class)
         :return: the node's functional_op
         """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/graph_matchers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/graph_matchers.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,27 +31,27 @@
 
         Args:
             operation: Which layer to check if matches.
         """
 
         self.operation = operation
 
-    def apply(self, input_node_object: Any) -> bool:
+    def apply(self, input_node_object: BaseNode) -> bool:
         """
         Check if input_node_object matches the matcher condition.
 
         Args:
             input_node_object: Node object to check the matcher on.
 
         Returns:
             True if input_node_object is the layer the NodeOperationMatcher holds. Otherwise,
             return nothing.
         """
 
-        if input_node_object.type == self.operation:
+        if input_node_object.is_match_type(self.operation):
             return True
 
 
 class NodeFrameworkAttrMatcher(node_matcher.BaseNodeMatcher):
     """
     Class NodeFrameworkAttrMatcher to check if a node's attribute has a specific value.
     """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/graph_searches.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/graph_searches.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -71,18 +71,16 @@
         Verifies bipartite correctness of a set of edges to add to the graph.
         If there is an edge in the list that violates the bipartite property - an Exception is raised.
 
         Args:
             edges_list: A list of edges to verify their correction.
         """
         for n1, n2 in edges_list:
-            if n1 in self.a_nodes and n2 in self.a_nodes:
-                Logger.critical(f"Can't add an edge {(n1, n2)} between two nodes in size A of a bipartite graph.")
-            if n1 in self.b_nodes and n2 in self.b_nodes:
-                Logger.critical(f"Can't add an edge {(n1, n2)} between two nodes in size B of a bipartite graph.")
+            if (n1 in self.a_nodes and n2 in self.a_nodes) or (n1 in self.b_nodes and n2 in self.b_nodes):
+                Logger.critical(f"Attempted to add edge {(n1, n2)} between nodes of the same partition in a bipartite graph, violating bipartite properties.")
 
     def add_nodes_to_a(self, new_nodes: List[Any]):
         """
         Add a set of nodes to side A of the bipartite graph.
 
         Args:
             new_nodes: New nodes to add to side A.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/cut.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     VIRTUAL_WEIGHTS_SUFFIX, VIRTUAL_ACTIVATION_SUFFIX, FLOAT_BITWIDTH
 
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 import numpy as np
 
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
+from model_compression_toolkit.logger import Logger
 
 
 class VirtualSplitNode(BaseNode):
     """
     A class that represents a node that was split from a kernel node (node with weights).
     """
 
@@ -56,27 +57,28 @@
 class VirtualSplitWeightsNode(VirtualSplitNode):
     """
     A class that represents a node that was split from a kernel node (node with weights) and holds the weights of
     the original node. This node contains the original node's weights and the relevant weights candidate quantization
     config.
     """
 
-    def __init__(self, origin_node: BaseNode):
+    def __init__(self, origin_node: BaseNode, kernel_attr: str):
         """
         Init a VirtualSplitWeightsNode object.
 
         Args:
             origin_node: The original node from which the new node was split.
+            kernel_attr: The name of the kernel attribute of the original node.
         """
 
         super().__init__(origin_node)
 
         self.name = origin_node.name + VIRTUAL_WEIGHTS_SUFFIX
 
-        self.candidates_quantization_cfg = origin_node.get_unique_weights_candidates()
+        self.candidates_quantization_cfg = origin_node.get_unique_weights_candidates(kernel_attr)
         for c in self.candidates_quantization_cfg:
             c.activation_quantization_cfg.enable_activation_quantization = False
             c.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
 
 
 class VirtualSplitActivationNode(VirtualSplitNode):
     """
@@ -107,33 +109,34 @@
             c.weights_quantization_cfg.enable_weights_quantization = False
             c.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
 
 
 class VirtualActivationWeightsNode(BaseNode):
     """
     A node that represents a composition of pair of sequential activation node and weights (kernel) node.
-    This structure is used for mixed-precision search with bit-operation KPI.
+    This structure is used for mixed-precision search with bit-operation constraint.
     The node's candidates are the cartesian product of both nodes' candidates.
 
     Important: note that not like regular BaseNode or FunctionalNode, in VirtualActivationWeightsNode the activation
     candidates config refer to the quantization config of the activation that precedes the linear operation! instead of
     the output of the linear operation.
     It is ok, since this node is not meant to be used in a graph for creating an actual model, but only a virtual
-    representation of the model's graph only for allowing to compute the bit-operations KPI in mixed-precision.
+    representation of the model's graph only for allowing to compute the bit-operations constraint in mixed-precision.
     """
 
     def __init__(self,
                  act_node: BaseNode,
                  weights_node: BaseNode,
                  name: str,
                  framework_attr: Dict[str, Any],
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
+                 fw_info: FrameworkInfo,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
                  has_activation: bool = True,
                  **kwargs):
         """
         Init a VirtualActivationWeightsNode object.
@@ -143,14 +146,15 @@
             weights_node: The original weights node.
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
             output_shape: Input tensor shape of the node.
             weights: Dictionary from a variable name to the weights with that name in the layer the node represents.
             layer_class: Class path of the layer this node represents.
+            fw_info: A FrameworkInfo object with framework specific information,
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             has_activation: Whether the node has activations that we might want to quantize.
             **kwargs: Additional arguments that can be passed but are not used (allows to init the object with an
                 existing node's __dict__).
 
@@ -176,15 +180,16 @@
         for c_a in act_node.candidates_quantization_cfg:
             for c_w in weights_node.candidates_quantization_cfg:
                 composed_candidate = CandidateNodeQuantizationConfig(activation_quantization_cfg=c_a.activation_quantization_cfg,
                                                                      weights_quantization_cfg=c_w.weights_quantization_cfg)
                 v_candidates.append(composed_candidate)
 
         # sorting the candidates by weights number of bits first and then by activation number of bits (reversed order)
-        v_candidates.sort(key=lambda c: (c.weights_quantization_cfg.weights_n_bits,
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        v_candidates.sort(key=lambda c: (c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
                                          c.activation_quantization_cfg.activation_n_bits), reverse=True)
 
         self.candidates_quantization_cfg = v_candidates
 
     def get_bops_count(self, fw_impl: Any, fw_info: FrameworkInfo, candidate_idx: int) -> float:
         """
         Computes the composed node's (edge) bit-operation count.
@@ -193,15 +198,17 @@
             fw_impl: A FrameworkImplementation object with framework specific methods.
             fw_info: A FrameworkInfo object with framework specific information,
             candidate_idx: The index of the node's quantization candidate configuration.
 
         Returns: The BOPS count of the composed node.
 
         """
+        kernel_attr = fw_info.get_kernel_op_attributes(self.original_weights_node.type)[0]
         node_mac = fw_impl.get_node_mac_operations(self.original_weights_node, fw_info)
         candidate = self.candidates_quantization_cfg[candidate_idx]
-        weights_bit = candidate.weights_quantization_cfg.weights_n_bits if \
-            candidate.weights_quantization_cfg.enable_weights_quantization else FLOAT_BITWIDTH
+        kernel_attr_cfg = candidate.weights_quantization_cfg.get_attr_config(kernel_attr)
+        weights_bit = kernel_attr_cfg.weights_n_bits if \
+            kernel_attr_cfg.enable_weights_quantization else FLOAT_BITWIDTH
         activation_bit = candidate.activation_quantization_cfg.activation_n_bits if \
             candidate.activation_quantization_cfg.enable_activation_quantization else FLOAT_BITWIDTH
         node_bops = weights_bit * activation_bit * node_mac
         return node_bops
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/base_graph_filter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/base_matcher.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/base_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/edge_matcher.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/edge_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/function.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/function.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/node_matcher.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/node_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/matchers/walk_matcher.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/matchers/walk_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/memory_computation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/memory_computation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py`

 * *Files 14% similar despite different names*

```diff
@@ -32,98 +32,118 @@
         mixed_precision_enable: Is mixed precision enabled.
         graph: A prepared for quantization graph to set its bit widths.
         bit_widths_config: MP configuration (a list of indices: one for each node's candidate
         quantization configuration).
 
     """
     if mixed_precision_enable:
-        assert all([len(n.candidates_quantization_cfg) > 0 for n in graph.get_configurable_sorted_nodes()]), \
+        assert all([len(n.candidates_quantization_cfg) > 0
+                    for n in graph.get_configurable_sorted_nodes(graph.fw_info)]), \
             "All configurable nodes in graph should have at least one candidate configuration in mixed precision mode"
 
-        Logger.info(f'Set bit widths from configuration: {bit_widths_config}')
         # Get a list of nodes' names we need to finalize (that they have at least one weight qc candidate).
-        sorted_nodes_names = graph.get_configurable_sorted_nodes_names()
-        for node in graph.nodes:  # set a specific node qc for each node final weights qc
+        sorted_nodes_names = graph.get_configurable_sorted_nodes_names(graph.fw_info)
+
+        for node in graph.nodes:  # set a specific node qc for each node final qc
             # If it's reused, take the configuration that the base node has
             node_name = node.name if not node.reuse else '_'.join(node.name.split('_')[:-2])
             if node_name in sorted_nodes_names:  # only configurable nodes are in this list
                 node_index_in_graph = sorted_nodes_names.index(node_name)
                 _set_node_final_qc(bit_widths_config,
                                    node,
-                                   node_index_in_graph)
-            elif node.is_activation_quantization_enabled():
-                # If we are here, this means that we are in weights-only mixed-precision
-                # (i.e., activations are quantized with fixed bitwidth or not quantized)
-                # and that this node doesn't have weights to quantize
-                assert len(node.candidates_quantization_cfg) > 0, \
-                    "Node need to have at least one quantization configuration in order to quantize its activation"
-                node.final_activation_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].activation_quantization_cfg)
-            elif node.is_weights_quantization_enabled():
-                # If we are here, this means that we are in activation-only mixed-precision
-                # (i.e., weights are quantized with fixed bitwidth or not quantized)
-                # and that this node doesn't have activations to quantize
-                assert len(node.candidates_quantization_cfg) > 0, \
-                    "Node need to have at least one quantization configuration in order to quantize its activation"
-                node.final_weights_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].weights_quantization_cfg)
+                                   node_index_in_graph,
+                                   graph.fw_info)
+            else:
+                if node.is_activation_quantization_enabled():
+                    # If we are here, this means that we are in weights-only mixed-precision
+                    # (i.e., activations are quantized with fixed bitwidth or not quantized)
+                    # and that this node doesn't have kernel to quantize
+                    # (since only the kernel is quantized in mixed precision).
+                    assert len(node.candidates_quantization_cfg) > 0, \
+                        "Node need to have at least one quantization configuration in order to quantize its activation"
+                    node.final_activation_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].activation_quantization_cfg)
+
+                if node.has_any_weight_attr_to_quantize():
+                    # If we are here, this means that we are in activation-only mixed-precision
+                    # (i.e., kernel is quantized with fixed bitwidth or not quantized)
+                    # and that this node doesn't have activations to quantize.
+                    assert len(node.candidates_quantization_cfg) > 0, \
+                        "Node need to have at least one quantization configuration in order to quantize its activation"
+                    node.final_weights_quantization_cfg = (
+                        copy.deepcopy(node.candidates_quantization_cfg[0].weights_quantization_cfg))
 
     # When working in non-mixed-precision mode, there's only one bitwidth, and we simply set the
     # only candidate of the node as its final weight and activation quantization configuration.
     else:
         for n in graph.nodes:
             assert len(n.candidates_quantization_cfg) == 1
             n.final_weights_quantization_cfg = copy.deepcopy(n.candidates_quantization_cfg[0].weights_quantization_cfg)
             n.final_activation_quantization_cfg = copy.deepcopy(n.candidates_quantization_cfg[0].activation_quantization_cfg)
 
     return graph
 
 
 def _get_node_qc_by_bit_widths(node: BaseNode,
                                bit_width_cfg: List[int],
-                               node_index_in_graph: int) -> Any:
+                               node_index_in_graph: int,
+                               fw_info) -> Any:
     """
     Get the node's quantization configuration that
     matches to the bit width index as in the MP configuration bit_width_cfg.
     If it was not found, return None.
 
     Args:
         node: Node to get its quantization configuration candidate.
         bit_width_cfg: Configuration which determines the node's desired bit width.
         node_index_in_graph: Index of the node in the bit_width_cfg.
+        fw_info: Information relevant to a specific framework about how layers should be quantized.
 
     Returns:
         Node quantization configuration if it was found, or None otherwise.
     """
+    # only the weights kernel attribute is quantized in weights mixed precision at the moment
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)
 
-    if node.is_weights_quantization_enabled() or node.is_activation_quantization_enabled():
+    if node.is_activation_quantization_enabled():
         bit_index_in_cfg = bit_width_cfg[node_index_in_graph]
         qc = node.candidates_quantization_cfg[bit_index_in_cfg]
+
         return qc
 
-    Logger.critical(f'Node {node.name} quantization configuration from configuration file'  # pragma: no cover
-                    f' was not found in candidates configurations.')
+    elif kernel_attr is not None:
+        if node.is_weights_quantization_enabled(kernel_attr[0]):
+            bit_index_in_cfg = bit_width_cfg[node_index_in_graph]
+            qc = node.candidates_quantization_cfg[bit_index_in_cfg]
+
+            return qc
+
+    Logger.critical(f"Quantization configuration for node '{node.name}' not found in candidate configurations.")  # pragma: no cover
 
 
 def _set_node_final_qc(bit_width_cfg: List[int],
                        node: BaseNode,
-                       node_index_in_graph: int):
+                       node_index_in_graph: int,
+                       fw_info):
     """
     Get the node's quantization configuration that
     matches to the bit width index as in the MP configuration bit_width_cfg, and use it to finalize the node's
     weights and activation quantization config.
     If the node quantization config was not found, raise an exception.
 
     Args:
         bit_width_cfg: Configuration which determines the node's desired bit width.
         node: Node to set its node quantization configuration.
         node_index_in_graph: Index of the node in the bit_width_cfg.
+        fw_info: Information relevant to a specific framework about how layers should be quantized.
 
     """
     node_qc = _get_node_qc_by_bit_widths(node,
                                          bit_width_cfg,
-                                         node_index_in_graph)
+                                         node_index_in_graph,
+                                         fw_info)
 
     if node_qc is None:
         Logger.critical(f'Node {node.name} quantization configuration from configuration file'  # pragma: no cover
                         f' was not found in candidates configurations.')
 
     else:
         node.final_weights_quantization_cfg = node_qc.weights_quantization_cfg
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,23 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from enum import Enum
+from functools import partial
 
 import numpy as np
 
 
 def get_average_weights(distance_matrix: np.ndarray) -> np.ndarray:
     """
     Get weights for weighting the sensitivity among different layers when evaluating MP configurations on
     model's sensitivity. This function returns equal weights for each layer, such that the sensitivity
     is averaged over all layers.
+
     Args:
         distance_matrix: Numpy array at shape (L,M): L -number of interest points, M number of samples.
         The matrix contain the distance for each interest point at each sample.
 
     Returns:
         Numpy array containing equal weights for sensitivity weighting.
     """
@@ -46,7 +49,25 @@
     Returns:
         Numpy array containing weights for sensitivity weighting (all zero but the last one).
     """
     num_nodes = len(distance_matrix)
     w = np.asarray([0 for _ in range(num_nodes)])
     w[-1] = 1
     return w
+
+
+class MpDistanceWeighting(Enum):
+    """
+    Defines mixed precision distance metric weighting methods.
+    The enum values can be used to call a function on a set of arguments and key-arguments.
+
+     AVG - take the average distance on all computed layers.
+
+     LAST_LAYER - take only the distance of the last layer output.
+
+    """
+
+    AVG = partial(get_average_weights)
+    LAST_LAYER = partial(get_last_layer_weights)
+
+    def __call__(self, distance_matrix: np.ndarray) -> np.ndarray:
+        return self.value(distance_matrix)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py`

 * *Files 18% similar despite different names*

```diff
@@ -17,89 +17,89 @@
 from functools import partial
 from typing import List, Any
 import numpy as np
 
 from pulp import lpSum
 
 
-def sum_kpi(kpi_vector: np.ndarray, set_constraints: bool = True) -> List[Any]:
+def sum_ru_values(ru_vector: np.ndarray, set_constraints: bool = True) -> List[Any]:
     """
-    Aggregates KPIs vector to a single KPI measure by summing all values.
+    Aggregates resource utilization vector to a single resource utilization measure by summing all values.
 
     Args:
-        kpi_vector: A vector with nodes' KPI values.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_vector: A vector with nodes' resource utilization values.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with an lpSum object for lp problem definition with the vector's sum.
 
     """
     if not set_constraints:
-        return [0] if len(kpi_vector) == 0 else [sum(kpi_vector)]
-    return [lpSum(kpi_vector)]
+        return [0] if len(ru_vector) == 0 else [sum(ru_vector)]
+    return [lpSum(ru_vector)]
 
 
-def max_kpi(kpi_vector: np.ndarray, set_constraints: bool = True) -> List[float]:
+def max_ru_values(ru_vector: np.ndarray, set_constraints: bool = True) -> List[float]:
     """
-    Aggregates KPIs vector to allow max constraint in the linear programming problem formalization.
-    In order to do so, we need to define a separate constraint on each value in the KPI vector,
-    to be bounded by the target KPI.
+    Aggregates resource utilization vector to allow max constraint in the linear programming problem formalization.
+    In order to do so, we need to define a separate constraint on each value in the resource utilization vector,
+    to be bounded by the target resource utilization.
 
     Args:
-        kpi_vector: A vector with nodes' KPI values.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_vector: A vector with nodes' resource utilization values.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with the vector's values, to be used to define max constraint
     in the linear programming problem formalization.
 
     """
     if not set_constraints:
-        return [0] if len(kpi_vector) == 0 else [max(kpi_vector)]
-    return [kpi for kpi in kpi_vector]
+        return [0] if len(ru_vector) == 0 else [max(ru_vector)]
+    return [ru for ru in ru_vector]
 
 
-def total_kpi(kpi_tensor: np.ndarray, set_constraints: bool = True) -> List[float]:
+def total_ru(ru_tensor: np.ndarray, set_constraints: bool = True) -> List[float]:
     """
-    Aggregates KPIs vector to allow weights and activation total kpi constraint in the linear programming
-    problem formalization. In order to do so, we need to define a separate constraint on each activation value in
-    the KPI vector, combined with the sum weights kpi.
-    Note that the given kpi_tensor should contain weights and activation kpi values in each entry.
+    Aggregates resource utilization vector to allow weights and activation total utilization constraint in the linear programming
+    problem formalization. In order to do so, we need to define a separate constraint on each activation memory utilization value in
+    the resource utilization vector, combined with the sum weights memory utilization.
+    Note that the given ru_tensor should contain weights and activation utilization values in each entry.
 
     Args:
-        kpi_tensor: A tensor with nodes' KPI values for weights and activation.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_tensor: A tensor with nodes' resource utilization values for weights and activation.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with lpSum objects, to be used to define total constraint
     in the linear programming problem formalization.
 
     """
     if not set_constraints:
-        weights_kpi = sum([kpi[0] for kpi in kpi_tensor])
-        activation_kpi = max([kpi[1] for kpi in kpi_tensor])
-        return [weights_kpi + activation_kpi]
+        weights_ru = sum([ru[0] for ru in ru_tensor])
+        activation_ru = max([ru[1] for ru in ru_tensor])
+        return [weights_ru + activation_ru]
 
-    weights_kpi = lpSum([kpi[0] for kpi in kpi_tensor])
-    total_kpis = [weights_kpi + activation_kpi for _, activation_kpi in kpi_tensor]
+    weights_ru = lpSum([ru[0] for ru in ru_tensor])
+    total_ru = [weights_ru + activation_ru for _, activation_ru in ru_tensor]
 
-    return total_kpis
+    return total_ru
 
 
-class MpKpiAggregation(Enum):
+class MpRuAggregation(Enum):
     """
-    Defines kpi aggregation functions that can be used to compute final KPI metric.
+    Defines resource utilization aggregation functions that can be used to compute final resource utilization metric.
     The enum values can be used to call a function on a set of arguments.
 
-     SUM - applies the sum_kpi function
+     SUM - applies the sum_ru_values function
 
-     MAX - applies the max_kpi function
+     MAX - applies the max_ru_values function
 
-     TOTAL - applies the total_kpi function
+     TOTAL - applies the total_ru function
 
     """
-    SUM = partial(sum_kpi)
-    MAX = partial(max_kpi)
-    TOTAL = partial(total_kpi)
+    SUM = partial(sum_ru_values)
+    MAX = partial(max_ru_values)
+    TOTAL = partial(total_ru)
 
     def __call__(self, *args):
         return self.value(*args)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,82 +8,78 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, Any
 import numpy as np
+from typing import Callable, Any
 
-from model_compression_toolkit.core import FrameworkInfo, KPI, CoreConfig
-from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.core import FrameworkInfo, ResourceUtilization, CoreConfig
+from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
+from model_compression_toolkit.core.graph_prep_runner import graph_preparation_runner
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.runner import read_model_to_graph, get_finalized_graph
 
 
-def compute_kpi_data(in_model: Any,
-                     representative_data_gen: Callable,
-                     core_config: CoreConfig,
-                     tpc: TargetPlatformCapabilities,
-                     fw_info: FrameworkInfo,
-                     fw_impl: FrameworkImplementation) -> KPI:
+def compute_resource_utilization_data(in_model: Any,
+                                      representative_data_gen: Callable,
+                                      core_config: CoreConfig,
+                                      tpc: TargetPlatformCapabilities,
+                                      fw_info: FrameworkInfo,
+                                      fw_impl: FrameworkImplementation) -> ResourceUtilization:
     """
-    Compute KPI information that can be relevant for defining target KPI for mixed precision search.
+    Compute Resource Utilization information that can be relevant for defining target ResourceUtilization for mixed precision search.
     Calculates maximal activation tensor, sum of weights' parameters and total (sum of both).
 
     Args:
         in_model:  Model to build graph from (the model that intended to be quantized).
         representative_data_gen: Dataset used for calibration.
         core_config: CoreConfig containing parameters of how the model should be quantized.
         tpc: TargetPlatformCapabilities object that models the inference target platform and
                                               the attached framework operator's information.
         fw_info: Information needed for quantization about the specific framework.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
-    Returns: A KPI object with the results.
+    Returns: A ResourceUtilization object with the results.
 
     """
 
-    graph = read_model_to_graph(in_model,
-                                representative_data_gen,
-                                tpc,
-                                fw_info,
-                                fw_impl)
-
-    transformed_graph = get_finalized_graph(graph,
-                                            tpc,
-                                            core_config.quantization_config,
-                                            fw_info,
-                                            tb_w=None,
-                                            fw_impl=fw_impl,
-                                            mixed_precision_enable=core_config.mixed_precision_enable)
+    # We assume that the resource_utilization_data API is used to compute the model resource utilization for
+    # mixed precision scenario, so we run graph preparation under the assumption of enabled mixed precision.
+    transformed_graph = graph_preparation_runner(in_model,
+                                                 representative_data_gen,
+                                                 core_config.quantization_config,
+                                                 fw_info,
+                                                 fw_impl,
+                                                 tpc,
+                                                 mixed_precision_enable=True)
 
     # Compute parameters sum
     weights_params = compute_nodes_weights_params(graph=transformed_graph, fw_info=fw_info)
     total_weights_params = 0 if len(weights_params) == 0 else sum(weights_params)
 
     # Compute max activation tensor
     activation_output_sizes = compute_activation_output_sizes(graph=transformed_graph)
     max_activation_tensor_size = 0 if len(activation_output_sizes) == 0 else max(activation_output_sizes)
 
-    # Compute total kpi - parameters sum + max activation tensor
+    # Compute total memory utilization - parameters sum + max activation tensor
     total_size = total_weights_params + max_activation_tensor_size
 
-    # Compute BOPS kpi - total count of bit-operations for all configurable layers with kernel
+    # Compute BOPS utilization - total count of bit-operations for all configurable layers with kernel
     bops_count = compute_total_bops(graph=transformed_graph, fw_info=fw_info, fw_impl=fw_impl)
     bops_count = np.inf if len(bops_count) == 0 else sum(bops_count)
 
-    return KPI(weights_memory=total_weights_params,
-               activation_memory=max_activation_tensor_size,
-               total_memory=total_size,
-               bops=bops_count)
+    return ResourceUtilization(weights_memory=total_weights_params,
+                               activation_memory=max_activation_tensor_size,
+                               total_memory=total_size,
+                               bops=bops_count)
 
 
 def compute_nodes_weights_params(graph: Graph, fw_info: FrameworkInfo) -> np.ndarray:
     """
     Computes a vector with the respective weights' parameters size for each node.
 
     Args:
@@ -93,21 +89,28 @@
 
     Returns: A vector of node's weights memory sizes.
 
     """
 
     weights_params = []
     for n in graph.nodes:
-        if n.has_weights_quantization_enabled_candidate() and not n.reuse:
-            node_num_weights_params = 0
-            for attr in fw_info.get_kernel_op_attributes(n.type):
-                if attr is not None:
-                    node_num_weights_params += n.get_weights_by_keys(attr).flatten().shape[0]
+        # TODO: when enabling multiple attribute quantization by default (currently,
+        #  only kernel quantization is enabled) we should include other attributes memory in the sum of all
+        #  weights memory.
+        #  When implementing this, we should just go over all attributes in the node instead of counting only kernels.
+        kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and not n.reuse:
+            kernel_candidates = n.get_all_weights_attr_candidates(kernel_attr)
+            if len(kernel_candidates) > 0 and any([c.enable_weights_quantization for c in kernel_candidates]):
+                node_num_weights_params = 0
+                for attr in fw_info.get_kernel_op_attributes(n.type):
+                    if attr is not None:
+                        node_num_weights_params += n.get_weights_by_keys(attr).flatten().shape[0]
 
-            weights_params.append(node_num_weights_params)
+                weights_params.append(node_num_weights_params)
 
     return np.array(weights_params)
 
 
 def compute_activation_output_sizes(graph: Graph) -> np.ndarray:
     """
     Computes a vector with the respective output tensor size for each node.
@@ -144,15 +147,15 @@
 
     """
 
     bops = []
 
     # Go over all configurable nodes that have kernels.
     for n in graph.get_topo_sorted_nodes():
-        if n.has_weights_to_quantize(fw_info):
+        if n.has_kernel_weight_to_quantize(fw_info):
             # If node doesn't have weights then its MAC count is 0, and we shouldn't consider it in the BOPS count.
             incoming_edges = graph.incoming_edges(n, sort_by_attr=EDGE_SINK_INDEX)
             assert len(incoming_edges) == 1, f"Can't compute BOPS metric for node {n.name} with multiple inputs."
 
             node_mac = fw_impl.get_node_mac_operations(n, fw_info)
 
             node_bops = (FLOAT_BITWIDTH ** 2) * node_mac
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,19 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPITarget
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
 
 
-# When adding a KPITarget that we want to consider in our mp search,
-# a matching pair of kpi_tools computation function and a kpi_tools
+# When adding a RUTarget that we want to consider in our mp search,
+# a matching pair of resource_utilization_tools computation function and a resource_utilization_tools
 # aggregation function should be added to this dictionary
-kpi_functions_mapping = {KPITarget.WEIGHTS: (MpKpiMetric.WEIGHTS_SIZE, MpKpiAggregation.SUM),
-                         KPITarget.ACTIVATION: (MpKpiMetric.ACTIVATION_OUTPUT_SIZE, MpKpiAggregation.MAX),
-                         KPITarget.TOTAL: (MpKpiMetric.TOTAL_WEIGHTS_ACTIVATION_SIZE, MpKpiAggregation.TOTAL),
-                         KPITarget.BOPS: (MpKpiMetric.BOPS_COUNT, MpKpiAggregation.SUM)}
+ru_functions_mapping = {RUTarget.WEIGHTS: (MpRuMetric.WEIGHTS_SIZE, MpRuAggregation.SUM),
+                        RUTarget.ACTIVATION: (MpRuMetric.ACTIVATION_OUTPUT_SIZE, MpRuAggregation.MAX),
+                        RUTarget.TOTAL: (MpRuMetric.TOTAL_WEIGHTS_ACTIVATION_SIZE, MpRuAggregation.TOTAL),
+                        RUTarget.BOPS: (MpRuMetric.BOPS_COUNT, MpRuAggregation.SUM)}
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py`

 * *Files 19% similar despite different names*

```diff
@@ -24,153 +24,167 @@
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
 from model_compression_toolkit.logger import Logger
 
 
-def weights_size_kpi(mp_cfg: List[int],
-                     graph: Graph,
-                     fw_info: FrameworkInfo,
-                     fw_impl: FrameworkImplementation) -> np.ndarray:
+def weights_size_utilization(mp_cfg: List[int],
+                             graph: Graph,
+                             fw_info: FrameworkInfo,
+                             fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective weights' memory size for the given weight configurable node,
+    Computes a resource utilization vector with the respective weights' memory size for the given weight configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation (not used in this method).
 
     Returns: A vector of node's weights memory sizes.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     weights_memory = []
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
-    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes()]
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
+    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes(fw_info)]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes resource utilization
+        # TODO: when enabling multiple attribute quantization by default (currently,
+        #  only kernel quantization is enabled) we should include other attributes memory in the sum of all
+        #  weights memory (when quantized to their default 8-bit, non-configurable).
+        #  When implementing this, we should just go over all attributes in the node instead of counting only kernels.
         for n in graph.nodes:
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is None:
+                continue
             non_configurable_node = n.name not in weights_mp_nodes \
-                                    and n.has_weights_quantization_enabled_candidate() \
                                     and not n.reuse \
-                                    and n.is_all_weights_candidates_equal()
+                                    and n.is_all_weights_candidates_equal(kernel_attr)
 
             if non_configurable_node:
-                node_nbits = n.candidates_quantization_cfg[0].weights_quantization_cfg.weights_n_bits
+                node_nbits = (n.candidates_quantization_cfg[0].weights_quantization_cfg
+                              .get_attr_config(kernel_attr).weights_n_bits)
                 node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
                 weights_memory.append(node_weights_memory_in_bytes)
     else:
-        # Go over configurable all nodes that should be taken into consideration when computing the weights KPI.
-        for n in graph.get_sorted_weights_configurable_nodes():
+        # Go over configurable all nodes that should be taken into consideration when computing the weights
+        # resource utilization.
+        for n in graph.get_sorted_weights_configurable_nodes(fw_info):
+            # Only nodes with kernel op can be considered configurable
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
             node_idx = mp_nodes.index(n.name)
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
-            node_nbits = node_qc.weights_quantization_cfg.weights_n_bits
+            node_nbits = node_qc.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
 
             node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
 
             weights_memory.append(node_weights_memory_in_bytes)
 
     return np.array(weights_memory)
 
 
-def activation_output_size_kpi(mp_cfg: List[int],
-                               graph: Graph,
-                               fw_info: FrameworkInfo,
-                               fw_impl: FrameworkImplementation) -> np.ndarray:
+def activation_output_size_utilization(mp_cfg: List[int],
+                                       graph: Graph,
+                                       fw_info: FrameworkInfo,
+                                       fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective output memory size for each activation configurable node,
+    Computes a resource utilization vector with the respective output memory size for each activation configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize)
             (not used in this method).
         fw_impl: FrameworkImplementation object with specific framework methods implementation(not used in this method).
 
     Returns: A vector of node's activation memory sizes.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     activation_memory = []
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
     activation_mp_nodes = [n.name for n in graph.get_sorted_activation_configurable_nodes()]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes resource utilization
         for n in graph.nodes:
             non_configurable_node = n.name not in activation_mp_nodes \
                                     and n.has_activation_quantization_enabled_candidate() \
                                     and n.is_all_activation_candidates_equal()
 
             if non_configurable_node:
                 node_nbits = n.candidates_quantization_cfg[0].activation_quantization_cfg.activation_n_bits
                 node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_nbits)
                 activation_memory.append(node_activation_memory_in_bytes)
     else:
-        # Go over all nodes that should be taken into consideration when computing the weights KPI.
+        # Go over all nodes that should be taken into consideration when computing the weights memory utilization.
         for n in graph.get_sorted_activation_configurable_nodes():
             node_idx = mp_nodes.index(n.name)
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
             node_nbits = node_qc.activation_quantization_cfg.activation_n_bits
 
             node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_nbits)
 
             activation_memory.append(node_activation_memory_in_bytes)
 
     return np.array(activation_memory)
 
 
-def total_weights_activation_kpi(mp_cfg: List[int],
-                                 graph: Graph,
-                                 fw_info: FrameworkInfo,
-                                 fw_impl: FrameworkImplementation) -> np.ndarray:
+def total_weights_activation_utilization(mp_cfg: List[int],
+                                         graph: Graph,
+                                         fw_info: FrameworkInfo,
+                                         fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes KPIs tensor with the respective weights size and output memory size for each activation configurable node,
+    Computes resource utilization tensor with the respective weights size and output memory size for each activation configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize)
             (not used in this method).
         fw_impl: FrameworkImplementation object with specific framework methods implementation(not used in this method).
 
     Returns: A 2D tensor of nodes' weights memory sizes and activation output memory size.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     weights_activation_memory = []
-    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes()]
+    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes(fw_info)]
     activation_mp_nodes = [n.name for n in graph.get_sorted_activation_configurable_nodes()]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes utilization
         for n in graph.nodes:
 
             non_configurable = False
             node_weights_memory_in_bytes, node_activation_memory_in_bytes = 0, 0
 
             # Non-configurable Weights
-            is_non_configurable_weights = n.name not in weights_mp_nodes and \
-                                          n.has_weights_quantization_enabled_candidate() and \
-                                          n.is_all_weights_candidates_equal() and \
-                                          not n.reuse
-
-            if is_non_configurable_weights:
-                node_nbits = n.candidates_quantization_cfg[0].weights_quantization_cfg.weights_n_bits
-                node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
-                non_configurable = True
+            # TODO: currently considering only kernel attributes in weights memory utilization.
+            #  When enabling multi-attribute quantization we need to modify this method to count all attributes.
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is not None:
+                is_non_configurable_weights = n.name not in weights_mp_nodes and \
+                                              n.is_all_weights_candidates_equal(kernel_attr) and \
+                                              not n.reuse
+
+                if is_non_configurable_weights:
+                    node_nbits = (n.candidates_quantization_cfg[0].weights_quantization_cfg
+                                  .get_attr_config(kernel_attr).weights_n_bits)
+                    node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
+                    non_configurable = True
 
             # Non-configurable Activation
             is_non_configurable_activation = n.name not in activation_mp_nodes and \
                                              n.has_activation_quantization_enabled_candidate() and \
                                              n.is_all_activation_candidates_equal()
 
             if is_non_configurable_activation:
@@ -179,116 +193,121 @@
                 non_configurable = True
 
             if non_configurable:
                 weights_activation_memory.append(
                     np.array([node_weights_memory_in_bytes, node_activation_memory_in_bytes]))
     else:
         # Go over all nodes that should be taken into consideration when computing the weights or
-        # activation KPI (all configurable nodes).
-        for node_idx, n in enumerate(graph.get_configurable_sorted_nodes()):
+        # activation memory utilization (all configurable nodes).
+        for node_idx, n in enumerate(graph.get_configurable_sorted_nodes(fw_info)):
+            # TODO: currently considering only kernel attributes in weights memory utilization. When enabling multi-attribute
+            #  quantization we need to modify this method to count all attributes.
+
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
-            node_weights_nbits = node_qc.weights_quantization_cfg.weights_n_bits
-            node_activation_nbits = node_qc.activation_quantization_cfg.activation_n_bits
 
             # Compute node's weights memory (if no weights to quantize then set to 0)
             node_weights_memory_in_bytes = 0
-            if n.is_weights_quantization_enabled() and not n.is_all_weights_candidates_equal():
-                node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_weights_nbits, fw_info)
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is not None:
+                if n.is_weights_quantization_enabled(kernel_attr) and not n.is_all_weights_candidates_equal(kernel_attr):
+                    node_weights_nbits = node_qc.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                    node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_weights_nbits, fw_info)
 
             # Compute node's activation memory (if node's activation are not being quantized then set to 0)
+            node_activation_nbits = node_qc.activation_quantization_cfg.activation_n_bits
             node_activation_memory_in_bytes = 0
             if n.is_activation_quantization_enabled() and not n.is_all_activation_candidates_equal():
                 node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_activation_nbits)
 
             weights_activation_memory.append(np.array([node_weights_memory_in_bytes, node_activation_memory_in_bytes]))
 
     return np.array(weights_activation_memory)
 
 
-def bops_kpi(mp_cfg: List[int],
-             graph: Graph,
-             fw_info: FrameworkInfo,
-             fw_impl: FrameworkImplementation,
-             set_constraints: bool = True) -> np.ndarray:
+def bops_utilization(mp_cfg: List[int],
+                     graph: Graph,
+                     fw_info: FrameworkInfo,
+                     fw_impl: FrameworkImplementation,
+                     set_constraints: bool = True) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective bit-operations (BOPS) count for each configurable node,
+    Computes a resource utilization vector with the respective bit-operations (BOPS) count for each configurable node,
     according to the given mixed-precision configuration of a virtual graph with composed nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A vector of node's BOPS count.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
 
     if not set_constraints:
-        return _bops_kpi(mp_cfg,
-                         graph,
-                         fw_info,
-                         fw_impl)
+        return _bops_utilization(mp_cfg,
+                                 graph,
+                                 fw_info,
+                                 fw_impl)
 
-    # BOPs KPI method considers non-configurable nodes, therefore, it doesn't need separate implementation
+    # BOPs utilization method considers non-configurable nodes, therefore, it doesn't need separate implementation
     # for non-configurable nodes for setting a constraint (no need for separate implementation for len(mp_cfg) = 0).
 
     virtual_bops_nodes = [n for n in graph.get_topo_sorted_nodes() if isinstance(n, VirtualActivationWeightsNode)]
 
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
     bops = [n.get_bops_count(fw_impl, fw_info, candidate_idx=_get_node_cfg_idx(n, mp_cfg, mp_nodes)) for n in virtual_bops_nodes]
 
     return np.array(bops)
 
 
-def _bops_kpi(mp_cfg: List[int],
-              graph: Graph,
-              fw_info: FrameworkInfo,
-              fw_impl: FrameworkImplementation) -> np.ndarray:
+def _bops_utilization(mp_cfg: List[int],
+                      graph: Graph,
+                      fw_info: FrameworkInfo,
+                      fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective bit-operations (BOPS) count for each configurable node,
+    Computes a resource utilization vector with the respective bit-operations (BOPS) count for each configurable node,
     according to the given mixed-precision configuration of an original graph.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
 
     Returns: A vector of node's BOPS count.
 
     """
 
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
 
-    # Go over all nodes that should be taken into consideration when computing the BOPS KPI.
+    # Go over all nodes that should be taken into consideration when computing the BOPS utilization.
     bops = []
     for n in graph.get_topo_sorted_nodes():
-        if n.has_weights_to_quantize(fw_info):
+        if n.has_kernel_weight_to_quantize(fw_info):
             # If node doesn't have weights then its MAC count is 0, and we shouldn't consider it in the BOPS count.
             incoming_edges = graph.incoming_edges(n, sort_by_attr=EDGE_SINK_INDEX)
             if len(incoming_edges) != 1:
-                Logger.critical(f"Can't compute BOPS metric for node {n.name} with multiple inputs.")  # pragma: no cover
-
+                Logger.critical(f"Unable to compute BOPS metric for node {n.name} due to multiple inputs.")  # pragma: no cover
             input_activation_node = incoming_edges[0].source_node
             if len(graph.out_edges(input_activation_node)) > 1:
                 # In the case where the activation node has multiple outgoing edges
-                # we don't consider this edge in the BOPS KPI calculation
+                # we don't consider this edge in the BOPS utilization calculation
                 continue
 
             input_activation_node_cfg = input_activation_node.candidates_quantization_cfg[_get_node_cfg_idx(input_activation_node, mp_cfg, mp_nodes)]
 
             node_mac = fw_impl.get_node_mac_operations(n, fw_info)
 
             node_qc = n.candidates_quantization_cfg[_get_node_cfg_idx(n, mp_cfg, mp_nodes)]
-            node_weights_nbits = node_qc.weights_quantization_cfg.weights_n_bits if \
-                node_qc.weights_quantization_cfg.enable_weights_quantization else FLOAT_BITWIDTH
+            kenrel_node_qc = node_qc.weights_quantization_cfg.get_attr_config(fw_info.get_kernel_op_attributes(n.type)[0])
+            node_weights_nbits = kenrel_node_qc.weights_n_bits if \
+                kenrel_node_qc.enable_weights_quantization else FLOAT_BITWIDTH
             input_activation_nbits = input_activation_node_cfg.activation_quantization_cfg.activation_n_bits if \
                 input_activation_node_cfg.activation_quantization_cfg.enable_activation_quantization else FLOAT_BITWIDTH
 
             node_bops = node_weights_nbits * input_activation_nbits * node_mac
             bops.append(node_bops)
 
     return np.array(bops)
@@ -315,15 +334,15 @@
         assert len(node.candidates_quantization_cfg) > 0, \
             "Any node should have at least one candidate configuration."
         return 0
 
 
 def _get_origin_weights_node(n: BaseNode) -> BaseNode:
     """
-    In case we run a KPI computation on a virtual graph,
+    In case we run a resource utilization computation on a virtual graph,
     this method is used to retrieve the original node out of a virtual weights node,
 
     Args:
         n: A possibly virtual node.
 
     Returns: A node from the original (non-virtual) graph which the given node represents.
 
@@ -335,15 +354,15 @@
         return n.origin_node
 
     return n
 
 
 def _get_origin_activation_node(n: BaseNode) -> BaseNode:
     """
-    In case we run a KPI computation on a virtual graph,
+    In case we run a resource utilization computation on a virtual graph,
     this method is used to retrieve the original node out of a virtual activation node,
 
     Args:
         n: A possibly virtual node.
 
     Returns: A node from the original (non-virtual) graph which the given node represents.
 
@@ -394,29 +413,29 @@
 
     origin_node = _get_origin_activation_node(n)
     node_output_size = origin_node.get_total_output_params()
 
     return node_output_size * node_nbits / BITS_TO_BYTES
 
 
-class MpKpiMetric(Enum):
+class MpRuMetric(Enum):
     """
-    Defines kpi computation functions that can be used to compute KPI for a given target for a given mp config.
-    The enum values can be used to call a function on a set of arguments.
+    Defines resource utilization computation functions that can be used to compute bops_utilization for a given target
+    for a given mp config. The enum values can be used to call a function on a set of arguments.
 
-     WEIGHTS_SIZE - applies the weights_size_kpi function
+     WEIGHTS_SIZE - applies the weights_size_utilization function
 
-     ACTIVATION_OUTPUT_SIZE - applies the activation_output_size_kpi function
+     ACTIVATION_OUTPUT_SIZE - applies the activation_output_size_utilization function
 
-     TOTAL_WEIGHTS_ACTIVATION_SIZE - applies the total_weights_activation_kpi function
+     TOTAL_WEIGHTS_ACTIVATION_SIZE - applies the total_weights_activation_utilization function
 
-     BOPS_COUNT - applies the bops_kpi function
+     BOPS_COUNT - applies the bops_utilization function
 
     """
 
-    WEIGHTS_SIZE = partial(weights_size_kpi)
-    ACTIVATION_OUTPUT_SIZE = partial(activation_output_size_kpi)
-    TOTAL_WEIGHTS_ACTIVATION_SIZE = partial(total_weights_activation_kpi)
-    BOPS_COUNT = partial(bops_kpi)
+    WEIGHTS_SIZE = partial(weights_size_utilization)
+    ACTIVATION_OUTPUT_SIZE = partial(activation_output_size_utilization)
+    TOTAL_WEIGHTS_ACTIVATION_SIZE = partial(total_weights_activation_utilization)
+    BOPS_COUNT = partial(bops_utilization)
 
     def __call__(self, *args):
         return self.value(*args)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py`

 * *Files 23% similar despite different names*

```diff
@@ -14,18 +14,19 @@
 # ==============================================================================
 
 import copy
 from enum import Enum
 import numpy as np
 from typing import List, Callable, Dict
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_functions_mapping import kpi_functions_mapping
+from model_compression_toolkit.core.common.hessian import HessianInfoService
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_functions_mapping import ru_functions_mapping
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.mixed_precision.search_methods.linear_programming import \
     mp_integer_programming_search
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.solution_refinement_procedure import \
     greedy_solution_refinement_procedure
@@ -42,86 +43,90 @@
 search_methods = {
     BitWidthSearchMethod.INTEGER_PROGRAMMING: mp_integer_programming_search}
 
 
 def search_bit_width(graph_to_search_cfg: Graph,
                      fw_info: FrameworkInfo,
                      fw_impl: FrameworkImplementation,
-                     target_kpi: KPI,
-                     mp_config: MixedPrecisionQuantizationConfigV2,
+                     target_resource_utilization: ResourceUtilization,
+                     mp_config: MixedPrecisionQuantizationConfig,
                      representative_data_gen: Callable,
-                     search_method: BitWidthSearchMethod = BitWidthSearchMethod.INTEGER_PROGRAMMING) -> List[int]:
+                     search_method: BitWidthSearchMethod = BitWidthSearchMethod.INTEGER_PROGRAMMING,
+                     hessian_info_service: HessianInfoService=None) -> List[int]:
     """
     Search for an MP configuration for a given graph. Given a search_method method (by default, it's linear
     programming), we use the sensitivity_evaluator object that provides a function to compute an
     evaluation for the expected sensitivity for a bit-width configuration.
-    Then, and after computing the KPI for each node in the graph for each bit-width in the search space,
-    we search for the optimal solution, given some target_kpi, the solution should fit.
-    target_kpi have to be passed. If it was not passed, the facade is not supposed to get here by now.
+    Then, and after computing the resource utilization for each node in the graph for each bit-width in the search space,
+    we search for the optimal solution, given some target_resource_utilization, the solution should fit.
+    target_resource_utilization have to be passed. If it was not passed, the facade is not supposed to get here by now.
 
     Args:
         graph_to_search_cfg: Graph to search a MP configuration for.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
-        target_kpi: Target KPI to bound our feasible solution space s.t the configuration does not violate it.
+        target_resource_utilization: Target Resource Utilization to bound our feasible solution space s.t the configuration does not violate it.
         mp_config: Mixed-precision quantization configuration.
         representative_data_gen: Dataset to use for retrieving images for the models inputs.
         search_method: BitWidthSearchMethod to define which searching method to use.
+        hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
 
     Returns:
         A MP configuration for the graph (list of integers, where the index in the list, is the node's
         index in the graph, when the graph is topology sorted, and the value in this index is the
         bit-width index on the node).
 
     """
 
-    # target_kpi have to be passed. If it was not passed, the facade is not supposed to get here by now.
-    if target_kpi is None:
-        Logger.critical('Target KPI have to be passed for search_methods bit-width configuration')  # pragma: no cover
+    # target_resource_utilization have to be passed. If it was not passed, the facade is not supposed to get here by now.
+    if target_resource_utilization is None:
+        Logger.critical("Target ResourceUtilization is required for the bit-width search method's configuration.")  # pragma: no cover
 
     # Set graph for MP search
     graph = copy.deepcopy(graph_to_search_cfg)  # Copy graph before searching
-    if target_kpi.bops < np.inf:
-        # Since Bit-operations count target KPI is set, we need to reconstruct the graph for the MP search
+    if target_resource_utilization.bops < np.inf:
+        # Since Bit-operations count target resource utilization is set, we need to reconstruct the graph for the MP search
         graph = substitute(graph, fw_impl.get_substitutions_virtual_weights_activation_coupling())
 
     # If we only run weights compression with MP than no need to consider activation quantization when computing the
     # MP metric (it adds noise to the computation)
-    disable_activation_for_metric = (target_kpi.weights_memory < np.inf and
-                                    (target_kpi.activation_memory == np.inf and
-                                     target_kpi.total_memory == np.inf and
-                                     target_kpi.bops == np.inf)) or graph_to_search_cfg.is_single_activation_cfg()
+    disable_activation_for_metric = (target_resource_utilization.weights_memory < np.inf and
+                                    (target_resource_utilization.activation_memory == np.inf and
+                                     target_resource_utilization.total_memory == np.inf and
+                                     target_resource_utilization.bops == np.inf)) or graph_to_search_cfg.is_single_activation_cfg()
 
     # Set Sensitivity Evaluator for MP search. It should always work with the original MP graph,
-    # even if a virtual graph was created (and is used only for BOPS KPI computation purposes)
+    # even if a virtual graph was created (and is used only for BOPS utilization computation purposes)
     se = fw_impl.get_sensitivity_evaluator(
         graph_to_search_cfg,
         mp_config,
         representative_data_gen=representative_data_gen,
         fw_info=fw_info,
-        disable_activation_for_metric=disable_activation_for_metric)
+        disable_activation_for_metric=disable_activation_for_metric,
+        hessian_info_service=hessian_info_service)
 
-    # Each pair of (KPI method, KPI aggregation) should match to a specific provided kpi target
-    kpi_functions = kpi_functions_mapping
+    # Each pair of (resource utilization method, resource utilization aggregation) should match to a specific
+    # provided target resource utilization
+    ru_functions = ru_functions_mapping
 
     # Instantiate a manager object
     search_manager = MixedPrecisionSearchManager(graph,
                                                  fw_info,
                                                  fw_impl,
                                                  se,
-                                                 kpi_functions,
-                                                 target_kpi,
+                                                 ru_functions,
+                                                 target_resource_utilization,
                                                  original_graph=graph_to_search_cfg)
 
     if search_method in search_methods:  # Get a specific search function
         search_method_fn = search_methods.get(search_method)
     else:
         raise NotImplemented  # pragma: no cover
 
     # Search for the desired mixed-precision configuration
     result_bit_cfg = search_method_fn(search_manager,
-                                      target_kpi)
+                                      target_resource_utilization)
 
     if mp_config.refine_mp_solution:
-        result_bit_cfg = greedy_solution_refinement_procedure(result_bit_cfg, search_manager, target_kpi)
+        result_bit_cfg = greedy_solution_refinement_procedure(result_bit_cfg, search_manager, target_resource_utilization)
 
     return result_bit_cfg
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py`

 * *Files 13% similar despite different names*

```diff
@@ -19,78 +19,78 @@
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPITarget, KPI
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import RUTarget, ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 
 
 class MixedPrecisionSearchManager:
     """
     Class to wrap and manage the search process of a mixed-precision configuration.
     """
 
     def __init__(self,
                  graph: Graph,
                  fw_info: FrameworkInfo,
                  fw_impl: FrameworkImplementation,
                  sensitivity_evaluator: SensitivityEvaluation,
-                 kpi_functions: Dict[KPITarget, Tuple[MpKpiMetric, MpKpiAggregation]],
-                 target_kpi: KPI,
+                 ru_functions: Dict[RUTarget, Tuple[MpRuMetric, MpRuAggregation]],
+                 target_resource_utilization: ResourceUtilization,
                  original_graph: Graph = None):
         """
 
         Args:
             graph: Graph to search for its MP configuration.
             fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
             fw_impl: FrameworkImplementation object with specific framework methods implementation.
             sensitivity_evaluator: A SensitivityEvaluation which provides a function that evaluates the sensitivity of
                 a bit-width configuration for the MP model.
-            kpi_functions: A dictionary with pairs of (MpKpiMethod, MpKpiAggregationMethod) mapping a KPITarget to
-                a couple of kpi metric function and kpi aggregation function.
-            target_kpi: Target KPI to bound our feasible solution space s.t the configuration does not violate it.
-            original_graph: In case we have a search over a virtual graph (if we have BOPS KPI target), then this argument
+            ru_functions: A dictionary with pairs of (MpRuMethod, MpRuAggregationMethod) mapping a RUTarget to
+                a couple of resource utilization metric function and resource utilization aggregation function.
+            target_resource_utilization: Target Resource Utilization to bound our feasible solution space s.t the configuration does not violate it.
+            original_graph: In case we have a search over a virtual graph (if we have BOPS utilization target), then this argument
                 will contain the original graph (for config reconstruction purposes).
         """
 
         self.graph = graph
         self.original_graph = graph if original_graph is None else original_graph
         self.fw_info = fw_info
         self.fw_impl = fw_impl
         self.sensitivity_evaluator = sensitivity_evaluator
         self.layer_to_bitwidth_mapping = self.get_search_space()
         self.compute_metric_fn = self.get_sensitivity_metric()
 
-        self.compute_kpi_functions = kpi_functions
-        self.target_kpi = target_kpi
-        self.min_kpi_config = self.graph.get_min_candidates_config()
-        self.max_kpi_config = self.graph.get_max_candidates_config()
-        self.min_kpi = self.compute_min_kpis()
-        self.non_conf_kpi_dict = self._non_configurable_nodes_kpi()
+        self.compute_ru_functions = ru_functions
+        self.target_resource_utilization = target_resource_utilization
+        self.min_ru_config = self.graph.get_min_candidates_config(fw_info)
+        self.max_ru_config = self.graph.get_max_candidates_config(fw_info)
+        self.min_ru = self.compute_min_ru()
+        self.non_conf_ru_dict = self._non_configurable_nodes_ru()
 
         self.config_reconstruction_helper = ConfigReconstructionHelper(virtual_graph=self.graph,
                                                                        original_graph=self.original_graph)
 
     def get_search_space(self) -> Dict[int, List[int]]:
         """
         The search space is a mapping from a node's index to a list of integers (possible bitwidths candidates indeces
         for the node).
 
         Returns:
             The entire search space of the graph.
         """
 
         indices_mapping = {}
-        nodes_to_configure = self.graph.get_configurable_sorted_nodes()
+        nodes_to_configure = self.graph.get_configurable_sorted_nodes(self.fw_info)
         for idx, n in enumerate(nodes_to_configure):
             # For each node, get all possible bitwidth indices for it
             # (which is a list from 0 to the length of the candidates mp_config list of the node).
             indices_mapping[idx] = list(range(len(n.candidates_quantization_cfg)))  # all search_methods space
         return indices_mapping
 
     def get_sensitivity_metric(self) -> Callable:
@@ -102,120 +102,122 @@
 
         """
         # Get from the framework an evaluation function on how a MP configuration,
         # affects the expected loss.
 
         return self.sensitivity_evaluator.compute_metric
 
-    def compute_min_kpis(self) -> Dict[KPITarget, np.ndarray]:
+    def compute_min_ru(self) -> Dict[RUTarget, np.ndarray]:
         """
-        Computes a KPIs vector with the values matching to the minimal mp configuration
+        Computes a resource utilization vector with the values matching to the minimal mp configuration
         (i.e., each node is configured with the quantization candidate that would give the minimal size of the
-        node's KPI).
-        The method computes the minimal KPIs vector for each kpi target.
+        node's resource utilization).
+        The method computes the minimal resource utilization vector for each target resource utilization.
 
-        Returns: A dictionary mapping each kpi target to its respective minimal KPIs values.
+        Returns: A dictionary mapping each target resource utilization to its respective minimal 
+        resource utilization values.
 
         """
-        min_kpis = {}
-        for kpi_target, kpi_fns in self.compute_kpi_functions.items():
-            # kpi_fns is a pair of kpi computation method and kpi aggregation method (in this method we only need
-            # the first one)
-            min_kpis[kpi_target] = kpi_fns[0](self.min_kpi_config, self.graph, self.fw_info, self.fw_impl)
+        min_ru = {}
+        for ru_target, ru_fns in self.compute_ru_functions.items():
+            # ru_fns is a pair of resource utilization computation method and 
+            # resource utilization aggregation method (in this method we only need the first one)
+            min_ru[ru_target] = ru_fns[0](self.min_ru_config, self.graph, self.fw_info, self.fw_impl)
 
-        return min_kpis
+        return min_ru
 
-    def compute_kpi_matrix(self, target: KPITarget) -> np.ndarray:
+    def compute_resource_utilization_matrix(self, target: RUTarget) -> np.ndarray:
         """
-        Computes and builds a KPIs matrix, to be used for the mixed-precision search problem formalization.
+        Computes and builds a resource utilization matrix, to be used for the mixed-precision search problem formalization.
         The matrix is constructed as follows (for a given target):
-        - Each row represents the set of KPI values for a specific KPI measure (number of rows should be equal to the
-            length of the output of the respective target compute_kpi function).
-        - Each entry in a specific column represents the KPI value of a given configuration (single layer is configured
-            with specific candidate, all other layer are at the minimal KPI configuration) for the KPI measure of the
-            respective row.
+        - Each row represents the set of resource utilization values for a specific resource utilization 
+            measure (number of rows should be equal to the length of the output of the respective target compute_ru function).
+        - Each entry in a specific column represents the resource utilization value of a given configuration 
+            (single layer is configured with specific candidate, all other layer are at the minimal resource 
+            utilization configuration) for the resource utilization measure of the respective row.
 
         Args:
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The resource target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: A KPI matrix.
+        Returns: A resource utilization matrix.
 
         """
-        assert isinstance(target, KPITarget), f"{target} is not a valid KPI target"
+        assert isinstance(target, RUTarget), f"{target} is not a valid resource target"
 
-        configurable_sorted_nodes = self.graph.get_configurable_sorted_nodes()
+        configurable_sorted_nodes = self.graph.get_configurable_sorted_nodes(self.fw_info)
 
-        kpi_matrix = []
+        ru_matrix = []
         for c, c_n in enumerate(configurable_sorted_nodes):
             for candidate_idx in range(len(c_n.candidates_quantization_cfg)):
-                if candidate_idx == self.min_kpi_config[c]:
-                    # skip KPI computation for min configuration. Since we compute the difference from min_kpi it'll
+                if candidate_idx == self.min_ru_config[c]:
+                    # skip ru computation for min configuration. Since we compute the difference from min_ru it'll
                     # always be 0 for all entries in the results vector.
-                    candidate_kpis = np.zeros(shape=self.min_kpi[target].shape)
+                    candidate_rus = np.zeros(shape=self.min_ru[target].shape)
                 else:
-                    candidate_kpis = self.compute_candidate_relative_kpis(c, candidate_idx, target)
-                kpi_matrix.append(np.asarray(candidate_kpis))
+                    candidate_rus = self.compute_candidate_relative_ru(c, candidate_idx, target)
+                ru_matrix.append(np.asarray(candidate_rus))
 
-        # We need to transpose the calculated kpi matrix to allow later multiplication with
+        # We need to transpose the calculated ru matrix to allow later multiplication with
         # the indicators' diagonal matrix.
         # We only move the first axis (num of configurations) to be last,
         # the remaining axes include the metric specific nodes (rows dimension of the new tensor)
-        # and the kpi metric values (if they are non-scalars)
-        np_kpi_matrix = np.array(kpi_matrix)
-        return np.moveaxis(np_kpi_matrix, source=0, destination=len(np_kpi_matrix.shape) - 1)
-
-    def compute_candidate_relative_kpis(self,
-                                        conf_node_idx: int,
-                                        candidate_idx: int,
-                                        target: KPITarget) -> np.ndarray:
-        """
-        Computes a KPIs vector for a given candidates of a given configurable node, i.e., the matching KPI vector
-        which is obtained by computing the given target's KPI function on a minimal configuration in which the given
+        # and the ru metric values (if they are non-scalars)
+        np_ru_matrix = np.array(ru_matrix)
+        return np.moveaxis(np_ru_matrix, source=0, destination=len(np_ru_matrix.shape) - 1)
+
+    def compute_candidate_relative_ru(self,
+                                      conf_node_idx: int,
+                                      candidate_idx: int,
+                                      target: RUTarget) -> np.ndarray:
+        """
+        Computes a resource utilization vector for a given candidates of a given configurable node, 
+        i.e., the matching resource utilization vector which is obtained by computing the given target's 
+        resource utilization function on a minimal configuration in which the given
         layer's candidates is changed to the new given one.
-        The result is normalized by subtracting the target's minimal KPIs vector.
+        The result is normalized by subtracting the target's minimal resource utilization vector.
 
         Args:
             conf_node_idx: The index of a node in a sorted configurable nodes list.
             candidate_idx: The index of a node's quantization configuration candidate.
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Normalized node's KPIs vector
+        Returns: Normalized node's resource utilization vector
 
         """
-        return self.compute_node_kpi_for_candidate(conf_node_idx, candidate_idx, target) - \
-               self.get_min_target_kpi(target)
+        return self.compute_node_ru_for_candidate(conf_node_idx, candidate_idx, target) - \
+               self.get_min_target_resource_utilization(target)
 
-    def get_min_target_kpi(self, target: KPITarget) -> np.ndarray:
+    def get_min_target_resource_utilization(self, target: RUTarget) -> np.ndarray:
         """
-        Returns the minimal KPIs vector (pre-calculated on initialization) of a specific target.
+        Returns the minimal resource utilization vector (pre-calculated on initialization) of a specific target.
 
         Args:
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Minimal KPIs vector.
+        Returns: Minimal resource utilization vector.
 
         """
-        return self.min_kpi[target]
+        return self.min_ru[target]
 
-    def compute_node_kpi_for_candidate(self, conf_node_idx: int, candidate_idx: int, target: KPITarget) -> np.ndarray:
+    def compute_node_ru_for_candidate(self, conf_node_idx: int, candidate_idx: int, target: RUTarget) -> np.ndarray:
         """
-        Computes a KPIs vector after replacing the given node's configuration candidate in the minimal
+        Computes a resource utilization vector after replacing the given node's configuration candidate in the minimal
         target configuration with the given candidate index.
 
         Args:
             conf_node_idx: The index of a node in a sorted configurable nodes list.
-            candidate_idx: Quantization config candidate to be used for the node's KPI computation.
-            target: The target for which the KPI is calculated (a KPITarget value).
+            candidate_idx: Quantization config candidate to be used for the node's resource utilization computation.
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Node's KPIs vector.
+        Returns: Node's resource utilization vector.
 
         """
-        return self.compute_kpi_functions[target][0](
+        return self.compute_ru_functions[target][0](
             self.replace_config_in_index(
-                self.min_kpi_config,
+                self.min_ru_config,
                 conf_node_idx,
                 candidate_idx),
             self.graph,
             self.fw_info,
             self.fw_impl)
 
     @staticmethod
@@ -232,72 +234,98 @@
         Returns: A new mixed-precision configuration.
 
         """
         updated_cfg = mp_cfg.copy()
         updated_cfg[idx] = value
         return updated_cfg
 
-    def _non_configurable_nodes_kpi(self) -> Dict[KPITarget, np.ndarray]:
+    def _non_configurable_nodes_ru(self) -> Dict[RUTarget, np.ndarray]:
         """
-        Computes a KPI vector of all non-configurable nodes in the given graph for each of the KPI target.
+        Computes a resource utilization vector of all non-configurable nodes in the given graph for each of the 
+        resource utilization targets.
 
-        Returns: A mapping between a KPITarget and its non-configurable nodes' KPI vector.
+        Returns: A mapping between a RUTarget and its non-configurable nodes' resource utilization vector.
         """
 
-        non_conf_kpi_dict = {}
-        for target, kpi_value in self.target_kpi.get_kpi_dict().items():
-            # Call for the KPI method of the given target - empty quantization configuration list is passed since we
+        non_conf_ru_dict = {}
+        for target, ru_value in self.target_resource_utilization.get_resource_utilization_dict().items():
+            # Call for the ru method of the given target - empty quantization configuration list is passed since we
             # compute for non-configurable nodes
-            if target == KPITarget.BOPS:
-                kpi_vector = None
+            if target == RUTarget.BOPS:
+                ru_vector = None
             else:
-                kpi_vector = self.compute_kpi_functions[target][0]([], self.graph, self.fw_info, self.fw_impl)
+                ru_vector = self.compute_ru_functions[target][0]([], self.graph, self.fw_info, self.fw_impl)
 
-            non_conf_kpi_dict[target] = kpi_vector
+            non_conf_ru_dict[target] = ru_vector
 
-        return non_conf_kpi_dict
+        return non_conf_ru_dict
 
-    def compute_kpi_for_config(self, config: List[int]) -> KPI:
+    def compute_resource_utilization_for_config(self, config: List[int]) -> ResourceUtilization:
         """
-        Computes the KPI values for a given mixed-precision configuration.
+        Computes the resource utilization values for a given mixed-precision configuration.
 
         Args:
             config: A mixed-precision configuration (list of candidates indices)
 
-        Returns: A KPI object with the model's KPI values when quantized with the given config.
+        Returns: A ResourceUtilization object with the model's resource utilization values when quantized 
+        with the given config.
 
         """
 
-        kpis_dict = {}
+        ru_dict = {}
 
-        for kpi_target, kpi_fns in self.compute_kpi_functions.items():
-            # Passing False to kpi methods and aggregations to indicates that the computations
+        for ru_target, ru_fns in self.compute_ru_functions.items():
+            # Passing False to ru methods and aggregations to indicates that the computations
             # are not for constraints setting
-            if kpi_target == KPITarget.BOPS:
-                configurable_nodes_kpi_vector = kpi_fns[0](config, self.original_graph, self.fw_info, self.fw_impl, False)
+            if ru_target == RUTarget.BOPS:
+                configurable_nodes_ru_vector = ru_fns[0](config, self.original_graph, self.fw_info, self.fw_impl, False)
             else:
-                configurable_nodes_kpi_vector = kpi_fns[0](config, self.original_graph, self.fw_info, self.fw_impl)
-            non_configurable_nodes_kpi_vector = self.non_conf_kpi_dict.get(kpi_target)
-            if non_configurable_nodes_kpi_vector is None or len(non_configurable_nodes_kpi_vector) == 0:
-                aggr_kpi = self.compute_kpi_functions[kpi_target][1](configurable_nodes_kpi_vector, False)
+                configurable_nodes_ru_vector = ru_fns[0](config, self.original_graph, self.fw_info, self.fw_impl)
+            non_configurable_nodes_ru_vector = self.non_conf_ru_dict.get(ru_target)
+            if non_configurable_nodes_ru_vector is None or len(non_configurable_nodes_ru_vector) == 0:
+                ru_ru = self.compute_ru_functions[ru_target][1](configurable_nodes_ru_vector, False)
             else:
-                aggr_kpi = self.compute_kpi_functions[kpi_target][1](
-                    np.concatenate([configurable_nodes_kpi_vector, non_configurable_nodes_kpi_vector]), False)
+                ru_ru = self.compute_ru_functions[ru_target][1](
+                    np.concatenate([configurable_nodes_ru_vector, non_configurable_nodes_ru_vector]), False)
+
+            ru_dict[ru_target] = ru_ru[0]
+
+        config_ru = ResourceUtilization()
+        config_ru.set_resource_utilization_by_target(ru_dict)
+        return config_ru
+
+    def finalize_distance_metric(self, layer_to_metrics_mapping: Dict[int, Dict[int, float]]):
+        """
+        Finalizing the distance metric building.
+        The method checks to see if the maximal distance value is larger than a given threshold, and if so,
+        it scales all metric values to prevent possible numerical issues.
+        Modification to the dictionary is done inplace.
 
-            kpis_dict[kpi_target] = aggr_kpi[0]
+        Args:
+            layer_to_metrics_mapping: A mapping between a node index to a mapping between
+            a bitwidth index to a distance value.
+
+        """
+        # normalize metric for numerical stability
 
-        config_kpi = KPI()
-        config_kpi.set_kpi_by_target(kpis_dict)
-        return config_kpi
+        max_dist = max([max([d for b, d in dists.items()]) for layer, dists in layer_to_metrics_mapping.items()])
+        if max_dist >= self.sensitivity_evaluator.quant_config.metric_normalization_threshold:
+            Logger.warning(f"The mixed precision distance metric values indicate a large error in the quantized model."
+                           f"this can cause numerical issues."
+                           f"The program will proceed with mixed precision search after scaling the metric values,"
+                           f"which can lead to unstable results.")
+            for layer, dists in layer_to_metrics_mapping.items():
+                for b, d in dists.items():
+                    layer_to_metrics_mapping[layer][b] /= max_dist
 
 
 class ConfigReconstructionHelper:
     """
     A class to help reconstruct an original mixed-precision configuration from a virtual one,
-    when running mixed-precision search with BOPS KPI.
+    when running mixed-precision search with BOPS utilization.
     It provides a reconstruct_config_from_virtual_graph which allows to translate a bit-width config of a virtual graph
     to a config of the original configurable nodes.
     """
 
     def __init__(self, virtual_graph: Graph, original_graph: Graph):
         """
         Init a ConfigReconstructionHelper object.
@@ -308,17 +336,18 @@
         Args:
             virtual_graph: The virtual graph.
             original_graph: The original graph.
         """
 
         self.virtual_graph = virtual_graph
         self.original_graph = original_graph
+        self.fw_info = original_graph.fw_info
 
-        self.virtual_sorted_nodes_names = self.virtual_graph.get_configurable_sorted_nodes_names()
-        self.origin_sorted_conf_nodes_names = self.original_graph.get_configurable_sorted_nodes_names()
+        self.virtual_sorted_nodes_names = self.virtual_graph.get_configurable_sorted_nodes_names(self.fw_info)
+        self.origin_sorted_conf_nodes_names = self.original_graph.get_configurable_sorted_nodes_names(self.fw_info)
 
         self.origin_node_idx_to_cfg = {}
 
     def _clear_reconstruction_dict(self):
         """
         Clears the origin_node_idx_to_cfg data structure.
         """
@@ -346,31 +375,30 @@
 
         Returns: A mixed-precision configuration (list of candidates indices) of the original graph.
 
         """
 
         if changed_virtual_nodes_idx is not None:
             if original_base_config is None:
-                Logger.critical("Must provide a base original config in order to run config reconstruction for partial"
-                                "set of nodes.")  # pragma: no cover
+                Logger.critical("To run config reconstruction for a partial set of nodes, a base original config must be provided.")  # pragma: no cover
 
             updated_virtual_nodes = \
-                [(idx, self.virtual_graph.get_configurable_sorted_nodes()[idx]) for idx in changed_virtual_nodes_idx]
+                [(idx, self.virtual_graph.get_configurable_sorted_nodes(self.fw_info)[idx]) for idx in changed_virtual_nodes_idx]
             # Iterating only over the virtual nodes that have updated config
             for virtual_node_idx, n in updated_virtual_nodes:
                 self.reconstruct_node_config(n, virtual_mp_cfg, virtual_node_idx)
             # Updating reconstructed config for all other nodes based on provided base_config
-            original_sorted_conf_nodes = self.original_graph.get_configurable_sorted_nodes()
+            original_sorted_conf_nodes = self.original_graph.get_configurable_sorted_nodes(self.fw_info)
             for i in range(len(original_base_config)):
                 if i not in list(self.origin_node_idx_to_cfg.keys()):
                     self.update_config_at_original_idx(n=original_sorted_conf_nodes[i],
                                                        origin_cfg_idx=original_base_config[i])
         else:
             # Reconstruct entire config
-            for virtual_node_idx, n in enumerate(self.virtual_graph.get_configurable_sorted_nodes()):
+            for virtual_node_idx, n in enumerate(self.virtual_graph.get_configurable_sorted_nodes(self.fw_info)):
                 self.reconstruct_node_config(n, virtual_mp_cfg, virtual_node_idx)
 
         res_config = [self.origin_node_idx_to_cfg[key] for key in sorted(self.origin_node_idx_to_cfg.keys())]
         self._clear_reconstruction_dict()
         return res_config
 
     def reconstruct_node_config(self,
@@ -389,17 +417,15 @@
         virtual_cfg_idx = virtual_mp_cfg[virtual_node_idx]
 
         if isinstance(n, VirtualActivationWeightsNode):
             weights_node = n.original_weights_node
             if isinstance(weights_node, VirtualSplitWeightsNode):
                 self.get_activation_for_split_weights(weights_node, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - all weights nodes should be split to weights and activation nodes"
-                             f"in order to construct the virtual graph, but node {n.name} is not of type "
-                             f"VirtualSplitWeightsNode")  # pragma: no cover
+                Logger.critical(f"Virtual graph construction error: Expected all weights nodes to be split into weights and activation nodes. Found node '{n.name}' not split as expected. Every weights node should correspond to a VirtualSplitWeightsNode type.")  # pragma: no cover
 
             activation_node = n.original_activation_node
             if isinstance(activation_node, VirtualSplitActivationNode):
                 self.get_weights_for_split_activation(activation_node, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
                 if activation_node.name in self.origin_sorted_conf_nodes_names:
                     # It is possible that the original activation node is not configurable,
@@ -412,23 +438,21 @@
             predecessor = self.virtual_graph.get_prev_nodes(n)
             assert len(predecessor) == 1  # Sanity check
             predecessor = predecessor[0]
             if len(self.virtual_graph.out_edges(predecessor)) > 1:
                 # It's ok, need to find the node's configuration
                 self.get_activation_for_split_weights(n, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - a weights node is not composed with an activation node,"
-                             f"but its predecessor doesn't have multiple outputs.")  # pragma: no cover
+                Logger.critical(f"Virtual graph configuration error: Expected the predecessor of node '{n.name}' to have multiple outputs when not composed with an activation node.")  # pragma: no cover
         elif isinstance(n, VirtualSplitActivationNode):
             self.get_weights_for_split_activation(n, n, virtual_cfg_idx, virtual_mp_cfg)
         else:
             # Node didn't change in virtual graph - candidates list is similar to original
             if n.name not in self.origin_sorted_conf_nodes_names:
-                Logger.error(f"Node {n.name} appears in virtual graph as configurable, "
-                             f"but is not configurable in the original graph.")  # pragma: no cover
+                Logger.critical(f"Configuration mismatch: Node '{n.name}' is configurable in the virtual graph but not in the original graph. Verify node configurations.")  # pragma: no cover
             origin_idx = self.origin_sorted_conf_nodes_names.index(n.name)
             self.origin_node_idx_to_cfg[origin_idx] = virtual_cfg_idx
 
     def retrieve_weights_only_config(self, weights_node: BaseNode, virtual_node: BaseNode, virtual_cfg_idx: int):
         """
         Retrieves the configuration of an original weights configurable node based on a
         virtual weights configurable node's chosen config idx, and updates (inplace) the origin_cfg_idx mapping dict.
@@ -439,18 +463,20 @@
             virtual_node: The virtual weights configurable node.
             virtual_cfg_idx: The virtual node's chosen config index.
         """
 
         if weights_node.name in self.origin_sorted_conf_nodes_names:
             # It is possible that the original weights node is not configurable,
             # in this case we don't need to retrieve its bit-width config
-            weights_bitwidth = virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg.weights_n_bits
+            kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+            weights_bitwidth = (virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg
+                                .get_attr_config(kernel_attr).weights_n_bits)
             origin_cfg_idx = [i for i, c in
                               enumerate(weights_node.candidates_quantization_cfg) if
-                              c.weights_quantization_cfg.weights_n_bits == weights_bitwidth]
+                              c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth]
 
             self.update_config_at_original_idx(weights_node, origin_cfg_idx[0])
 
     def retrieve_activation_only_config(self, activation_node: BaseNode, virtual_node: BaseNode, virtual_cfg_idx: int):
         """
         Retrieves the configuration of an original activation configurable node based on a
         virtual activation configurable node's chosen config idx, and updates (inplace) the origin_cfg_idx mapping dict.
@@ -491,19 +517,22 @@
             virtual_cfg_idx: The virtual node's chosen config index.
             virtual_mp_cfg: The virtual graph's chosen mp config.
         """
 
         activation_bitwidth = activation_node.candidates_quantization_cfg[virtual_mp_cfg[
             self.virtual_sorted_nodes_names.index(activation_node.name)]].activation_quantization_cfg.activation_n_bits
 
-        weights_bitwidth = virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg.weights_n_bits
+        kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+
+        weights_bitwidth = (virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg
+                            .get_attr_config(kernel_attr).weights_n_bits)
 
         origin_cfg_idx = [i for i, c in
                           enumerate(weights_node.origin_node.candidates_quantization_cfg) if
-                          c.weights_quantization_cfg.weights_n_bits == weights_bitwidth and
+                          c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth and
                           c.activation_quantization_cfg.activation_n_bits == activation_bitwidth]
 
         self.update_config_at_original_idx(weights_node.origin_node, origin_cfg_idx[0])
 
     def retrieve_weights_activation_config(self,
                                            activation_node: BaseNode,
                                            weights_node: BaseNode,
@@ -519,22 +548,25 @@
             activation_node: The virtual node that contains the activation representation of an original node.
             weights_node: The virtual node that contains the weights that matches the activation node in the original graph.
             virtual_node: The virtual node that contains the virtual activation node (either a composed node or a split activation node).
             virtual_cfg_idx: The virtual node's chosen config index.
             virtual_mp_cfg: The virtual graph's chosen mp config.
         """
 
-        weights_bitwidth = weights_node.candidates_quantization_cfg[virtual_mp_cfg[
-            self.virtual_sorted_nodes_names.index(weights_node.name)]].weights_quantization_cfg.weights_n_bits
+        kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+
+        weights_bitwidth = (weights_node.candidates_quantization_cfg[virtual_mp_cfg[
+            self.virtual_sorted_nodes_names.index(weights_node.name)]]
+                            .weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits)
 
         activation_bitwidth = virtual_node.candidates_quantization_cfg[
             virtual_cfg_idx].activation_quantization_cfg.activation_n_bits
 
         origin_cfg_idx = [i for i, c in enumerate(activation_node.origin_node.candidates_quantization_cfg) if
-                          c.weights_quantization_cfg.weights_n_bits == weights_bitwidth and
+                          c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth and
                           c.activation_quantization_cfg.activation_n_bits == activation_bitwidth]
 
         self.update_config_at_original_idx(activation_node.origin_node, origin_cfg_idx[0])
 
     def get_activation_for_split_weights(self,
                                          weights_node: BaseNode,
                                          virtual_node: BaseNode,
@@ -596,16 +628,17 @@
         # This is an activation node that was split, means it has a weights node that should come before it,
         # and we need its configuration in order to reconstruct the original node's configuration.
         matching_weights_node = self.virtual_graph.get_prev_nodes(virtual_node)
         assert len(matching_weights_node) == 1
         weights_node = matching_weights_node[0]
 
         if isinstance(weights_node, VirtualActivationWeightsNode):
-            if weights_node.original_weights_node.is_weights_quantization_enabled() and not \
-                    weights_node.original_weights_node.is_all_weights_candidates_equal():
+            kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+            if weights_node.original_weights_node.is_weights_quantization_enabled(kernel_attr) and not \
+                    weights_node.original_weights_node.is_all_weights_candidates_equal(kernel_attr):
                 assert weights_node.name in self.virtual_sorted_nodes_names  # Sanity check
                 # The original node is both weights and activation configurable
                 self.retrieve_weights_activation_config(activation_node, weights_node, virtual_node, virtual_cfg_idx, virtual_mp_cfg)
             else:
                 # The original node is only activation configurable
                 # activation_node here is a split activation node therefore must have 'origin_node'
                 self.retrieve_activation_only_config(activation_node.origin_node, virtual_node, virtual_cfg_idx)
@@ -616,16 +649,15 @@
             predecessor = self.virtual_graph.get_prev_nodes(weights_node)
             assert len(predecessor) == 1  # Sanity check
             predecessor = predecessor[0]
             if len(self.virtual_graph.out_edges(predecessor)) > 1:
                 # It's ok, need to find the node's configuration
                 self.retrieve_weights_activation_config(activation_node, weights_node, virtual_node, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - a weights node is not composed with an activation node,"
-                             f"but its predecessor doesn't have multiple outputs.")  # pragma: no cover
+                Logger.critical(f"Virtual graph configuration error: Expected the predecessor of node '{n.name}' to have multiple outputs when not composed with an activation node.")  # pragma: no cover
 
     def update_config_at_original_idx(self, n: BaseNode, origin_cfg_idx: int):
         """
         Updates (inplace) the origin_node_idx_to_cfg mapping wit hthe given index for a given original node index
         (in the original graph's sorted configurable nodes list).
 
         Args:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py`

 * *Files 26% similar despite different names*

```diff
@@ -15,59 +15,60 @@
 
 import numpy as np
 from pulp import *
 from tqdm import tqdm
 from typing import Dict, List, Tuple, Callable
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 
 # Limit ILP solver runtime in seconds
 SOLVER_TIME_LIMIT = 60
 
+
 def mp_integer_programming_search(search_manager: MixedPrecisionSearchManager,
-                                  target_kpi: KPI = None) -> List[int]:
+                                  target_resource_utilization: ResourceUtilization = None) -> List[int]:
     """
     Searching and returning a mixed-precision configuration using an ILP optimization solution.
     It first builds a mapping from each layer's index (in the model) to a dictionary that maps the
     bitwidth index to the observed sensitivity of the model when using that bitwidth for that layer.
     Then, it creates a mapping from each node's index (in the graph) to a dictionary
     that maps the bitwidth index to the contribution of configuring this node with this
-    bitwidth to the minimal possible KPI of the model.
+    bitwidth to the minimal possible resource utilization of the model.
     Then, and using these mappings, it builds an LP problem and finds an optimal solution.
     If a solution could not be found, exception is thrown.
 
     Args:
         search_manager: MixedPrecisionSearchManager object to be used for problem formalization.
-        target_kpi: KPI to constrain our LP problem with some resources limitations (like model' weights memory
+        target_resource_utilization: Target resource utilization to constrain our LP problem with some resources limitations (like model' weights memory
         consumption).
 
     Returns:
         The mixed-precision configuration (list of indices. Each indicates the bitwidth index of a node).
 
     """
 
     # Build a mapping from each layer's index (in the model) to a dictionary that maps the
     # bitwidth index to the observed sensitivity of the model when using that bitwidth for that layer.
 
-    if target_kpi is None or search_manager is None:
-        Logger.critical("Can't run mixed precision search with given target_kpi=None or search_manager=None."
-                        "Please provide a valid target_kpi and check the mixed precision parameters values.")
+    if target_resource_utilization is None or search_manager is None:
+        Logger.critical("Invalid parameters: 'target_resource_utilization' and 'search_manager' must not be 'None' "
+                        "for mixed-precision search. Ensure valid inputs are provided.")
 
-    layer_to_metrics_mapping = _build_layer_to_metrics_mapping(search_manager, target_kpi)
+    layer_to_metrics_mapping = _build_layer_to_metrics_mapping(search_manager, target_resource_utilization)
 
     # Init variables to find their values when solving the lp problem.
     layer_to_indicator_vars_mapping, layer_to_objective_vars_mapping = _init_problem_vars(layer_to_metrics_mapping)
 
     # Add all equations and inequalities that define the problem.
     lp_problem = _formalize_problem(layer_to_indicator_vars_mapping,
                                     layer_to_metrics_mapping,
                                     layer_to_objective_vars_mapping,
-                                    target_kpi,
+                                    target_resource_utilization,
                                     search_manager)
 
     # Use default PULP solver. Limit runtime in seconds
     solver = PULP_CBC_CMD(timeLimit=SOLVER_TIME_LIMIT)
     lp_problem.solve(solver=solver)  # Try to solve the problem.
 
     assert lp_problem.status == LpStatusOptimal, Logger.critical(
@@ -77,15 +78,15 @@
     # Take the bitwidth index only if its corresponding indicator is one.
     config = np.asarray(
         [[nbits for nbits, indicator in nbits_to_indicator.items() if indicator.varValue == 1.0] for
          nbits_to_indicator
          in layer_to_indicator_vars_mapping.values()]
     ).flatten()
 
-    if target_kpi.bops < np.inf:
+    if target_resource_utilization.bops < np.inf:
         return search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(config)
     else:
         return config
 
 
 def _init_problem_vars(layer_to_metrics_mapping: Dict[int, Dict[int, float]]) -> Tuple[
     Dict[int, Dict[int, LpVariable]], Dict[int, LpVariable]]:
@@ -118,28 +119,28 @@
 
     return layer_to_indicator_vars_mapping, layer_to_objective_vars_mapping
 
 
 def _formalize_problem(layer_to_indicator_vars_mapping: Dict[int, Dict[int, LpVariable]],
                        layer_to_metrics_mapping: Dict[int, Dict[int, float]],
                        layer_to_objective_vars_mapping: Dict[int, LpVariable],
-                       target_kpi: KPI,
+                       target_resource_utilization: ResourceUtilization,
                        search_manager: MixedPrecisionSearchManager) -> LpProblem:
     """
     Formalize the LP problem by defining all inequalities that define the solution space.
 
     Args:
         layer_to_indicator_vars_mapping: Dictionary that maps each node's index to a dictionary of bitwidth to
         indicator variable.
         layer_to_metrics_mapping: Dictionary that maps each node's index to a dictionary of bitwidth to sensitivity
         evaluation.
         layer_to_objective_vars_mapping: Dictionary that maps each node's index to a bitwidth variable we find its
         value.
-        target_kpi: KPI to reduce our feasible solution space.
-        search_manager: MixedPrecisionSearchManager object to be used for kpi constraints formalization.
+        target_resource_utilization: Target resource utilization to reduce our feasible solution space.
+        search_manager: MixedPrecisionSearchManager object to be used for resource utilization constraints formalization.
 
     Returns:
         The formalized LP problem.
     """
 
     lp_problem = LpProblem()  # minimization problem by default
     lp_problem += lpSum([layer_to_objective_vars_mapping[layer] for layer in
@@ -151,150 +152,160 @@
                              for nbits, indicator in layer_to_indicator_vars_mapping[layer].items()]) == \
                       layer_to_objective_vars_mapping[layer]
 
         # Constraint of only one indicator==1
         lp_problem += lpSum(
             [v for v in layer_to_indicator_vars_mapping[layer].values()]) == 1
 
-    # Bound the feasible solution space with the desired KPI.
-    # Creates separate constraints for weights KPI and activation KPI.
-    if target_kpi is not None:
+    # Bound the feasible solution space with the desired resource utilization values.
+    # Creates separate constraints for weights utilization and activation utilization.
+    if target_resource_utilization is not None:
         indicators = []
         for layer in layer_to_metrics_mapping.keys():
             for _, indicator in layer_to_indicator_vars_mapping[layer].items():
                 indicators.append(indicator)
 
         indicators_arr = np.array(indicators)
         indicators_matrix = np.diag(indicators_arr)
 
-        for target, kpi_value in target_kpi.get_kpi_dict().items():
-            if not np.isinf(kpi_value):
-                non_conf_kpi_vector = None if search_manager.non_conf_kpi_dict is None \
-                    else search_manager.non_conf_kpi_dict.get(target)
-                _add_set_of_kpi_constraints(search_manager=search_manager,
-                                            target=target,
-                                            target_kpi_value=kpi_value,
-                                            indicators_matrix=indicators_matrix,
-                                            lp_problem=lp_problem,
-                                            non_conf_kpi_vector=non_conf_kpi_vector)
+        for target, ru_value in target_resource_utilization.get_resource_utilization_dict().items():
+            if not np.isinf(ru_value):
+                non_conf_ru_vector = None if search_manager.non_conf_ru_dict is None \
+                    else search_manager.non_conf_ru_dict.get(target)
+                _add_set_of_ru_constraints(search_manager=search_manager,
+                                           target=target,
+                                           target_resource_utilization_value=ru_value,
+                                           indicators_matrix=indicators_matrix,
+                                           lp_problem=lp_problem,
+                                           non_conf_ru_vector=non_conf_ru_vector)
     else:  # pragma: no cover
-        raise Logger.critical("Can't run mixed-precision search with given target_kpi=None."
-                              "Please provide a valid target_kpi.")
+        Logger.critical("Unable to execute mixed-precision search: 'target_resource_utilization' is None. "
+                        "A valid 'target_resource_utilization' is required.")
     return lp_problem
 
 
-def _add_set_of_kpi_constraints(search_manager: MixedPrecisionSearchManager,
-                                target: KPITarget,
-                                target_kpi_value: float,
-                                indicators_matrix: np.ndarray,
-                                lp_problem: LpProblem,
-                                non_conf_kpi_vector: np.ndarray):
+def _add_set_of_ru_constraints(search_manager: MixedPrecisionSearchManager,
+                               target: RUTarget,
+                               target_resource_utilization_value: float,
+                               indicators_matrix: np.ndarray,
+                               lp_problem: LpProblem,
+                               non_conf_ru_vector: np.ndarray):
     """
-    Adding a constraint for the Lp problem for the given KPI target.
+    Adding a constraint for the Lp problem for the given target resource utilization.
     The update to the Lp problem object is done inplace.
 
     Args:
-        search_manager:  MixedPrecisionSearchManager object to be used for kpi constraints formalization.
-        target: A KPITarget.
-        target_kpi_value: Target KPI value of the given KPI target for which the constraint is added.
+        search_manager:  MixedPrecisionSearchManager object to be used for resource utilization constraints formalization.
+        target: A RUTarget.
+        target_resource_utilization_value: Target resource utilization value of the given target resource utilization
+        for which the constraint is added.
         indicators_matrix: A diagonal matrix of the Lp problem's indicators.
         lp_problem: An Lp problem object to add constraint to.
-        non_conf_kpi_vector: A non-configurable nodes' KPI vector.
+        non_conf_ru_vector: A non-configurable nodes' resource utilization vector.
 
     """
 
-    kpi_matrix = search_manager.compute_kpi_matrix(target)
-    indicated_kpi_matrix = np.matmul(kpi_matrix, indicators_matrix)
+    ru_matrix = search_manager.compute_resource_utilization_matrix(target)
+    indicated_ru_matrix = np.matmul(ru_matrix, indicators_matrix)
     # Need to re-organize the tensor such that the configurations' axis will be second,
     # and all metric values' axis will come afterword
-    indicated_kpi_matrix = np.moveaxis(indicated_kpi_matrix, source=len(indicated_kpi_matrix.shape) - 1, destination=1)
+    indicated_ru_matrix = np.moveaxis(indicated_ru_matrix, source=len(indicated_ru_matrix.shape) - 1, destination=1)
 
-    # In order to get the result KPI according to a chosen set of indicators, we sum each row in the result matrix.
-    # Each row represents the KPI values for a specific KPI metric, such that only elements corresponding
-    # to a configuration which implied by the set of indicators will have some positive value different than 0
-    # (and will contribute to the total KPI).
-    kpi_sum_vector = np.array([
-        np.sum(indicated_kpi_matrix[i], axis=0) +  # sum of metric values over all configurations in a row
-        search_manager.min_kpi[target][i] for i in range(indicated_kpi_matrix.shape[0])])
-
-    # search_manager.compute_kpi_functions contains a pair of kpi_metric and kpi_aggregation for each kpi target
-    # get aggregated KPI, considering both configurable and non-configurable nodes
-    if non_conf_kpi_vector is None or len(non_conf_kpi_vector) == 0:
-        aggr_kpi = search_manager.compute_kpi_functions[target][1](kpi_sum_vector)
+    # In order to get the result resource utilization according to a chosen set of indicators, we sum each row in
+    # the result matrix. Each row represents the resource utilization values for a specific resource utilization metric,
+    # such that only elements corresponding to a configuration which implied by the set of indicators will have some
+    # positive value different than 0 (and will contribute to the total resource utilization).
+    ru_sum_vector = np.array([
+        np.sum(indicated_ru_matrix[i], axis=0) +  # sum of metric values over all configurations in a row
+        search_manager.min_ru[target][i] for i in range(indicated_ru_matrix.shape[0])])
+
+    # search_manager.compute_ru_functions contains a pair of ru_metric and ru_aggregation for each ru target
+    # get aggregated ru, considering both configurable and non-configurable nodes
+    if non_conf_ru_vector is None or len(non_conf_ru_vector) == 0:
+        aggr_ru = search_manager.compute_ru_functions[target][1](ru_sum_vector)
     else:
-        aggr_kpi = search_manager.compute_kpi_functions[target][1](np.concatenate([kpi_sum_vector, non_conf_kpi_vector]))
+        aggr_ru = search_manager.compute_ru_functions[target][1](np.concatenate([ru_sum_vector, non_conf_ru_vector]))
 
-    for v in aggr_kpi:
+    for v in aggr_ru:
         if isinstance(v, float):
-            if v > target_kpi_value:
-                Logger.critical(f"The model can't be quantized to satisfy target KPI {target.value} with value {target_kpi_value}")  # pragma: no cover
+            if v > target_resource_utilization_value:
+                Logger.critical(
+                    f"The model cannot be quantized to meet the specified target resource utilization {target.value} "
+                    f"with the value {target_resource_utilization_value}.")  # pragma: no cover
         else:
-            lp_problem += v <= target_kpi_value
+            lp_problem += v <= target_resource_utilization_value
 
 
 def _build_layer_to_metrics_mapping(search_manager: MixedPrecisionSearchManager,
-                                    target_kpi: KPI) -> Dict[int, Dict[int, float]]:
+                                    target_resource_utilization: ResourceUtilization,
+                                    eps: float = EPS) -> Dict[int, Dict[int, float]]:
     """
     This function measures the sensitivity of a change in a bitwidth of a layer on the entire model.
     It builds a mapping from a node's index, to its bitwidht's effect on the model sensitivity.
     For each node and some possible node's bitwidth (according to the given search space), we use
     the framework function compute_metric_fn in order to infer
     a batch of images, and compute (using the inference results) the sensitivity metric of
     the configured mixed-precision model.
 
     Args:
         search_manager: MixedPrecisionSearchManager object to be used for problem formalization.
-        target_kpi: KPI to constrain our LP problem with some resources limitations (like model' weights memory
-        consumption).
+        target_resource_utilization: ResourceUtilization to constrain our LP problem with some resources limitations
+        (like model' weights memory consumption).
+        eps: Epsilon value to manually increase metric value (if necessary) for numerical stability
 
     Returns:
         Mapping from each node's index in a graph, to a dictionary from the bitwidth index (of this node) to
         the sensitivity of the model.
 
     """
 
     Logger.info('Starting to evaluate metrics')
     layer_to_metrics_mapping = {}
 
-    is_bops_target_kpi = target_kpi.bops < np.inf
+    is_bops_target_resource_utilization = target_resource_utilization.bops < np.inf
 
-    if is_bops_target_kpi:
-        origin_max_config = search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(search_manager.max_kpi_config)
+    if is_bops_target_resource_utilization:
+        origin_max_config = search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(search_manager.max_ru_config)
         max_config_value = search_manager.compute_metric_fn(origin_max_config)
     else:
-        max_config_value = search_manager.compute_metric_fn(search_manager.max_kpi_config)
+        max_config_value = search_manager.compute_metric_fn(search_manager.max_ru_config)
 
     for node_idx, layer_possible_bitwidths_indices in tqdm(search_manager.layer_to_bitwidth_mapping.items(),
                                                            total=len(search_manager.layer_to_bitwidth_mapping)):
         layer_to_metrics_mapping[node_idx] = {}
 
         for bitwidth_idx in layer_possible_bitwidths_indices:
-            if search_manager.max_kpi_config[node_idx] == bitwidth_idx:
+            if search_manager.max_ru_config[node_idx] == bitwidth_idx:
                 # This is a computation of the metric for the max configuration, assign pre-calculated value
                 layer_to_metrics_mapping[node_idx][bitwidth_idx] = max_config_value
                 continue
 
             # Create a configuration that differs at one layer only from the baseline model
-            mp_model_configuration = search_manager.max_kpi_config.copy()
+            mp_model_configuration = search_manager.max_ru_config.copy()
             mp_model_configuration[node_idx] = bitwidth_idx
 
             # Build a distance matrix using the function we got from the framework implementation.
-            if is_bops_target_kpi:
+            if is_bops_target_resource_utilization:
                 # Reconstructing original graph's configuration from virtual graph's configuration
                 origin_mp_model_configuration = \
                     search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(
                         mp_model_configuration,
                         changed_virtual_nodes_idx=[node_idx],
                         original_base_config=origin_max_config)
                 origin_changed_nodes_indices = [i for i, c in enumerate(origin_max_config) if
                                                 c != origin_mp_model_configuration[i]]
-                layer_to_metrics_mapping[node_idx][bitwidth_idx] = search_manager.compute_metric_fn(
+                metric_value = search_manager.compute_metric_fn(
                     origin_mp_model_configuration,
                     origin_changed_nodes_indices,
                     origin_max_config)
             else:
-                layer_to_metrics_mapping[node_idx][bitwidth_idx] = search_manager.compute_metric_fn(
+                metric_value = search_manager.compute_metric_fn(
                     mp_model_configuration,
                     [node_idx],
-                    search_manager.max_kpi_config)
+                    search_manager.max_ru_config)
+
+            layer_to_metrics_mapping[node_idx][bitwidth_idx] = max(metric_value, max_config_value + eps)
+
+    # Finalize distance metric mapping
+    search_manager.finalize_distance_metric(layer_to_metrics_mapping)
 
     return layer_to_metrics_mapping
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py`

 * *Files 15% similar despite different names*

```diff
@@ -11,37 +11,44 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 
 import numpy as np
-from typing import Callable, Any, List
+from typing import Callable, Any, List, Tuple
 
-from model_compression_toolkit.core import FrameworkInfo, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.constants import AXIS
+from model_compression_toolkit.core import FrameworkInfo, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
+from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
+from model_compression_toolkit.core.common.similarity_analyzer import compute_kl_divergence
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, \
+    HessianInfoGranularity, HessianInfoService
+from model_compression_toolkit.core.common.hessian import hessian_info_utils as hessian_utils
 
 
 class SensitivityEvaluation:
     """
     Class to wrap and manage the computation on distance metric for Mixed-Precision quantization search.
     It provides a function that evaluates the sensitivity of a bit-width configuration for the MP model.
     """
 
     def __init__(self,
                  graph: Graph,
-                 quant_config: MixedPrecisionQuantizationConfigV2,
+                 quant_config: MixedPrecisionQuantizationConfig,
                  representative_data_gen: Callable,
                  fw_info: FrameworkInfo,
                  fw_impl: Any,
                  set_layer_to_bitwidth: Callable,
-                 get_quant_node_name: Callable,
-                 disable_activation_for_metric: bool = False):
+                 disable_activation_for_metric: bool = False,
+                 hessian_info_service: HessianInfoService = None
+                 ):
         """
         Initiates all relevant objects to manage a sensitivity evaluation for MP search.
         Create an object that allows to compute the sensitivity metric of an MP model (the sensitivity
         is computed based on the similarity of the interest points' outputs between the MP model
         and the float model).
         First, we initiate a SensitivityEvaluationManager that handles the components which are necessary for
         evaluating the sensitivity. It initializes an MP model (a model where layers that can be configured in
@@ -54,70 +61,102 @@
             fw_info: FrameworkInfo object about the specific framework
                 (e.g., attributes of different layers' weights to quantize).
             quant_config: MP Quantization configuration for how the graph should be quantized.
             representative_data_gen: Dataset used for getting batches for inference.
             fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             set_layer_to_bitwidth: A fw-dependent function that allows to configure a configurable MP model
                     with a specific bit-width configuration.
-            get_quant_node_name: A fw-dependent function that takes a node's name and outputs the node's name in a
-                quantized model (according to the fw conventions).
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
+
         """
         self.graph = graph
         self.quant_config = quant_config
         self.representative_data_gen = representative_data_gen
         self.fw_info = fw_info
         self.fw_impl = fw_impl
         self.set_layer_to_bitwidth = set_layer_to_bitwidth
-        self.get_quant_node_name = get_quant_node_name
         self.disable_activation_for_metric = disable_activation_for_metric
+        if self.quant_config.use_hessian_based_scores:
+            if not isinstance(hessian_info_service, HessianInfoService):
+                Logger.critical(f"When using Hessian-based approximations for sensitivity evaluation, a valid HessianInfoService object is required; found {type(hessian_info_service)}.")
+            self.hessian_info_service = hessian_info_service
+
+        self.sorted_configurable_nodes_names = graph.get_configurable_sorted_nodes_names(self.fw_info)
 
-        # Get interest points for distance measurement and a list of sorted configurable nodes names
-        self.sorted_configurable_nodes_names = graph.get_configurable_sorted_nodes_names()
+        # Get interest points and output points set for distance measurement and set other helper datasets
+        # We define a separate set of output nodes of the model for the purpose of sensitivity computation.
         self.interest_points = get_mp_interest_points(graph,
                                                       fw_impl.count_node_for_mixed_precision_interest_points,
                                                       quant_config.num_interest_points_factor)
 
-        self.outputs_replacement_nodes = None
-        self.output_nodes_indices = None
-        if self.quant_config.use_grad_based_weights is True:
-            # Getting output replacement (if needed) - if a model's output layer is not compatible for the task of
-            # gradients computation then we find a predecessor layer which is compatible,
-            # add it to the set of interest points and use it for the gradients' computation.
-            # Note that we need to modify the set of interest points before building the models,
-            # therefore, it is separated from the part where we compute the actual gradient weights.
-            self.outputs_replacement_nodes = get_output_replacement_nodes(graph, fw_impl)
-            self.output_nodes_indices = self._update_ips_with_outputs_replacements()
+        self.ips_distance_fns, self.ips_axis = self._init_metric_points_lists(self.interest_points)
+
+        self.output_points = get_output_nodes_for_metric(graph)
+        self.out_ps_distance_fns, self.out_ps_axis = self._init_metric_points_lists(self.output_points)
+
+        # Setting lists with relative position of the interest points
+        # and output points in the list of all mp model activation tensors
+        graph_sorted_nodes = self.graph.get_topo_sorted_nodes()
+        all_out_tensors_indices = [graph_sorted_nodes.index(n) for n in self.interest_points + self.output_points]
+        global_ipts_indices = [graph_sorted_nodes.index(n) for n in self.interest_points]
+        global_out_pts_indices = [graph_sorted_nodes.index(n) for n in self.output_points]
+        self.ips_act_indices = [all_out_tensors_indices.index(i) for i in global_ipts_indices]
+        self.out_ps_act_indices = [all_out_tensors_indices.index(i) for i in global_out_pts_indices]
 
         # Build a mixed-precision model which can be configured to use different bitwidth in different layers.
         # And a baseline model.
-        self.baseline_model, self.model_mp = self._build_models()
+        # Also, returns a mapping between a configurable graph's node and its matching layer(s)
+        # in the new built MP model.
+        self.baseline_model, self.model_mp, self.conf_node2layers = self._build_models()
 
         # Build images batches for inference comparison
         self.images_batches = self._get_images_batches(quant_config.num_of_images)
 
         # Get baseline model inference on all samples
         self.baseline_tensors_list = []  # setting from outside scope
 
         # Casting images tensors to the framework tensor type.
         self.images_batches = list(map(lambda in_arr: self.fw_impl.to_tensor(in_arr), self.images_batches))
 
         # Initiating baseline_tensors_list since it is not initiated in SensitivityEvaluationManager init.
         self._init_baseline_tensors_list()
 
-        # Computing gradient-based weights for weighted average distance metric computation (only if requested),
+        # Computing Hessian-based scores for weighted average distance metric computation (only if requested),
         # and assigning distance_weighting method accordingly.
-        self.interest_points_gradients = None
-        if self.quant_config.use_grad_based_weights is True:
-            assert self.outputs_replacement_nodes is not None and self.output_nodes_indices is not None, \
-                f"{self.outputs_replacement_nodes} and {self.output_nodes_indices} " \
-                f"should've been assigned before computing the gradient-based weights."
+        self.interest_points_hessians = None
+        if self.quant_config.use_hessian_based_scores is True:
+            self.interest_points_hessians = self._compute_hessian_based_scores()
+            self.quant_config.distance_weighting_method = lambda d: self.interest_points_hessians
+
+    def _init_metric_points_lists(self, points: List[BaseNode]) -> Tuple[List[Callable], List[int]]:
+        """
+        Initiates required lists for future use when computing the sensitivity metric.
+        Each point on which the metric is computed uses a dedicated distance function based on its type.
+        In addition, all distance functions preform batch computation. Axis is needed only for KL Divergence computation.
+
+        Args:
+            points: The set of nodes in the graph for which we need to initiate the lists.
+
+        Returns: A lists with distance functions and an axis list for each node.
 
-            self.interest_points_gradients = self._compute_gradient_based_weights()
-            self.quant_config.distance_weighting_method = lambda d: self.interest_points_gradients
+        """
+        distance_fns_list = []
+        axis_list = []
+        for n in points:
+            axis = n.framework_attr.get(AXIS) if not isinstance(n, FunctionalNode) else n.op_call_kwargs.get(AXIS)
+            distance_fn = self.fw_impl.get_node_distance_fn(
+                layer_class=n.layer_class,
+                framework_attrs=n.framework_attr,
+                compute_distance_fn=self.quant_config.compute_distance_fn,
+                axis=axis)
+            distance_fns_list.append(distance_fn)
+            # Axis is needed only for KL Divergence calculation, otherwise we use per-tensor computation
+            axis_list.append(axis if distance_fn==compute_kl_divergence else None)
+        return distance_fns_list, axis_list
 
     def compute_metric(self,
                        mp_model_configuration: List[int],
                        node_idx: List[int] = None,
                        baseline_mp_configuration: List[int] = None) -> float:
         """
         Compute the sensitivity metric of the MP model for a given configuration (the sensitivity
@@ -131,30 +170,27 @@
                 compute the metric for the given configuration.
 
         Returns:
             The sensitivity metric of the MP model for a given configuration.
         """
 
         # Configure MP model with the given configuration.
-        self._configure_bitwidths_model(self.model_mp,
-                                        self.sorted_configurable_nodes_names,
-                                        mp_model_configuration,
+        self._configure_bitwidths_model(mp_model_configuration,
                                         node_idx)
 
-        # Compute the distance matrix
-        distance_matrix = self._build_distance_matrix()
+        # Compute the distance metric
+        ipts_distances, out_pts_distances = self._compute_distance()
 
         # Configure MP model back to the same configuration as the baseline model if baseline provided
         if baseline_mp_configuration is not None:
-            self._configure_bitwidths_model(self.model_mp,
-                                            self.sorted_configurable_nodes_names,
-                                            baseline_mp_configuration,
+            self._configure_bitwidths_model(baseline_mp_configuration,
                                             node_idx)
 
-        return self._compute_mp_distance_measure(distance_matrix, self.quant_config.distance_weighting_method)
+        return self._compute_mp_distance_measure(ipts_distances, out_pts_distances,
+                                                 self.quant_config.distance_weighting_method)
 
     def _init_baseline_tensors_list(self):
         """
         Evaluates the baseline model on all images and saves the obtained lists of tensors in a list for later use.
         Initiates a class variable self.baseline_tensors_list
         """
         self.baseline_tensors_list = [self.fw_impl.to_numpy(self.fw_impl.sensitivity_eval_inference(self.baseline_model,
@@ -173,163 +209,220 @@
         evaluation_graph = copy.deepcopy(self.graph)
 
         if self.disable_activation_for_metric:
             for n in evaluation_graph.get_topo_sorted_nodes():
                 for c in n.candidates_quantization_cfg:
                     c.activation_quantization_cfg.enable_activation_quantization = False
 
-        model_mp, _ = self.fw_impl.model_builder(evaluation_graph,
-                                                 mode=ModelBuilderMode.MIXEDPRECISION,
-                                                 append2output=self.interest_points,
-                                                 fw_info=self.fw_info)
+        model_mp, _, conf_node2layers = self.fw_impl.model_builder(evaluation_graph,
+                                                                   mode=ModelBuilderMode.MIXEDPRECISION,
+                                                                   append2output=self.interest_points + self.output_points,
+                                                                   fw_info=self.fw_info)
 
         # Build a baseline model.
         baseline_model, _ = self.fw_impl.model_builder(evaluation_graph,
                                                        mode=ModelBuilderMode.FLOAT,
-                                                       append2output=self.interest_points)
+                                                       append2output=self.interest_points + self.output_points)
 
-        return baseline_model, model_mp
+        return baseline_model, model_mp, conf_node2layers
 
-    def _compute_gradient_based_weights(self) -> np.ndarray:
+    def _compute_hessian_based_scores(self) -> np.ndarray:
         """
-        Computes the gradient-based weights using the framework's model_grad method per batch of images.
+        Compute Hessian-based scores for each interest point.
+
+        Returns: A vector of scores, one for each interest point,
+         to be used for the distance metric weighted average computation.
+
+        """
+        # Dictionary to store the trace Hessian approximations for each interest point (target node)
+        compare_point_to_trace_hessian_approximations = {}
+
+        # Iterate over each interest point to fetch the trace Hessian approximations
+        for target_node in self.interest_points:
+            # Create a request for trace Hessian approximation with specific configurations
+            # (here we use per-tensor approximation of the Hessian's trace w.r.t the node's activations)
+            trace_hessian_request = TraceHessianRequest(mode=HessianMode.ACTIVATION,
+                                                        granularity=HessianInfoGranularity.PER_TENSOR,
+                                                        target_node=target_node)
+
+            # Fetch the trace Hessian approximations for the current interest point
+            node_approximations = self.hessian_info_service.fetch_hessian(trace_hessian_request=trace_hessian_request,
+                                                                          required_size=self.quant_config.num_of_images)
+            # Store the fetched approximations in the dictionary
+            compare_point_to_trace_hessian_approximations[target_node] = node_approximations
+
+        # List to store the approximations for each image
+        approx_by_image = []
+        # Iterate over each image
+        for image_idx in range(self.quant_config.num_of_images):
+            # List to store approximations for the current image for each interest point
+            approx_by_image_per_interest_point = []
+            # Iterate over each interest point to gather approximations
+            for target_node in self.interest_points:
+                # Ensure the approximation for the current interest point and image is a list
+                assert isinstance(compare_point_to_trace_hessian_approximations[target_node][image_idx], list)
+                # Ensure the approximation list contains only one element (since, granularity is per-tensor)
+                assert len(compare_point_to_trace_hessian_approximations[target_node][image_idx]) == 1
+                # Append the single approximation value to the list for the current image
+                approx_by_image_per_interest_point.append(compare_point_to_trace_hessian_approximations[target_node][image_idx][0])
+
+            if self.quant_config.norm_scores:
+                approx_by_image_per_interest_point = \
+                    hessian_utils.normalize_scores(hessian_approximations=approx_by_image_per_interest_point)
+
+            # Append the approximations for the current image to the main list
+            approx_by_image.append(approx_by_image_per_interest_point)
 
-        Returns: A vector of weights, one for each interest point,
-        to be used for the distance metric weighted average computation.
-        """
-
-        grad_per_batch = []
-        for images in self.images_batches:
-            batch_ip_gradients = []
-            for i in range(1, images[0].shape[0] + 1):
-                Logger.info(f"Computing Jacobian-based weights approximation for image sample {i} out of {images[0].shape[0]}...")
-                image_ip_gradients = self.fw_impl.model_grad(self.graph,
-                                                             {inode: images[0][i - 1:i] for inode in
-                                                              self.graph.get_inputs()},
-                                                             self.interest_points,
-                                                             self.outputs_replacement_nodes,
-                                                             self.output_nodes_indices,
-                                                             self.quant_config.output_grad_factor,
-                                                             norm_weights=self.quant_config.norm_weights)
-                batch_ip_gradients.append(image_ip_gradients)
-            grad_per_batch.append(np.mean(batch_ip_gradients, axis=0))
-        return np.mean(grad_per_batch, axis=0)
+        # Return the mean approximation value across all images for each interest point
+        return np.mean(approx_by_image, axis=0)
 
     def _configure_bitwidths_model(self,
-                                   model_mp: Any,
-                                   sorted_configurable_nodes_names: List[str],
                                    mp_model_configuration: List[int],
                                    node_idx: List[int]):
         """
         Configure a dynamic model (namely, model with layers that their weights and activation
         bit-width can be configured) using an MP model configuration mp_model_configuration.
 
         Args:
-            model_mp: Dynamic model to configure.
-            sorted_configurable_nodes_names: List of configurable nodes names sorted topology.
             mp_model_configuration: Configuration of bit-width indices to set to the model.
             node_idx: List of nodes' indices to configure (the rest layers are configured as the baseline model).
         """
 
         # Configure model
         # Note: Not all nodes in the graph are included in the MP model that is returned by the model builder.
         # Thus, the last configurable layer must be included in the interest points for evaluating the metric,
         # otherwise, not all configurable nodes will be considered throughout the MP optimization search (since
         # they will not affect the metric value).
-        model_mp_layers_names = self.fw_impl.get_model_layers_names(model_mp)
         if node_idx is not None:  # configure specific layers in the mp model
             for node_idx_to_configure in node_idx:
-                node_name = self.get_quant_node_name(sorted_configurable_nodes_names[node_idx_to_configure])
-                if node_name in model_mp_layers_names:
-                    current_layer = self.fw_impl.get_model_layer_by_name(model_mp, node_name)
-                    self.set_layer_to_bitwidth(current_layer, mp_model_configuration[node_idx_to_configure])
-                else:
-                    raise Exception("The last configurable node is not included in the list of interest points for"  # pragma: no cover
-                                    "sensitivity evaluation metric for the mixed-precision search.")
-
+                self._configure_node_bitwidth(self.sorted_configurable_nodes_names,
+                                              mp_model_configuration, node_idx_to_configure)
         else:  # use the entire mp_model_configuration to configure the model
             for node_idx_to_configure, bitwidth_idx in enumerate(mp_model_configuration):
-                node_name = self.get_quant_node_name(sorted_configurable_nodes_names[node_idx_to_configure])
-                if node_name in model_mp_layers_names:
-                    current_layer = self.fw_impl.get_model_layer_by_name(model_mp, node_name)
-                    self.set_layer_to_bitwidth(current_layer, mp_model_configuration[node_idx_to_configure])
-                else:
-                    raise Exception("The last configurable node is not included in the list of interest points for"
-                                    "sensitivity evaluation metric for the mixed-precision search.")  # pragma: no cover
+                self._configure_node_bitwidth(self.sorted_configurable_nodes_names,
+                                              mp_model_configuration, node_idx_to_configure)
+
+    def _configure_node_bitwidth(self,
+                                 sorted_configurable_nodes_names: List[str],
+                                 mp_model_configuration: List[int],
+                                 node_idx_to_configure: int):
+        """
+        Configures a node with multiple quantization candidates to the bitwidth candidate in the given index.
+        Args:
+            sorted_configurable_nodes_names: A list of configurable nodes names sorted according to the graph
+                topological sort order.
+            mp_model_configuration: Configuration of bit-width indices to set to the model.
+            node_idx_to_configure: Quantization configuration candidate to configure.
+
+        Returns:
+
+        """
+        node_name = sorted_configurable_nodes_names[node_idx_to_configure]
+        layers_to_config = self.conf_node2layers.get(node_name, None)
+        if layers_to_config is None:
+            Logger.critical(f"Matching layers for node {node_name} not found in the mixed precision model configuration.")  # pragma: no cover
+
+        for current_layer in layers_to_config:
+            self.set_layer_to_bitwidth(current_layer, mp_model_configuration[node_idx_to_configure])
 
-    def _compute_distance_matrix(self,
+    def _compute_points_distance(self,
                                  baseline_tensors: List[Any],
-                                 mp_tensors: List[Any]):
+                                 mp_tensors: List[Any],
+                                 points_distance_fns: List[Callable],
+                                 points_axis: List[int]):
         """
-        Compute the distance between the MP model's outputs and the baseline model's outputs
+        Compute the distance on the given set of points outputs between the MP model and the baseline model
         for each image in the batch that was inferred.
+
         Args:
-            baseline_tensors: Baseline model's output tensors.
-            mp_tensors: MP model's output tensors.
+            baseline_tensors: Baseline model's output tensors of the given points.
+            mp_tensors: MP model's output tensors pf the given points.
+            points_distance_fns: A list with distance function to compute the distance between each given
+                point's output tensors.
+            points_axis: A list with the matching axis of each given point's output tensors.
+
         Returns:
-            A distance matrix that maps each node's index to the distance between this node's output
+            A distance vector that maps each node's index in the given nodes list to the distance between this node's output
              and the baseline model's output for all images that were inferred.
         """
 
-        assert len(baseline_tensors) == len(self.interest_points)
-        num_interest_points = len(baseline_tensors)
-        num_samples = len(baseline_tensors[0])
-        distance_matrix = np.ndarray((num_interest_points, num_samples))
-
-        for i in range(num_interest_points):
-            point_distance_fn = \
-                self.fw_impl.get_node_distance_fn(layer_class=self.interest_points[i].layer_class,
-                                                  framework_attrs=self.interest_points[i].framework_attr,
-                                                  compute_distance_fn=self.quant_config.compute_distance_fn)
+        distance_v = [fn(x, y, batch=True, axis=axis) for fn, x, y, axis
+                      in zip(points_distance_fns, baseline_tensors, mp_tensors, points_axis)]
 
-            distance_matrix[i] = point_distance_fn(baseline_tensors[i], mp_tensors[i], batch=True)
+        return np.asarray(distance_v)
 
-        return distance_matrix
-
-    def _build_distance_matrix(self):
+    def _compute_distance(self) -> Tuple[np.ndarray, np.ndarray]:
         """
-        Builds a matrix that contains the distances between the baseline and MP models for each interest point.
-        Returns: A distance matrix.
+        Computing the interest points distance and the output points distance, and using them to build a
+        unified distance vector.
+
+        Returns: A distance vector.
         """
-        # List of distance matrices. We create a distance matrix for each sample from the representative_data_gen
-        # and merge all of them eventually.
-        distance_matrices = []
+
+        ipts_per_batch_distance = []
+        out_pts_per_batch_distance = []
 
         # Compute the distance matrix for num_of_images images.
         for images, baseline_tensors in zip(self.images_batches, self.baseline_tensors_list):
             # when using model.predict(), it does not use the QuantizeWrapper functionality
             mp_tensors = self.fw_impl.sensitivity_eval_inference(self.model_mp, images)
             mp_tensors = self.fw_impl.to_numpy(mp_tensors)
 
-            # Build distance matrix: similarity between the baseline model to the float model
+            # Compute distance: similarity between the baseline model to the float model
             # in every interest point for every image in the batch.
-            distance_matrices.append(self._compute_distance_matrix(baseline_tensors, mp_tensors))
+            ips_distance = self._compute_points_distance([baseline_tensors[i] for i in self.ips_act_indices],
+                                                         [mp_tensors[i] for i in self.ips_act_indices],
+                                                         self.ips_distance_fns,
+                                                         self.ips_axis)
+            outputs_distance = self._compute_points_distance([baseline_tensors[i] for i in self.out_ps_act_indices],
+                                                             [mp_tensors[i] for i in self.out_ps_act_indices],
+                                                             self.out_ps_distance_fns,
+                                                             self.out_ps_axis)
+
+            # Extending the dimensions for the concatenation at the end in case we need to
+            ips_distance = ips_distance if len(ips_distance.shape) > 1 else ips_distance[:, None]
+            outputs_distance = outputs_distance if len(outputs_distance.shape) > 1 else outputs_distance[:, None]
+            ipts_per_batch_distance.append(ips_distance)
+            out_pts_per_batch_distance.append(outputs_distance)
 
         # Merge all distance matrices into a single distance matrix.
-        distance_matrix = np.concatenate(distance_matrices, axis=1)
+        ipts_distances = np.concatenate(ipts_per_batch_distance, axis=1)
+        out_pts_distances = np.concatenate(out_pts_per_batch_distance, axis=1)
 
-        return distance_matrix
+        return ipts_distances, out_pts_distances
 
     @staticmethod
-    def _compute_mp_distance_measure(distance_matrix: np.ndarray, metrics_weights_fn: Callable) -> float:
+    def _compute_mp_distance_measure(ipts_distances: np.ndarray,
+                                     out_pts_distances: np.ndarray,
+                                     metrics_weights_fn: Callable) -> float:
         """
         Computes the final distance value out of a distance matrix.
 
         Args:
-            distance_matrix: A matrix that contains the distances between the baseline and MP models
+            ipts_distances: A matrix that contains the distances between the baseline and MP models
                 for each interest point.
-            metrics_weights_fn:
+            out_pts_distances: A matrix that contains the distances between the baseline and MP models
+                for each output point.
+            metrics_weights_fn: A callable that produces the scores to compute weighted distance for interest points.
 
         Returns: Distance value.
         """
-        # Compute the distance between the baseline model's outputs and the MP model's outputs.
-        # The distance is the mean of distances over all images in the batch that was inferred.
-        mean_distance_per_layer = distance_matrix.mean(axis=1)
-        # Use weights such that every layer's distance is weighted differently (possibly).
-        return np.average(mean_distance_per_layer, weights=metrics_weights_fn(distance_matrix))
+        mean_ipts_distance = 0
+        if len(ipts_distances) > 0:
+            mean_distance_per_layer = ipts_distances.mean(axis=1)
+
+            # Use weights such that every layer's distance is weighted differently (possibly).
+            mean_ipts_distance = np.average(mean_distance_per_layer, weights=metrics_weights_fn(ipts_distances))
+
+        mean_output_distance = 0
+        if len(out_pts_distances) > 0:
+            mean_distance_per_output = out_pts_distances.mean(axis=1)
+            mean_output_distance = np.average(mean_distance_per_output)
+
+        return mean_output_distance + mean_ipts_distance
 
     def _get_images_batches(self, num_of_images: int) -> List[Any]:
         """
         Construct batches of image samples for inference.
 
         Args:
             num_of_images: Num of total images for evaluation.
@@ -355,45 +448,22 @@
             samples_count += batch_size
         else:
             if samples_count < num_of_images:
                 Logger.warning(f'Not enough images in representative dataset to generate {num_of_images} data points, '
                                f'only {samples_count} were generated')
         return images_batches
 
-    def _update_ips_with_outputs_replacements(self):
-        """
-        Updates the list of interest points with the set of pre-calculated replacement outputs.
-        Also, returns the indices of all output nodes (original, replacements and nodes in between them) in a
-        topological sorted interest points list (for later use in gradients computation and normalization).
-
-        Returns: A list of indices of the output nodes in the sorted interest points list.
-
-        """
-
-        assert self.outputs_replacement_nodes is not None, \
-            "Trying to update interest points list with new output nodes but outputs_replacement_nodes list is None."
-
-        replacement_outputs_to_ip = [r_node for r_node in self.outputs_replacement_nodes if
-                                     r_node not in self.interest_points]
-        updated_interest_points = self.interest_points + replacement_outputs_to_ip
-
-        # Re-sort interest points in a topological order according to the graph's sort
-        self.interest_points = [n for n in self.graph.get_topo_sorted_nodes() if n in updated_interest_points]
-
-        output_indices = [self.interest_points.index(n.node) for n in self.graph.get_outputs()]
-        replacement_indices = [self.interest_points.index(n) for n in self.outputs_replacement_nodes]
-        return list(set(output_indices + replacement_indices))
-
 
 def get_mp_interest_points(graph: Graph,
                            interest_points_classifier: Callable,
                            num_ip_factor: float) -> List[BaseNode]:
     """
     Gets a list of interest points for the mixed precision metric computation.
-    The list is constructed from a filtered set of the convolutions nodes in the graph.
+    The list is constructed from a filtered set of nodes in the graph.
+    Note that the output layers are separated from the interest point set for metric computation purposes.
 
     Args:
         graph: Graph to search for its MP configuration.
         interest_points_classifier: A function that indicates whether a given node in considered as a potential
             interest point for mp metric computation purposes.
         num_ip_factor: Percentage out of the total set of interest points that we want to actually use.
 
@@ -401,47 +471,38 @@
 
     """
     sorted_nodes = graph.get_topo_sorted_nodes()
     ip_nodes = list(filter(lambda n: interest_points_classifier(n), sorted_nodes))
 
     interest_points_nodes = bound_num_interest_points(ip_nodes, num_ip_factor)
 
-    # We add output layers of the model to interest points
-    # in order to consider the model's output in the distance metric computation (and also to make sure
-    # all configurable layers are included in the configured mp model for metric computation purposes)
-    output_nodes = [n.node for n in graph.get_outputs() if n.node not in interest_points_nodes]
-    interest_points = interest_points_nodes + output_nodes
+    # We exclude output nodes from the set of interest points since they are used separately in the sensitivity evaluation.
+    output_nodes = [n.node for n in graph.get_outputs()]
+
+    interest_points = [n for n in interest_points_nodes if n not in output_nodes]
 
     return interest_points
 
 
-def get_output_replacement_nodes(graph: Graph,
-                                 fw_impl: Any) -> List[BaseNode]:
+def get_output_nodes_for_metric(graph: Graph) -> List[BaseNode]:
     """
-    If a model's output node is not compatible for the task of gradients computation we need to find a predecessor
-    node in the model's graph representation which is compatible and add it to the set of interest points and use it
-    for the gradients' computation. This method searches for this predecessor node for each output of the model.
+    Returns a list of output nodes that are also quantized (either kernel weights attribute or activation)
+    to be used as a set of output points in the distance metric computation.
 
     Args:
-        graph: Graph to search for replacement output nodes.
-        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        graph: Graph to search for its MP configuration.
 
-    Returns: A list of output replacement nodes.
+    Returns: A list of output nodes.
 
     """
-    replacement_outputs = []
-    for n in graph.get_outputs():
-        prev_node = n.node
-        while not fw_impl.is_node_compatible_for_metric_outputs(prev_node):
-            prev_node = graph.get_prev_nodes(n.node)
-            assert len(prev_node) == 1, "A none MP compatible output node has multiple inputs, " \
-                                        "which is incompatible for metric computation."
-            prev_node = prev_node[0]
-        replacement_outputs.append(prev_node)
-    return replacement_outputs
+
+    return [n.node for n in graph.get_outputs()
+            if (graph.fw_info.is_kernel_op(n.node.type) and
+                n.node.is_weights_quantization_enabled(graph.fw_info.get_kernel_op_attributes(n.node.type)[0])) or
+            n.node.is_activation_quantization_enabled()]
 
 
 def bound_num_interest_points(sorted_ip_list: List[BaseNode], num_ip_factor: float) -> List[BaseNode]:
     """
     Filters the list of interest points and returns a shorter list with number of interest points smaller than some
     default threshold.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,113 +11,136 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import List
 
-from model_compression_toolkit.core import KPI
+from model_compression_toolkit.core import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import \
     MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.logger import Logger
 import numpy as np
 
 
 def greedy_solution_refinement_procedure(mp_solution: List[int],
                                          search_manager: MixedPrecisionSearchManager,
-                                         target_kpi: KPI) -> List[int]:
+                                         target_resource_utilization: ResourceUtilization) -> List[int]:
     """
     A greedy procedure to try and improve a mixed-precision solution that was found by a mixed-precision optimization
     algorithm.
     This procedure tries to increase the bit-width precision of configurable nodes that did not get the maximal
     candidate
     in the found solution.
-    It iteratively goes over all such nodes, computes the KPI values on a modified configuration (with the node's next
-    best candidate), filters out all configs that hold the KPI constraints and chooses one of them as an improvement
+    It iteratively goes over all such nodes, computes the resource utilization values on a modified configuration (with the node's next
+    best candidate), filters out all configs that hold the resource utilization constraints and chooses one of them as an improvement
     step
-    The choice is done in a greedy approach where we take the configuration that modifies the KPI the least.
+    The choice is done in a greedy approach where we take the configuration that modifies the resource utilization the least.
 
     Args:
         mp_solution: A mixed-precision configuration that was found by a mixed-precision optimization algorithm.
         search_manager: A MixedPrecisionSearchManager object.
-        target_kpi: The target KPIs for the mixed-precision search.
+        target_resource_utilization: The target resource utilization for the mixed-precision search.
 
     Returns: A new, possibly updated, mixed-precision bit-width configuration.
 
     """
-    # Refinement is not supported for BOPs KPI for now...
-    if target_kpi.bops < np.inf:
-        Logger.info(f'Target KPI constraint BOPs - Skipping MP greedy solution refinement')
+    # Refinement is not supported for BOPs utilization for now...
+    if target_resource_utilization.bops < np.inf:
+        Logger.info(f'Target resource utilization constraint BOPs - Skipping MP greedy solution refinement')
         return mp_solution
 
     new_solution = mp_solution.copy()
     changed = True
 
     while changed:
         changed = False
-        nodes_kpis = {}
+        nodes_ru = {}
         nodes_next_candidate = {}
 
         for node_idx in range(len(mp_solution)):
             if new_solution[node_idx] == 0:
                 # layer has max config in the given solution, nothing to optimize
                 continue
 
-            node_candidates = search_manager.graph.get_configurable_sorted_nodes()[node_idx].candidates_quantization_cfg
-            valid_candidates = _get_valid_candidates_indices(node_candidates, new_solution[node_idx])
+            current_node = search_manager.graph.get_configurable_sorted_nodes(search_manager.fw_info)[node_idx]
+            node_candidates = current_node.candidates_quantization_cfg
 
-            # Create a list of KPIs for the valid candidates.
-            updated_kpis = []
+            # only weights kernel attribute is quantized with weights mixed precision
+            kernel_attr = search_manager.fw_info.get_kernel_op_attributes(current_node)
+            kernel_attr = None if kernel_attr is None else kernel_attr[0]
+            valid_candidates = _get_valid_candidates_indices(node_candidates, new_solution[node_idx], kernel_attr)
+
+            # Create a list of ru for the valid candidates.
+            updated_ru = []
             for valid_idx in valid_candidates:
-                node_updated_kpis = search_manager.compute_kpi_for_config(
+                node_updated_ru = search_manager.compute_resource_utilization_for_config(
                     config=search_manager.replace_config_in_index(new_solution, node_idx, valid_idx))
-                updated_kpis.append(node_updated_kpis)
+                updated_ru.append(node_updated_ru)
 
-            # filter out new configs that don't hold the KPI restrictions
-            node_filtered_kpis = [(node_idx, kpis) for node_idx, kpis in zip(valid_candidates,updated_kpis) if
-                               target_kpi.holds_constraints(kpis)]
-
-            if len(node_filtered_kpis) > 0:
-                sorted_by_kpi = sorted(node_filtered_kpis, key=lambda node_kpis: (node_kpis[1].total_memory,
-                                                                               node_kpis[1].weights_memory,
-                                                                               node_kpis[1].activation_memory))
-                nodes_kpis[node_idx] = sorted_by_kpi[0][1]
-                nodes_next_candidate[node_idx] = sorted_by_kpi[0][0]
-
-
-        if len(nodes_kpis) > 0:
-            # filter out new configs that don't hold the KPI restrictions
-            node_filtered_kpis = [(node_idx, kpis) for node_idx, kpis in nodes_kpis.items()]
-            sorted_by_kpi = sorted(node_filtered_kpis, key=lambda node_kpis: (node_kpis[1].total_memory,
-                                                                           node_kpis[1].weights_memory,
-                                                                           node_kpis[1].activation_memory))
+            # filter out new configs that don't hold the resource utilization restrictions
+            node_filtered_ru = [(node_idx, ru) for node_idx, ru in zip(valid_candidates, updated_ru) if
+                                target_resource_utilization.holds_constraints(ru)]
+
+            if len(node_filtered_ru) > 0:
+                sorted_by_ru = sorted(node_filtered_ru, key=lambda node_ru: (node_ru[1].total_memory,
+                                                                             node_ru[1].weights_memory,
+                                                                             node_ru[1].activation_memory))
+                nodes_ru[node_idx] = sorted_by_ru[0][1]
+                nodes_next_candidate[node_idx] = sorted_by_ru[0][0]
+
+        if len(nodes_ru) > 0:
+            # filter out new configs that don't hold the ru restrictions
+            node_filtered_ru = [(node_idx, ru) for node_idx, ru in nodes_ru.items()]
+            sorted_by_ru = sorted(node_filtered_ru, key=lambda node_ru: (node_ru[1].total_memory,
+                                                                         node_ru[1].weights_memory,
+                                                                         node_ru[1].activation_memory))
 
-            node_idx_to_upgrade = sorted_by_kpi[0][0]
+            node_idx_to_upgrade = sorted_by_ru[0][0]
             new_solution[node_idx_to_upgrade] = nodes_next_candidate[node_idx_to_upgrade]
             changed = True
 
-    Logger.info(f'Greedy MP algorithm changed configuration from: {mp_solution} to {new_solution}')
+    if any([mp_solution[i] != new_solution[i] for i in range(len(mp_solution))]):
+        Logger.info(f'Greedy MP algorithm changed configuration from (numbers represent indices of the '
+                    f'chosen bit-width candidate for each layer):\n{mp_solution}\nto\n{new_solution}')
+
     return new_solution
 
 
 def _get_valid_candidates_indices(node_candidates: List[CandidateNodeQuantizationConfig],
-                                  current_chosen_index: int) -> List[int]:
+                                  current_chosen_index: int,
+                                  kernel_attr: str = None) -> List[int]:
     """
     Find node's valid candidates to try and improve the node's MP chosen candidate.
-    Valid indices are indices of candidates that have higher number of bits for both weights and activations.
+    Valid indices are indices of candidates that have higher number of bits for both weights and activations
+    (if they are quantized in this node).
 
     Args:
         node_candidates: Candidates of the node.
         current_chosen_index: Current index in MP configuration of the node.
+        kernel_attr: The name of the kernel attribute on the node, otherwise None.
 
     Returns:
         List of indices of valid candidates.
     """
-
     current_candidate = node_candidates[current_chosen_index]
-    weights_num_bits = current_candidate.weights_quantization_cfg.weights_n_bits
-    activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
 
-    # Filter candidates that have higher bit-width for both weights and activations (except for the current index).
-    return [i for i, c in enumerate(node_candidates) if c.activation_quantization_cfg.activation_n_bits >= activation_num_bits and c.weights_quantization_cfg.weights_n_bits >= weights_num_bits and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits and c.weights_quantization_cfg.weights_n_bits == weights_num_bits)]
+    if kernel_attr is None:
+        # In this node we only quantize activation, so no need to check weights number of bits
+        activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
+
+        # Filter candidates that have higher bit-width for activations
+        return [i for i, c in enumerate(node_candidates) if
+                c.activation_quantization_cfg.activation_n_bits >= activation_num_bits
+                and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits)]
+    else:
+        weights_num_bits = current_candidate.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+        activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
+
+        # Filter candidates that have higher bit-width for both weights and activations (except for the current index).
+        return [i for i, c in enumerate(node_candidates) if
+                c.activation_quantization_cfg.activation_n_bits >= activation_num_bits
+                and c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits >= weights_num_bits
+                and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits
+                         and c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_num_bits)]
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/model_builder_mode.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/model_builder_mode.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/model_validation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/model_validation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/actions.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/actions.py`

 * *Files 16% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 from abc import ABC, abstractmethod
 from collections import namedtuple
 from typing import Callable
 
+from mct_quantizers import QuantizationMethod
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.logger import Logger
 
 
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
@@ -33,19 +34,20 @@
 
 class EditRule(_EditRule):
     """
     A tuple of a node filter and an action. The filter matches nodes in the graph which represents the model,
     and the action is applied on these nodes during the quantization process.
 
     Examples:
-        Create an EditRule to quantize all Conv2D wights using 9 bits:
+        Create an EditRule to quantize all Conv2D kernel attribute weights using 9 bits:
 
         >>> import model_compression_toolkit as mct
+        >>> from model_compression_toolkit.core.keras.constants import KERNEL
         >>> from tensorflow.keras.layers import Conv2D
-        >>> er_list = [mct.network_editor.EditRule(filter=mct.network_editor.NodeTypeFilter(Conv2D), action=mct.network_editor.ChangeCandidatesWeightsQuantConfigAttr(weights_n_bits=9))]
+        >>> er_list = [mct.core.network_editor.EditRule(filter=mct.core.network_editor.NodeTypeFilter(Conv2D), action=mct.core.network_editor.ChangeCandidatesWeightsQuantConfigAttr(attr_name=KERNEL, weights_n_bits=9))]
 
         Then the rules list can be passed to :func:`~model_compression_toolkit.keras_post_training_quantization`
         to modify the network during the quantization process.
 
     """
 
     def __repr__(self):
@@ -80,54 +82,61 @@
 
 
 class ChangeCandidatesWeightsQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's weights quantization configuration candidates.
     """
 
-    def __init__(self, **kwargs):
+    def __init__(self, attr_name: str = None, **kwargs):
         """
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for.
             kwargs: Dictionary of attr_name and attr_value to change layer's weights quantization configuration candidates.
         """
         self.kwargs = kwargs
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the attribute 'attr_name' in weights quantization config candidates with 'attr_value'.
 
         Args:
             node: Node object to change its quant_config.
             graph: Graph to apply the action on.
             fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
                      groups of layers by how they should be quantized, etc.)
         Returns:
             The node after its weights' quantization config candidates have been modified.
         """
+
         for nqc in node.candidates_quantization_cfg:
-            for attr_name, attr_value in self.kwargs.items():
-                nqc.weights_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                nqc.weights_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value,
+                                                                   attr_name=self.attr_name)
 
 
 class ChangeFinalWeightsQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's final weights quantization config.
     """
 
-    def __init__(self, **kwargs):
+    def __init__(self, attr_name: str = None, **kwargs):
         """
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for.
             kwargs: Dictionary of attr_name and attr_value to change layer's final weights quantization config.
         """
         self.kwargs = kwargs
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         if node.final_weights_quantization_cfg is not None:
-            for attr_name, attr_value in self.kwargs.items():
-                node.final_weights_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                node.final_weights_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value,
+                                                                          self.attr_name)
 
 
 class ChangeCandidatesActivationQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's activation quantization configuration candidates.
     """
 
@@ -147,16 +156,16 @@
             graph: Graph to apply the action on.
             fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
                      groups of layers by how they should be quantized, etc.)
         Returns:q
             The node after its activation quantization configuration candidates have been modified.
         """
         for nqc in node.candidates_quantization_cfg:
-            for attr_name, attr_value in self.kwargs.items():
-                nqc.activation_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                nqc.activation_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value)
 
 
 class ChangeFinalActivationQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's final activation quantization config.
     """
 
@@ -165,33 +174,38 @@
         Args:
             kwargs: Dictionary of attr_name and attr_value to change layer's final activation quantization config.
         """
         self.kwargs = kwargs
 
     def apply(self, node: BaseNode, graph, fw_info):
         if node.final_activation_quantization_cfg is not None:
-            for attr_name, attr_value in self.kwargs.items():
-                node.final_activation_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                node.final_activation_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value)
 
 
 class ChangeQuantizationParamFunction(BaseAction):
     """
     Class ChangeQuantizationParamFunction to change a node's weights/activations quantization params function.
     """
 
-    def __init__(self, activation_quantization_params_fn=None, weights_quantization_params_fn=None):
+    def __init__(self,
+                 attr_name: str = None,
+                 activation_quantization_params_fn: Callable = None,
+                 weights_quantization_params_fn: Callable = None):
         """
         Init a ChangeQuantizationParamFunction object.
 
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for (if setting weights params).
             activation_quantization_params_fn: a params function for a node's activations.
             weights_quantization_params_fn: a params function for a node's weights.
         """
         self.activation_quantization_params_fn = activation_quantization_params_fn
         self.weights_quantization_params_fn = weights_quantization_params_fn
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the node's weights/activations quantization params function.
 
         Args:
             node: Node object to change its quantization params function.
@@ -203,15 +217,16 @@
             The node after its quantization params function has been modified.
         """
         for nqc in node.candidates_quantization_cfg:
             if self.activation_quantization_params_fn is not None:
                 nqc.activation_quantization_cfg.set_activation_quantization_params_fn(
                     self.activation_quantization_params_fn)
             if self.weights_quantization_params_fn is not None:
-                nqc.weights_quantization_cfg.set_weights_quantization_params_fn(self.weights_quantization_params_fn)
+                (nqc.weights_quantization_cfg.get_attr_config(self.attr_name)
+                 .set_weights_quantization_params_fn(self.weights_quantization_params_fn))
 
 
 class ChangeFinalActivationQuantizationMethod(BaseAction):
     """
     Class ChangeFinalActivationQuantizationMethod to change a node's weights/activations quantizer function.
     """
 
@@ -286,34 +301,36 @@
                     self.activation_quantization_method)
 
                 qc.activation_quantization_cfg.set_activation_quantization_params_fn(activation_quantization_params_fn)
                 activation_quantization_fn = fw_info.activation_quantizer_mapping.get(
                     self.activation_quantization_method)
 
                 if activation_quantization_fn is None:
-                    raise Exception('Unknown quantization method for activations')  # pragma: no cover
+                    Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
                 qc.activation_quantization_cfg.set_activation_quantization_fn(activation_quantization_fn)
                 qc.activation_quantization_cfg.activation_quantization_method = self.activation_quantization_method
 
 
 class ChangeFinalWeightsQuantizationMethod(BaseAction):
     """
     Class ChangeFinalWeightsQuantizationMethod to change a node's weights/activations quantizer function.
     """
 
-    def __init__(self, weights_quantization_method=None):
+    def __init__(self, attr_name: str, weights_quantization_method=None):
         """
         Init a ChangeFinalWeightsQuantizationMethod object.
 
         Args:
+            attr_name: The weights attribute's name to set the weights quantization method for.
             weights_quantization_method: a quantization method for a node's weights.
         """
 
         self.weights_quantization_method = weights_quantization_method
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the node's weights quantization function.
 
         Args:
             node: Node object to change its threshold selection function.
@@ -325,38 +342,43 @@
             The node after its quantization function has been modified.
         """
 
         if self.weights_quantization_method is not None and node.final_weights_quantization_cfg is not None:
 
             weights_quantization_params_fn = get_weights_quantization_params_fn(self.weights_quantization_method)
 
-            node.final_weights_quantization_cfg.set_weights_quantization_params_fn(weights_quantization_params_fn)
+            (node.final_weights_quantization_cfg.get_attr_config(self.attr_name)
+             .set_weights_quantization_params_fn(weights_quantization_params_fn))
 
             weights_quantization_fn = get_weights_quantization_fn(self.weights_quantization_method)
 
             if weights_quantization_fn is None:
-                raise Exception('Unknown quantization method for weights')  # pragma: no cover
+                Logger.critical('Unknown weights quantization method specified.')  # pragma: no cover
 
-            node.final_weights_quantization_cfg.set_weights_quantization_fn(weights_quantization_fn)
-            node.final_weights_quantization_cfg.weights_quantization_method = self.weights_quantization_method
+            (node.final_weights_quantization_cfg.get_attr_config(self.attr_name)
+             .set_weights_quantization_fn(weights_quantization_fn))
+            node.final_weights_quantization_cfg.get_attr_config(self.attr_name).weights_quantization_method = \
+                self.weights_quantization_method
 
 
 class ChangeCandidatesWeightsQuantizationMethod(BaseAction):
     """
     Class ChangeCandidatesWeightsQuantizationMethod to change a node's weights quantizer function.
     """
 
-    def __init__(self, weights_quantization_method=None):
+    def __init__(self, attr_name: str, weights_quantization_method: QuantizationMethod = None):
         """
         Init a ChangeCandidatesWeightsQuantizationMethod object.
 
         Args:
             weights_quantization_method: a quantization method for a node's weights.
+            attr_name: The weights attribute's name to set the weights quantization params function for.
         """
         self.weights_quantization_method = weights_quantization_method
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph: Graph, fw_info: FrameworkInfo):
         """
         Change the node's weights quantization function.
 
         Args:
             node: Node object to change its threshold selection function.
@@ -369,23 +391,24 @@
         """
 
         if self.weights_quantization_method is not None:
             for qc in node.candidates_quantization_cfg:
 
                 weights_quantization_params_fn = get_weights_quantization_params_fn(self.weights_quantization_method)
 
-                qc.weights_quantization_cfg.set_weights_quantization_params_fn(weights_quantization_params_fn)
+                attr_qc = qc.weights_quantization_cfg.get_attr_config(self.attr_name)
+                attr_qc.set_weights_quantization_params_fn(weights_quantization_params_fn)
 
                 weights_quantization_fn = get_weights_quantization_fn(self.weights_quantization_method)
 
                 if weights_quantization_fn is None:
-                    raise Exception('Unknown quantization method for weights')  # pragma: no cover
+                    Logger.critical('Unknown weights quantization method specified.')  # pragma: no cover
 
-                qc.weights_quantization_cfg.set_weights_quantization_fn(weights_quantization_fn)
-                qc.weights_quantization_cfg.weights_quantization_method = self.weights_quantization_method
+                attr_qc.set_weights_quantization_fn(weights_quantization_fn)
+                attr_qc.weights_quantization_method = self.weights_quantization_method
 
 
 class ReplaceLayer(BaseAction):
 
     def __init__(self, layer_type: type, get_params_and_weights_fn: Callable):
         """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/edit_network.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/edit_network.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 def edit_network_graph(graph: Graph,
                        fw_info: FrameworkInfo,
                        network_editor: List[EditRule]):
     """
     Apply a list of edit rules on a graph.
 
     Args:
-        graph_to_edit: The graph to edit.
+        graph: The graph to edit.
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
         groups of layers by how they should be quantized, etc.)
         network_editor: List of edit rules to apply to the graph.
 
     Returns:
         The graph after it has been applied the edit rules from the network editor list.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/network_editors/node_filters.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/network_editors/node_filters.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/core_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/core_config.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,37 +8,41 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
 
 
 class CoreConfig:
     """
     A class to hold the configurations classes of the MCT-core.
     """
     def __init__(self,
                  quantization_config: QuantizationConfig = QuantizationConfig(),
-                 mixed_precision_config: MixedPrecisionQuantizationConfigV2 = None,
+                 mixed_precision_config: MixedPrecisionQuantizationConfig = None,
                  debug_config: DebugConfig = DebugConfig()
                  ):
         """
 
         Args:
             quantization_config (QuantizationConfig): Config for quantization.
-            mixed_precision_config (MixedPrecisionQuantizationConfigV2): Config for mixed precision quantization (optional, default=None).
+            mixed_precision_config (MixedPrecisionQuantizationConfig): Config for mixed precision quantization.
+            If None, a default MixedPrecisionQuantizationConfig is used.
             debug_config (DebugConfig): Config for debugging and editing the network quantization process.
         """
         self.quantization_config = quantization_config
-        self.mixed_precision_config = mixed_precision_config
         self.debug_config = debug_config
 
+        if mixed_precision_config is None:
+            self.mixed_precision_config = MixedPrecisionQuantizationConfig()
+        else:
+            self.mixed_precision_config = mixed_precision_config
+
     @property
     def mixed_precision_enable(self):
-        return self.mixed_precision_config is not None
+        return self.mixed_precision_config is not None and self.mixed_precision_config.mixed_precision_enable
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/debug_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/debug_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py`

 * *Files 25% similar despite different names*

```diff
@@ -11,14 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from typing import List
 
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.constants import FLOAT_BITWIDTH
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 
 
 def filter_nodes_candidates(graph: Graph):
@@ -29,57 +31,108 @@
     Updating the lists is preformed inplace on the graph object.
 
     Args:
         graph: Graph for which to add quantization info to each node.
     """
     nodes = list(graph.nodes)
     for n in nodes:
-        n.candidates_quantization_cfg = filter_node_candidates(node=n)
+        n.candidates_quantization_cfg = filter_node_candidates(node=n, fw_info=graph.fw_info)
 
     return graph
 
 
-def filter_node_candidates(node: BaseNode) -> List[CandidateNodeQuantizationConfig]:
+def _filter_bit_method_dups(candidates: List[CandidateNodeQuantizationConfig],
+                            kernel_attr: str = None) -> List[CandidateNodeQuantizationConfig]:
+    """
+    Filters out duplications in candidates configuration list, based on similarity in
+    (weights_n_bits, weights_quantization_method, activation_n_bits, activation_quantization_method).
+    Weights quantization configuration considers only kernel attributes.
+
+    Args:
+        candidates: A list of quantization configuration candidates.
+        kernel_attr: The name of the node's kernel attribute if such exists.
+
+    Returns: A filtered list of quantization configuration candidates.
+
+    """
+    seen_bits_method_combinations = set()
+    final_candidates = []
+    for c in candidates:
+        weight_n_bits = None if kernel_attr is None else (
+            c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits)
+        weights_quantization_method = None if kernel_attr is None else (
+            c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method)
+        comb = (weight_n_bits,
+                weights_quantization_method,
+                c.activation_quantization_cfg.activation_n_bits,
+                c.activation_quantization_cfg.activation_quantization_method)
+        if comb not in seen_bits_method_combinations:
+            final_candidates.append(c)
+            seen_bits_method_combinations.add(comb)
+
+    return final_candidates
+
+
+def filter_node_candidates(node: BaseNode, fw_info) -> List[CandidateNodeQuantizationConfig]:
     """
     Updates a node's candidates configuration list.
     If the node's weights quantization is disabled (or it only has activations to quantize), then the updated list
     will have a candidate with any of the different original activation bitwidths candidates and a default value
     for its weights bitwidth (that doesn't have any impact on the quantization or the mixed-precision search.
     If the node's activation quantization is disabled, the same filtering applied for the weights bitwidth candidates.
 
     Args:
         node: Node to set its quantization configurations.
+        fw_info: FrameworkInfo object with information about the specific framework's model.
+
     """
 
     filtered_candidates = copy.deepcopy(node.candidates_quantization_cfg)
+    final_candidates = copy.deepcopy(node.candidates_quantization_cfg)
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)[0]
 
-    if not node.is_weights_quantization_enabled() and not node.is_activation_quantization_enabled():
-        # If both weights and activation quantization are disabled, but for some reason the node has multiple candidates
-        # then replace it with a single dummy candidate with default bit-width values.
+    if (kernel_attr is None or not node.is_weights_quantization_enabled(kernel_attr)) and not node.is_activation_quantization_enabled():
+        # If activation quantization is disabled and the node doesn't have a kernel or doesn't quantize the kernel,
+        # but for some reason the node has multiple candidates then replace it with a single dummy candidate with
+        # default bit-width values.
         single_dummy_candidate = filtered_candidates[0]
         single_dummy_candidate.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
-        single_dummy_candidate.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
-        filtered_candidates = [single_dummy_candidate]
+        single_dummy_candidate.activation_quantization_cfg.activation_quantization_method = QuantizationMethod.POWER_OF_TWO
+
+        if kernel_attr is not None:
+            kernel_config = single_dummy_candidate.weights_quantization_cfg.get_attr_config(kernel_attr)
+            kernel_config.weights_n_bits = FLOAT_BITWIDTH
+            kernel_config.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
+
+        final_candidates = [single_dummy_candidate]
 
     elif not node.is_activation_quantization_enabled():
         # Remove candidates that have duplicated weights candidates for node with disabled activation quantization.
         # Replacing the activation n_bits in the remained configurations with default value to prevent confusion.
         seen_candidates = set()
         filtered_candidates = [candidate for candidate in filtered_candidates if
                                candidate.weights_quantization_cfg not in seen_candidates
                                and not seen_candidates.add(candidate.weights_quantization_cfg)]
 
         for c in filtered_candidates:
             c.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
+            c.activation_quantization_cfg.activation_quantization_method = QuantizationMethod.POWER_OF_TWO
+
+        final_candidates = _filter_bit_method_dups(filtered_candidates, kernel_attr)
 
-    elif not node.is_weights_quantization_enabled():
+    elif kernel_attr is None or not node.is_weights_quantization_enabled(kernel_attr):
         # Remove candidates that have duplicated activation candidates for node with disabled weights quantization.
         # Replacing the weights n_bits in the remained configurations with default value to prevent confusion.
         seen_candidates = set()
         filtered_candidates = [candidate for candidate in filtered_candidates if
                                candidate.activation_quantization_cfg not in seen_candidates
                                and not seen_candidates.add(candidate.activation_quantization_cfg)]
 
         for c in filtered_candidates:
-            c.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
+            if kernel_attr is not None:
+                kernel_config = c.weights_quantization_cfg.get_attr_config(kernel_attr)
+                kernel_config.weights_n_bits = FLOAT_BITWIDTH
+                kernel_config.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
+
+        final_candidates = _filter_bit_method_dups(filtered_candidates, kernel_attr)
 
-    return filtered_candidates
+    return final_candidates
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/node_quantization_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,384 +1,293 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Tuple, Any, Dict, Union, List
 
+from packaging import version
+import tensorflow as tf
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.base_layer import Layer
+else:
+    from keras.engine.base_layer import Layer
+
+from keras.models import Model
+from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder, QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.core.common.user_info import UserInformation
+from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
+from model_compression_toolkit.core.keras.mixed_precision.configurable_activation_quantizer import \
+    ConfigurableActivationQuantizer
+from model_compression_toolkit.core.keras.mixed_precision.configurable_weights_quantizer import \
+    ConfigurableWeightsQuantizer
 
-from typing import Callable, Any
-
-import numpy as np
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
+    get_inferable_quantizer_kwargs
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
-    get_activation_quantization_params_fn, get_weights_quantization_params_fn
-
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, \
-    QuantizationErrorMethod
-from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig
-
-
-##########################################
-# Every node holds a quantization configuration
-# for its weights and activations quantization, and a different quantization
-# configuration for its activation quantization configuration.
-##########################################
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 
 
-class BaseNodeQuantizationConfig(object):
+class MixedPrecisionKerasModelBuilder(KerasModelBuilder):
     """
-    Base class for node quantization configuration
+    Builder of mixed-precision Keras models.
     """
 
-    def set_quant_config_attr(self, attr_name, attr_value):
-        """
-        Changes a BaseNodeQuantizationConfig's attribute.
-
-        Args:
-            attr_name: attribute name to change.
-            attr_value: attribute value to change.
-
-        """
-
-        if hasattr(self, attr_name):
-            setattr(self, attr_name, attr_value)
-
-    def __repr__(self) -> str:
-        """
-        Returns: String to display a NodeQuantizationConfig object.
-        """
-        repr_str = ''
-        for k, v in self.__dict__.items():
-            repr_str += f'{k}: {v}\n'
-        return repr_str
-
-
-class NodeActivationQuantizationConfig(BaseNodeQuantizationConfig):
-    """
-    Attributes for configuring the quantization of the activations of a node.
-    """
     def __init__(self,
-                 qc: QuantizationConfig,
-                 op_cfg: OpQuantizationConfig,
-                 activation_quantization_fn: Callable,
-                 activation_quantization_params_fn: Callable
-                 ):
+                 graph: common.Graph,
+                 append2output=None,
+                 fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
+                 return_float_outputs: bool = False):
         """
 
         Args:
-            qc: QuantizationConfig to create the node's config from.
-            op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
-            activation_quantization_fn: Function to use when quantizing the node's activations.
-            activation_quantization_params_fn: Function to use when computing the threshold for quantizing a node's activations.
-        """
-
-        self.activation_quantization_fn = activation_quantization_fn
-        self.activation_quantization_params_fn = activation_quantization_params_fn
-        self.activation_quantization_params = {}
-        self.activation_quantization_method = op_cfg.activation_quantization_method
-        self.activation_error_method = qc.activation_error_method
-        self.activation_n_bits = op_cfg.activation_n_bits
-        self.relu_bound_to_power_of_2 = qc.relu_bound_to_power_of_2
-        self.enable_activation_quantization = op_cfg.enable_activation_quantization
-        self.activation_channel_equalization = qc.activation_channel_equalization
-        self.input_scaling = qc.input_scaling
-        self.min_threshold = qc.min_threshold
-        self.l_p_value = qc.l_p_value
-        self.shift_negative_activation_correction = qc.shift_negative_activation_correction
-        self.z_threshold = qc.z_threshold
-        self.shift_negative_ratio = qc.shift_negative_ratio
-        self.shift_negative_threshold_recalculation = qc.shift_negative_threshold_recalculation
-
-    def quantize_node_output(self,
-                             tensors: Any) -> Any:
+            graph: Graph to build the model from.
+            append2output: Nodes to append to model's output.
+            fw_info: Information about the specific framework of the model that is built.
+            return_float_outputs: Whether the model returns float tensors or not.
         """
 
-        Args:
-            tensors: framework tensor/s
+        self.graph = graph
 
-        Returns:
-            Framework tensor/s after applying fake quantization.
+        super().__init__(graph,
+                         append2output,
+                         fw_info,
+                         return_float_outputs,
+                         wrapper=self.mixed_precision_wrapper,
+                         get_activation_quantizer_holder_fn=self.mixed_precision_activation_holder)
 
+    def mixed_precision_wrapper(self,
+                                n: common.BaseNode,
+                                layer: Layer) -> Union[KerasQuantizationWrapper, Layer]:
         """
-        fake_quant = self.activation_quantization_fn(self.activation_n_bits,
-                                                     self.activation_quantization_params)
+        A function which takes a computational graph node and a keras layer and perform the quantization
+        wrapping for mixed precision.
 
-        if fake_quant is None:
-            Logger.error('Layer is meant to be quantized but fake_quant function is None')  # pragma: no cover
-        return fake_quant(tensors)
+        Args:
+            n: A node of mct graph.
+            layer: A keras layer
 
-    @property
-    def activation_error_method(self) -> QuantizationErrorMethod:
-        """
-        activation_error_method getter.
-        """
-        return self._activation_error_method
+        Returns: Wrapped layer with a configurable quantizer if the layer should quantized in mixed precision,
+        otherwise returns either the layer wrapped with a fixed precision inferable quantizer or the layer as is if it's
+        not supposed to be quantized.
 
-    @activation_error_method.setter
-    def activation_error_method(self, value: QuantizationErrorMethod):
         """
-        activation_error_method setter.
-
-        Args:
-            value: New activation_error_method to set to the node activation configuration.
 
-        """
-        self._activation_error_method = value
-        self.activation_quantization_params_fn = get_activation_quantization_params_fn(activation_quantization_method=self.activation_quantization_method)
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+            weights_conf_nodes_names = [node.name for node in self.graph.get_weights_configurable_nodes(self.fw_info)]
+            if n.name in weights_conf_nodes_names:
+                return KerasQuantizationWrapper(layer,
+                                                weights_quantizers={
+                                                    kernel_attr: ConfigurableWeightsQuantizer(
+                                                        **self._get_weights_configurable_quantizer_kwargs(n,
+                                                                                                          kernel_attr))})
+            else:
+                # TODO: Do we want to include other quantized attributes that are not
+                #  the kernel attribute in the mixed precision model?
+                #  Currently, we only consider kernel attribute quantization (whether it is in mixed precision
+                #  or single precision).
+                node_weights_qc = n.get_unique_weights_candidates(kernel_attr)
+                if not len(node_weights_qc) == 1:
+                    Logger.critical(f"Expected a unique weights configuration for node {n.name}, but found {len(node_weights_qc)} configurations.")# pragma: no cover
 
-    def set_activation_quantization_fn(self, activation_quantization_fn: Callable):
-        """
-        Sets activation quantization function for the node.
+                quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
+                                                                  node_weights_qc[0].weights_quantization_cfg
+                                                                  .get_attr_config(kernel_attr)
+                                                                  .weights_quantization_method,
+                                                                  BaseKerasInferableQuantizer)
+                kwargs = get_inferable_quantizer_kwargs(node_weights_qc[0].weights_quantization_cfg,
+                                                        QuantizationTarget.Weights,
+                                                        kernel_attr)
 
-        Args:
-            activation_quantization_fn: Function for quantazing the activations.
+                return KerasQuantizationWrapper(layer,
+                                                weights_quantizers={kernel_attr: quantier_for_node(**kwargs)})
 
-        """
-        self.activation_quantization_fn = activation_quantization_fn
+        return layer
 
-    def set_activation_quantization_params_fn(self, activation_quantization_params_fn:Callable):
+    def _get_weights_configurable_quantizer_kwargs(self, n: BaseNode, attr: str) -> Dict[str, Any]:
         """
-        Sets activation params function for the node.
+        Get the quantization parameters for a configurable quantizer.
 
         Args:
-            activation_quantization_params_fn: Function for calculating activation params.
+            n: The node for which the quantizer is being created.
+            attr: The name of the weights attribute to be quantized.
 
+        Returns:
+            The quantization parameters as a dictionary.
         """
-        self.activation_quantization_params_fn = activation_quantization_params_fn
 
-    def set_activation_quantization_param(self,
-                                          activation_params: dict):
-        """
-         Set a quantization parameter for the node's activation.
+        assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
+        node_q_cfg_candidates = n.candidates_quantization_cfg
+        # sort by descending bit width so using indices would be easier
+        node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.get_attr_config(attr).weights_n_bits,
+                                                  x.activation_quantization_cfg.activation_n_bits), reverse=True)
 
-        Args:
-            activation_params: Dictionary that contains weight quantization params.
+        float_weights = n.get_weights_by_keys(attr)
 
-        """
-        assert self.enable_activation_quantization
-        for param_name, param_value in activation_params.items():
-            self.activation_quantization_params[param_name] = param_value
+        max_cfg_candidates = n.find_max_candidates_indices()
+        if not len(max_cfg_candidates) == 1:
+            Logger.critical(f"A maximal configuration candidate must be defined; found multiple potential maximal candidates.")# pragma: no cover
 
-    def has_activation_quantization_params(self) -> bool:
-        """
+        max_candidate_idx = max_cfg_candidates[0]
 
-        Returns: Whether NodeQuantizationConfig has a activation quantization params or not.
+        return {'node_q_cfg': node_q_cfg_candidates,
+                'float_weights': float_weights,
+                'max_candidate_idx': max_candidate_idx,
+                'kernel_attr': attr,
+                }
 
+    def mixed_precision_activation_holder(self, n: BaseNode) -> KerasActivationQuantizationHolder:
         """
-        return len(self.activation_quantization_params) > 0
+        Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
+        The layer should hold either a configurable activation quantizer, if it is quantized with mixed precision,
+        or an inferable quantizer for fixed single bit-width quantization.
 
-    def no_quantization(self) -> bool:
-        """
-        Returns: Whether NodeQuantizationConfig does not have activation params.
-        """
-        return (not self.has_activation_quantization_params())
+        Args:
+            n: Node to get KerasActivationQuantizationHolder to attach in its output.
 
-    def __eq__(self, other: Any) -> bool:
+        Returns:
+            A KerasActivationQuantizationHolder layer for the node activation quantization.
         """
-        Compares the object to another object to find if they are equal.
 
-        Args:
-            other: An object to compare to.
+        activation_conf_nodes_names = [n.name for n in self.graph.get_activation_configurable_nodes()]
 
-        Returns: Whether the objects are identical or not.
+        activation_quantizers = []
+        if n.is_activation_quantization_enabled():
+            num_of_outputs = len(n.output_shape) if isinstance(n.output_shape, list) else 1
 
-        """
-        if not isinstance(other, NodeActivationQuantizationConfig):
-            return False
+            if n.name in activation_conf_nodes_names:
+                assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
+                node_q_cfg_candidates = n.candidates_quantization_cfg
 
-        return self.activation_quantization_fn == other.activation_quantization_fn and \
-               self.activation_quantization_params_fn == other.activation_quantization_params_fn and \
-               self.activation_error_method == other.activation_error_method and \
-               self.activation_quantization_method == other.activation_quantization_method and \
-               self.activation_n_bits == other.activation_n_bits and \
-               self.enable_activation_quantization == other.enable_activation_quantization and \
-               self.activation_channel_equalization == other.activation_channel_equalization and \
-               self.input_scaling == other.input_scaling and \
-               self.min_threshold == other.min_threshold and \
-               self.l_p_value == other.l_p_value and \
-               self.shift_negative_activation_correction == other.shift_negative_activation_correction and \
-               self.z_threshold == other.z_threshold and \
-               self.shift_negative_ratio == other.shift_negative_ratio and \
-               self.shift_negative_threshold_recalculation == other.shift_negative_threshold_recalculation
-
-    def __hash__(self):
-        return hash((self.activation_quantization_fn,
-                     self.activation_quantization_params_fn,
-                     self.activation_error_method,
-                     self.activation_quantization_method,
-                     self.activation_n_bits,
-                     self.enable_activation_quantization,
-                     self.activation_channel_equalization,
-                     self.input_scaling,
-                     self.min_threshold,
-                     self.l_p_value,
-                     self.shift_negative_activation_correction,
-                     self.z_threshold,
-                     self.shift_negative_ratio,
-                     self.shift_negative_threshold_recalculation))
+                # sorting the candidates by kernel attribute weights number of bits first and then by
+                # activation number of bits (in reversed order).
+                # since only kernel attribute is quantized in weights mixed precision,
+                # if the node doesn't have a kernel attribute, we only sort by activation_n_bits.
+                n.sort_node_candidates(self.fw_info)
 
+                max_cfg_candidates = n.find_max_candidates_indices()
+                assert len(max_cfg_candidates) == 1, \
+                    f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
+                max_candidate_idx = max_cfg_candidates[0]
 
-class NodeWeightsQuantizationConfig(BaseNodeQuantizationConfig):
-    """
-    Attributes for configuring the quantization of the weights of a node.
-    """
-    def __init__(self,
-                 qc: QuantizationConfig,
-                 op_cfg: OpQuantizationConfig,
-                 weights_quantization_fn: Callable,
-                 weights_quantization_params_fn: Callable,
-                 weights_channels_axis: int):
-        """
+                kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+                activation_quantizers = [ConfigurableActivationQuantizer(**{'node_q_cfg': node_q_cfg_candidates,
+                                                                            'max_candidate_idx': max_candidate_idx,
+                                                                            'kernel_attr': kernel_attr})] \
+                                        * num_of_outputs
+            else:
+                node_act_qc = n.get_unique_activation_candidates()
+                assert len(node_act_qc) == 1, f"Expecting node {n.name} to have a unique activation configuration, " \
+                                              f"but {len(node_act_qc)} different configurations exist."
+                quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
+                                                                   node_act_qc[0].activation_quantization_cfg.activation_quantization_method,
+                                                                   BaseKerasInferableQuantizer)
+                kwargs = get_inferable_quantizer_kwargs(node_act_qc[0].activation_quantization_cfg,
+                                                        QuantizationTarget.Activation)
 
-        Args:
-            qc: QuantizationConfig to create the node's config from.
-            op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
-            weights_quantization_fn: Function to use when quantizing the node's weights.
-            weights_quantization_params_fn:  Function to use when computing the threshold for quantizing a node's weights.
-            weights_channels_axis: Axis to quantize a node's kernel when quantizing per-channel.
-        """
-
-        self.weights_quantization_fn = weights_quantization_fn
-        self.weights_quantization_params_fn = weights_quantization_params_fn
-        self.weights_channels_axis = weights_channels_axis
-        self.weights_quantization_params = {}
-        self.weights_quantization_method = op_cfg.weights_quantization_method
-        self.weights_error_method = qc.weights_error_method
-        self.weights_n_bits = op_cfg.weights_n_bits
-        self.weights_bias_correction = qc.weights_bias_correction
-        self.weights_second_moment_correction = qc.weights_second_moment_correction
-        self.weights_per_channel_threshold = op_cfg.weights_per_channel_threshold
-        self.enable_weights_quantization = op_cfg.enable_weights_quantization
-        self.min_threshold = qc.min_threshold
-        self.l_p_value = qc.l_p_value
-
-
-    @property
-    def weights_error_method(self) -> QuantizationErrorMethod:
-        """
-        weights_error_method getter.
-        """
-        return self._weights_error_method
+                activation_quantizers = [quantizer_for_node(**kwargs)] * num_of_outputs
 
-    @weights_error_method.setter
-    def weights_error_method(self, value: QuantizationErrorMethod):
-        """
-        weights_error_method setter.
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node with no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return KerasActivationQuantizationHolder(activation_quantizers[0])
 
-        Args:
-            value: New weights_error_method to set to the node weights configuration.
+        Logger.critical(f"'KerasActivationQuantizationHolder' supports only one quantizer, but found {len(activation_quantizers)} for node {n}")# pragma: no cover
 
+    def build_model(self) -> Tuple[Model, UserInformation,
+                                   Dict[str, Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]]:
         """
-        self._weights_error_method = value
-        self.weights_quantization_params_fn = get_weights_quantization_params_fn(weights_quantization_method=self.weights_quantization_method)
+        Build a Keras mixed-precision model and return it.
+        Used the basic Keras model builder to build the model, and adding a mapping between each configurable node to
+        a list of layers (from the new model) that are matching to the node (either KerasQuantizationWrapper or
+        KerasActivationQuantizationHolder type layers).
+        This mapping is used during mixed precision metric computation to enforce pairs of weights-activation bit-width
+        candidates when configuring a model.
 
+        Returns: Mixed-precision Keras model.
 
-    def set_weights_quantization_fn(self, weights_quantization_fn: Callable):
         """
-        Sets weights quantization function for the node.
+        model, user_info = super().build_model()
 
-        Args:
-            weights_quantization_fn: Function for quantazing the weights.
+        # creating a mapping between graph nodes and model's layers for mixed precision configurability
+        conf_node2layers = {n.name: self._find_layers_in_model_by_node(n, model.layers)
+                            for n in self.graph.get_configurable_sorted_nodes(self.fw_info)}
 
-        """
-        self.weights_quantization_fn = weights_quantization_fn
+        return model, user_info, conf_node2layers
 
-    def set_weights_quantization_params_fn(self, weights_quantization_params_fn: Callable):
+    @staticmethod
+    def _get_weights_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasQuantizationWrapper]:
         """
-        Sets weights params function for the node.
+        Filters KerasQuantizationWrapper layers from an MP model that are matching to the given graph node.
 
         Args:
-            weights_quantization_params_fn: Function for calculating the weights params.
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-        """
-        self.weights_quantization_params_fn = weights_quantization_params_fn
+        Returns: A list of layers that responsible for the node's weights quantization.
 
-    def set_weights_quantization_param(self,
-                                       weights_params: dict):
         """
-         Set a quantization parameter for the node's weights.
-
-        Args:
-            weights_params: Dictionary that contains weight quantization params.
+        return [_l for _l in layers_list if isinstance(_l, KerasQuantizationWrapper) and _l.layer.name == n.name]
 
+    @staticmethod
+    def _get_activation_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasActivationQuantizationHolder]:
         """
-        assert self.enable_weights_quantization
-        for param_name, param_value in weights_params.items():
-            self.weights_quantization_params[param_name] = param_value
+        Filters KerasActivationQuantizationHolder layers from an MP model that are matching to the given graph node.
 
-    def calculate_and_set_weights_params(self, tensor_data: np.ndarray) -> float:
-        """
         Args:
-            tensor_data: Tensor content as Numpy array.
-
-        Returns:
-            Recalculated weights quantization params from the kernel and channel axis.
-
-        """
-        assert self.enable_weights_quantization
-        if self.weights_quantization_params_fn is not None:
-            self.set_weights_quantization_param(self.weights_quantization_params_fn(tensor_data,
-                                                                                    p=self.l_p_value,
-                                                                                    n_bits=self.weights_n_bits,
-                                                                                    per_channel=self.weights_per_channel_threshold and self.weights_channels_axis is not None,
-                                                                                    channel_axis=self.weights_channels_axis,
-                                                                                    min_threshold=self.min_threshold))
-        else:
-            return self.set_weights_quantization_param({})
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-    def has_weights_quantization_params(self) -> bool:
-        """
-
-        Returns: Whether NodeQuantizationConfig has weights quantization params or not.
+        Returns: A list of layers that responsible for the node's activation quantization.
 
         """
-        return len(self.weights_quantization_params) > 0
+        return [_l for _l in layers_list if isinstance(_l, KerasActivationQuantizationHolder)
+                and (_l.inbound_nodes[0].inbound_layers.name == n.name or
+                     (isinstance(_l.inbound_nodes[0].inbound_layers, KerasQuantizationWrapper) and
+                      _l.inbound_nodes[0].inbound_layers.layer.name == n.name))]
 
-    def __eq__(self, other: Any) -> bool:
+    def _find_layers_in_model_by_node(self, n: BaseNode, layers_list: List[Layer]) -> \
+            List[Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]:
         """
-        Compares the object to another object to find if they are equal.
+        Retrieves layers from an MP model that are matching to the given graph node, that is, these are either
+        KerasQuantizationWrapper layers or KerasActivationQuantizationHolder layers that are responsible for the graph
+        configurable model quantization.
 
         Args:
-            other: An object to compare to.
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-        Returns: Whether the objects are identical or not.
+        Returns: A list of layers that responsible for the node's quantization.
 
         """
-        if not isinstance(other, NodeWeightsQuantizationConfig):
-            return False
+        # Only layers with kernel op are considered weights configurable
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        weights_quant = False if kernel_attr is None else n.is_weights_quantization_enabled(kernel_attr)
+        act_quant = n.is_activation_quantization_enabled()
+
+        if weights_quant and not act_quant:
+            return self._get_weights_quant_layers(n, layers_list)
+        elif not weights_quant and act_quant:
+            return self._get_activation_quant_layers(n, layers_list)
+        elif weights_quant and act_quant:
+            return self._get_weights_quant_layers(n, layers_list) + self._get_activation_quant_layers(n, layers_list)
+        else:
+            Logger.critical(f"Expected node {n.name} to have either weights or activation quantization configured, but both are disabled.")# pragma: no cover
 
-        return self.weights_quantization_fn == other.weights_quantization_fn and \
-               self.weights_quantization_params_fn == other.weights_quantization_params_fn and \
-               self.weights_channels_axis == other.weights_channels_axis and \
-               self.weights_error_method == other.weights_error_method and \
-               self.weights_quantization_method == other.weights_quantization_method and \
-               self.weights_n_bits == other.weights_n_bits and \
-               self.weights_bias_correction == other.weights_bias_correction and \
-               self.weights_second_moment_correction == other.weights_second_moment_correction and \
-               self.weights_per_channel_threshold == other.weights_per_channel_threshold and \
-               self.enable_weights_quantization == other.enable_weights_quantization and \
-               self.min_threshold == other.min_threshold and \
-               self.l_p_value == other.l_p_value
-
-    def __hash__(self):
-        return hash((self.weights_quantization_fn,
-                     self.weights_quantization_params_fn,
-                     self.weights_channels_axis,
-                     self.weights_error_method,
-                     self.weights_quantization_method,
-                     self.weights_n_bits,
-                     self.weights_bias_correction,
-                     self.weights_second_moment_correction,
-                     self.weights_per_channel_threshold,
-                     self.enable_weights_quantization,
-                     self.min_threshold,
-                     self.l_p_value))
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_config.py`

 * *Files 8% similar despite different names*

```diff
@@ -46,15 +46,14 @@
 class QuantizationConfig:
 
     def __init__(self,
                  activation_error_method: QuantizationErrorMethod = QuantizationErrorMethod.MSE,
                  weights_error_method: QuantizationErrorMethod = QuantizationErrorMethod.MSE,
                  relu_bound_to_power_of_2: bool = False,
                  weights_bias_correction: bool = True,
-                 weights_per_channel_threshold: bool = True,
                  weights_second_moment_correction: bool = False,
                  input_scaling: bool = False,
                  softmax_shift: bool = False,
                  shift_negative_activation_correction: bool = False,
                  activation_channel_equalization: bool = False,
                  z_threshold: float = math.inf,
                  min_threshold: float = MIN_THRESHOLD,
@@ -69,49 +68,47 @@
 
         Args:
             activation_error_method (QuantizationErrorMethod): Which method to use from QuantizationErrorMethod for activation quantization threshold selection.
             weights_error_method (QuantizationErrorMethod): Which method to use from QuantizationErrorMethod for activation quantization threshold selection.
             relu_bound_to_power_of_2 (bool): Whether to use relu to power of 2 scaling correction or not.
             weights_bias_correction (bool): Whether to use weights bias correction or not.
             weights_second_moment_correction (bool): Whether to use weights second_moment correction or not.
-            weights_per_channel_threshold (bool): Whether to quantize the weights per-channel or not (per-tensor).
             input_scaling (bool): Whether to use input scaling or not.
             softmax_shift (bool): Whether to use softmax shift or not.
             shift_negative_activation_correction (bool): Whether to use shifting negative activation correction or not.
             activation_channel_equalization (bool): Whether to use activation channel equalization correction or not.
             z_threshold (float): Value of z score for outliers removal.
             min_threshold (float): Minimum threshold to use during thresholds selection.
             l_p_value (int): The p value of L_p norm threshold selection.
             block_collapsing (bool): Whether to collapse block one to another in the input network
             shift_negative_ratio (float): Value for the ratio between the minimal negative value of a non-linearity output to its activation threshold, which above it - shifting negative activation should occur if enabled.
             shift_negative_threshold_recalculation (bool): Whether or not to recompute the threshold after shifting negative activation.
-            shift_negative_params_search (bool): Whether to search for optimal shift and threshold in shift negative activation (experimental)
+            shift_negative_params_search (bool): Whether to search for optimal shift and threshold in shift negative activation.
 
         Examples:
             One may create a quantization configuration to quantize a model according to.
             For example, to quantize a model's weights and activation using thresholds, such that
             weights threshold selection is done using MSE, activation threshold selection is done using NOCLIPPING (min/max),
-            enabling relu_bound_to_power_of_2, weights_bias_correction, and quantizing the weights per-channel,
+            enabling relu_bound_to_power_of_2, weights_bias_correction,
             one can instantiate a quantization configuration:
 
             >>> import model_compression_toolkit as mct
-            >>> qc = mct.core.QuantizationConfig(activation_error_method=mct.core.QuantizationErrorMethod.NOCLIPPING,weights_error_method=mct.core.QuantizationErrorMethod.MSE,relu_bound_to_power_of_2=True,weights_bias_correction=True,weights_per_channel_threshold=True)
+            >>> qc = mct.core.QuantizationConfig(activation_error_method=mct.core.QuantizationErrorMethod.NOCLIPPING, weights_error_method=mct.core.QuantizationErrorMethod.MSE, relu_bound_to_power_of_2=True, weights_bias_correction=True)
 
 
             The QuantizationConfig instanse can then be passed to
             :func:`~model_compression_toolkit.ptq.keras_post_training_quantization`
 
         """
 
         self.activation_error_method = activation_error_method
         self.weights_error_method = weights_error_method
         self.relu_bound_to_power_of_2 = relu_bound_to_power_of_2
         self.weights_bias_correction = weights_bias_correction
         self.weights_second_moment_correction = weights_second_moment_correction
-        self.weights_per_channel_threshold = weights_per_channel_threshold
         self.activation_channel_equalization = activation_channel_equalization
         self.input_scaling = input_scaling
         self.softmax_shift = softmax_shift
         self.min_threshold = min_threshold
         self.shift_negative_activation_correction = shift_negative_activation_correction
         self.z_threshold = z_threshold
         self.l_p_value = l_p_value
@@ -122,15 +119,10 @@
         self.shift_negative_params_search = shift_negative_params_search
 
     def __repr__(self):
         return str(self.__dict__)
 
 
 # Default quantization configuration the library use.
-DEFAULTCONFIG = QuantizationConfig(QuantizationErrorMethod.MSE,
-                                   QuantizationErrorMethod.MSE,
-                                   relu_bound_to_power_of_2=False,
-                                   weights_bias_correction=True,
-                                   weights_second_moment_correction=False,
-                                   weights_per_channel_threshold=True,
-                                   input_scaling=False,
-                                   softmax_shift=False)
+DEFAULTCONFIG = QuantizationConfig(QuantizationErrorMethod.MSE, QuantizationErrorMethod.MSE,
+                                   relu_bound_to_power_of_2=False, weights_bias_correction=True,
+                                   weights_second_moment_correction=False, input_scaling=False, softmax_shift=False)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py`

 * *Files 26% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.quantization.quantizers.kmeans_quantizer import kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.lut_kmeans_quantizer import lut_kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import power_of_two_quantizer, \
     symmetric_quantizer, uniform_quantizer
 
 
 def get_weights_quantization_fn(weights_quantization_method: QuantizationMethod) -> Callable:
     """
@@ -36,15 +36,14 @@
 
     if weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
         quantizer_fn = power_of_two_quantizer
     elif weights_quantization_method == QuantizationMethod.SYMMETRIC:
         quantizer_fn = symmetric_quantizer
     elif weights_quantization_method == QuantizationMethod.UNIFORM:
         quantizer_fn = uniform_quantizer
-    elif weights_quantization_method == QuantizationMethod.KMEANS:
-        quantizer_fn = kmeans_quantizer
     elif weights_quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER, QuantizationMethod.LUT_SYM_QUANTIZER]:
         quantizer_fn = lut_kmeans_quantizer
     else:
-        raise Exception(
-            f'No quantizer function for the configuration of quantization method {weights_quantization_method}')
+        Logger.critical(
+            f"No quantizer function found for the specified quantization method: {weights_quantization_method}")
+
     return quantizer_fn
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,15 +14,14 @@
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.kmeans_params import kmeans_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.lut_kmeans_params import \
     lut_kmeans_tensor, lut_kmeans_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import \
     symmetric_selection_tensor, symmetric_selection_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.uniform_selection import \
     uniform_selection_histogram, uniform_selection_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import \
@@ -44,17 +43,16 @@
     elif activation_quantization_method == QuantizationMethod.SYMMETRIC:
         params_fn = symmetric_selection_histogram
     elif activation_quantization_method == QuantizationMethod.UNIFORM:
         params_fn = uniform_selection_histogram
     elif activation_quantization_method == QuantizationMethod.LUT_POT_QUANTIZER:
         params_fn = lut_kmeans_histogram
     else:
-        Logger.error(
-            f'No params function for the configuration of '
-            f'quantization method {activation_quantization_method}')  # pragma: no cover
+        Logger.critical(
+            f"No parameter function found for the specified quantization method: {activation_quantization_method}")  # pragma: no cover
     return params_fn
 
 
 def get_weights_quantization_params_fn(weights_quantization_method: QuantizationMethod) -> Callable:
     """
     Generate a function for finding weights quantization parameters.
 
@@ -66,18 +64,15 @@
     """
     if weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
         params_fn = power_of_two_selection_tensor
     elif weights_quantization_method == QuantizationMethod.SYMMETRIC:
         params_fn = symmetric_selection_tensor
     elif weights_quantization_method == QuantizationMethod.UNIFORM:
         params_fn = uniform_selection_tensor
-    elif weights_quantization_method == QuantizationMethod.KMEANS:
-        params_fn = kmeans_tensor
     elif weights_quantization_method == QuantizationMethod.LUT_POT_QUANTIZER:
         params_fn = partial(lut_kmeans_tensor, is_symmetric=False)
     elif weights_quantization_method == QuantizationMethod.LUT_SYM_QUANTIZER:
         params_fn = partial(lut_kmeans_tensor, is_symmetric=True)
     else:
-        Logger.error(
-            f'No params function for the configuration of '
-            f'quantization method {weights_quantization_method}')  # pragma: no cover
+        Logger.critical(
+            f"No parameter function found for the specified quantization method: {weights_quantization_method}")  # pragma: no cover
     return params_fn
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,12 +10,11 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import power_of_two_no_clipping_selection_min_max, \
     power_of_two_selection_histogram, power_of_two_selection_tensor
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.kmeans_params import kmeans_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.lut_kmeans_params import lut_kmeans_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import symmetric_no_clipping_selection_min_max
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.uniform_selection import uniform_no_clipping_selection_min_max
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.outlier_filter import z_score_filter
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py`

 * *Files 4% similar despite different names*

```diff
@@ -83,14 +83,15 @@
 
     Returns:
         The Lp-norm distance between the two histograms.
     """
 
     return np.sum((np.power(np.abs((q_bins - bins)[:-1]), p) * counts)) / np.sum(counts)
 
+
 def _kl_error_function(x: np.ndarray,
                        range_min: float,
                        range_max: float,
                        n_bins: int = 2048,
                        n_bits: int = 8) -> np.float32:
     """
     Compute the error function between a tensor to its quantized version.
@@ -139,14 +140,43 @@
                                bcq,
                                bv,
                                bc,
                                range_min=range_min,
                                range_max=range_max)
 
 
+def _kl_error_function_wrapper(x: np.ndarray,
+                               range_min: np.ndarray,
+                               range_max: np.ndarray,
+                               n_bins: int = 2048,
+                               n_bits: int = 8) -> np.ndarray:
+    """
+    Computes the error function between a tensor and its quantized version for each channel.
+    The error is based on the KL-divergence between the distributions.
+    The function uses a specified number of bins to compute the histogram of the float tensor.
+    It requires the threshold and number of bits used for quantization to determine the histogram's boundaries and the number of quantized bins.
+
+    Args:
+        x: Float tensor.
+        range_min: Array specifying the minimum bound of the quantization range for each channel.
+        range_max: Array specifying the maximum bound of the quantization range for each channel.
+        n_bins: Number of bins for the float histogram.
+        n_bits: Number of bits used for quantization.
+
+    Returns:
+        An array containing the KL-divergence between the float and quantized histograms of the tensor for each channel.
+
+    """
+
+    error_list = []
+    for j in range(x.shape[0]):  # iterate all channels of the tensor.
+        error_list.append(_kl_error_function(x[j], range_min[j], range_max[j], n_bins=n_bins, n_bits=n_bits))
+    return np.asarray(error_list)
+
+
 def _kl_error_histogram(q_bins: np.ndarray,
                         q_count: np.ndarray,
                         bins: np.ndarray,
                         counts: np.ndarray,
                         range_min: float,
                         range_max: float) -> np.float32:
     """
@@ -335,39 +365,41 @@
 
     return bins_subset, counts_subset
 
 
 def get_threshold_selection_tensor_error_function(quantization_method: QuantizationMethod,
                                                   quant_error_method: qc.QuantizationErrorMethod,
                                                   p: int,
+                                                  axis: int = None,
                                                   norm: bool = False,
                                                   n_bits: int = 8,
                                                   signed: bool = True) -> Callable:
     """
     Returns the error function compatible to the provided threshold method,
     to be used in the threshold optimization search for tensor quantization.
     Args:
-        quantization_method: Quantization method for threshold selection
-        quant_error_method: the requested error function type.
-        p: p-norm to use for the Lp-norm distance.
-        norm: whether to normalize the error function result.
-        n_bits: Number of bits to quantize the tensor.
-        signed: signed input
+        quantization_method: Method used for selecting the quantization threshold.
+        quant_error_method: Type of error function requested.
+        p: P-norm to use for calculating the Lp-norm distance.
+        axis: Axis along which the operation has been performed.
+        norm: Indicates whether to normalize the result of the error function.
+        n_bits: Number of bits used to quantize the tensor.
+        signed: Indicates whether the input is signed.
 
     Returns: a Callable method that calculates the error between a tensor and a quantized tensor.
     """
 
     quant_method_error_function_mapping = {
-        qc.QuantizationErrorMethod.MSE: lambda x, y, threshold: compute_mse(x, y, norm=norm),
-        qc.QuantizationErrorMethod.MAE: lambda x, y, threshold: compute_mae(x, y, norm=norm),
-        qc.QuantizationErrorMethod.LP: lambda x, y, threshold: compute_lp_norm(x, y, p=p, norm=norm),
+        qc.QuantizationErrorMethod.MSE: lambda x, y, threshold: compute_mse(x, y, norm=norm, axis=axis),
+        qc.QuantizationErrorMethod.MAE: lambda x, y, threshold: compute_mae(x, y, norm=norm, axis=axis),
+        qc.QuantizationErrorMethod.LP: lambda x, y, threshold: compute_lp_norm(x, y, p=p, norm=norm, axis=axis),
         qc.QuantizationErrorMethod.KL:
-            lambda x, y, threshold: _kl_error_function(x, range_min=threshold[0], range_max=threshold[1],
+            lambda x, y, threshold: _kl_error_function_wrapper(x, range_min=threshold[:,0], range_max=threshold[:,1],
                                                        n_bits=n_bits) if quantization_method == QuantizationMethod.UNIFORM
-            else _kl_error_function(x, range_min=0 if not signed else -threshold, range_max=threshold, n_bits=n_bits)
+            else _kl_error_function_wrapper(x, range_min=0 if not signed else -threshold, range_max=threshold, n_bits=n_bits)
     }
 
     return quant_method_error_function_mapping[quant_error_method]
 
 
 def get_threshold_selection_histogram_error_function(quantization_method: QuantizationMethod,
                                                      quant_error_method: qc.QuantizationErrorMethod,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py`

 * *Files 12% similar despite different names*

```diff
@@ -13,16 +13,16 @@
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 from sklearn.cluster import KMeans
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL, \
-    MULTIPLIER_N_BITS, THRESHOLD
+from model_compression_toolkit.constants import LUT_VALUES, MIN_THRESHOLD, SCALE_PER_CHANNEL, \
+    LUT_VALUES_BITWIDTH, THRESHOLD
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import \
     max_power_of_two, int_quantization_with_threshold
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import \
     symmetric_selection_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import \
     power_of_two_selection_tensor
 
@@ -37,15 +37,15 @@
                       n_iter: int = 10,
                       min_threshold: float = MIN_THRESHOLD,
                       quant_error_method: qc.QuantizationErrorMethod = None,
                       is_symmetric=False) -> dict:
     """
     The quantizer first finds the closest max value per channel of tensor_data.
     Now, we divide tensor_data with the threshold vector per channel. In addition, we scale the result to the range
-    [-2^(MULTIPLIER_N_BITS-1), 2^(MULTIPLIER_N_BITS-1)-1].
+    [-2^(LUT_VALUES_BITWIDTH-1), 2^(LUT_VALUES_BITWIDTH-1)-1].
     Next, we take the scaled tensor_data and perform k-means clustering with 2^nbit clusters.
     We return the rounded cluster centers, and threshold per channel. We use these to quantize the data.
     Args:
         tensor_data: Tensor content as Numpy array.
         p: p-norm to use for the Lp-norm distance.
         n_bits: Number of bits to quantize the tensor.
         per_channel: Whether the quantization should be per-channel or not.
@@ -55,33 +55,37 @@
         quant_error_method: an error function to optimize the parameters' selection accordingly (not used for this method).
         is_symmetric (bool): Whether to apply symmetric weight quantization (default is False, meaning power of 2 quantization)
 
     Returns:
         A dictionary containing the cluster assignments according to the k-means algorithm,
         the thresholds per channel and the multiplier num bits.
     """
-    if n_bits > MULTIPLIER_N_BITS:
-        Logger.critical(f'Look-Up-Table bit configuration has {n_bits} bits, but must be less or equal to '
-                        f'{MULTIPLIER_N_BITS}')  # pragma: no cover
+    if n_bits >= LUT_VALUES_BITWIDTH:
+        Logger.critical(f'Look-Up-Table (LUT) bit configuration exceeds maximum: {n_bits} bits provided, must be less than {LUT_VALUES_BITWIDTH} bits.')  # pragma: no cover
     # TODO: need to set this externally
     if len(np.unique(tensor_data.flatten())) < 2 ** n_bits:
         n_clusters = len(np.unique(tensor_data.flatten()))
     else:
         n_clusters = 2 ** n_bits
-    kmeans = KMeans(n_clusters=n_clusters)
+    kmeans = KMeans(n_clusters=n_clusters, n_init=10)
 
     threshold_selection_tensor = symmetric_selection_tensor if is_symmetric else power_of_two_selection_tensor
     thresholds_per_channel = threshold_selection_tensor(tensor_data, p, n_bits, per_channel,
                                                         channel_axis, n_iter, min_threshold,
                                                         qc.QuantizationErrorMethod.NOCLIPPING)[THRESHOLD]
 
-    tensor_for_kmeans = int_quantization_with_threshold(tensor_data, thresholds_per_channel, MULTIPLIER_N_BITS)
+    tensor_for_kmeans = int_quantization_with_threshold(tensor_data, thresholds_per_channel, LUT_VALUES_BITWIDTH)
     kmeans.fit(tensor_for_kmeans.reshape(-1, 1))
 
-    return {CLUSTER_CENTERS: np.round(kmeans.cluster_centers_),
+    # Add 0 to the LUT
+    cc = np.round(kmeans.cluster_centers_)
+    closest2zero_idx = (np.abs(cc - 0)).argmin()
+    cc[closest2zero_idx] = 0.0
+
+    return {LUT_VALUES: cc,
             SCALE_PER_CHANNEL: thresholds_per_channel}
 
 
 def lut_kmeans_histogram(bins: np.ndarray,
                          counts: np.ndarray,
                          p: int,
                          n_bits: int,
@@ -111,27 +115,26 @@
         quant_error_method: an error function to optimize the parameters' selection accordingly (not used for this method).
 
     Returns:
         A dictionary containing the cluster assignments according to the k-means algorithm and
         the threshold for pre-clustering quantization.
     """
 
-    if n_bits >= MULTIPLIER_N_BITS:
-        Logger.critical(f'Look-Up-Table bit configuration has {n_bits} bits. It must be less then '
-                        f'{MULTIPLIER_N_BITS}')  # pragma: no cover
+    if n_bits >= LUT_VALUES_BITWIDTH:
+        Logger.critical(f'Look-Up-Table (LUT) bit configuration exceeds maximum: {n_bits} bits provided, must be less than {LUT_VALUES_BITWIDTH} bits.')  # pragma: no cover
 
     bins_with_values = np.abs(bins)[1:][counts > 0]
     if len(np.unique(bins_with_values.flatten())) < 2 ** n_bits:
         n_clusters = len(np.unique(bins_with_values.flatten()))
     else:
         n_clusters = 2 ** n_bits
 
-    kmeans = KMeans(n_clusters=n_clusters)
+    kmeans = KMeans(n_clusters=n_clusters, n_init=10)
     tensor_max = np.max(bins_with_values)
     threshold = max_power_of_two(tensor_max, min_threshold)
 
     signed = np.any(bins[:-1][counts != 0] < 0)  # Whether histogram contains negative values or not.
-    tensor_for_kmeans = int_quantization_with_threshold(data=bins, threshold=threshold, n_bits=MULTIPLIER_N_BITS, signed=signed)
+    tensor_for_kmeans = int_quantization_with_threshold(data=bins, threshold=threshold, n_bits=LUT_VALUES_BITWIDTH, signed=signed)
     kmeans.fit(tensor_for_kmeans.reshape(-1, 1), sample_weight=np.insert(counts, 0, 0))
 
-    return {CLUSTER_CENTERS: np.float32(np.round(kmeans.cluster_centers_)),
+    return {LUT_VALUES: np.float32(np.round(kmeans.cluster_centers_)),
             THRESHOLD: threshold}
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,16 +51,17 @@
     """
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits)
         threshold = max_power_of_two(tensor_max, min_threshold)
     else:
         signed = True  # weights are always signed
+        axis = -1 if per_channel else None
         error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.POWER_OF_TWO,
-                                                                       quant_error_method, p, norm=False, n_bits=n_bits,
+                                                                       quant_error_method, p, axis=axis, norm=False, n_bits=n_bits,
                                                                        signed=signed)
         threshold = qparams_selection_tensor_search(error_function,
                                                     tensor_data,
                                                     n_bits,
                                                     per_channel=per_channel,
                                                     channel_axis=channel_axis,
                                                     n_iter=n_iter,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,61 +8,68 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from tqdm import tqdm
 from typing import List
 
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation import \
-    get_weights_qparams, get_channels_axis
+    get_weights_qparams
+from model_compression_toolkit.logger import Logger
 
 
 def calculate_quantization_params(graph: Graph,
-                                  fw_info: FrameworkInfo,
                                   nodes: List[BaseNode] = [],
-                                  specific_nodes: bool = False,
-                                  fw_impl: FrameworkImplementation = None):
+                                  specific_nodes: bool = False):
     """
     For a graph, go over its nodes, compute quantization params (for both weights and activations according
     to the given framework info), and create and attach a NodeQuantizationConfig to each node (containing the
     computed params).
     By default, the function goes over all nodes in the graph. However, the specific_nodes flag enables
-    to compute quantization paramss for specific nodes if the default behavior is unnecessary. For that,
-    a list of nodes nodes should be passed as well.
+    to compute quantization params for specific nodes if the default behavior is unnecessary. For that,
+    a list of nodes should be passed as well.
 
     Args:
-        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
         groups of layers by how they should be quantized, etc.)
         graph: Graph to compute its nodes' thresholds.
         nodes: List of nodes to compute their thresholds instead of computing it for all nodes in the graph.
         specific_nodes: Flag to compute thresholds for only specific nodes.
-        fw_impl: FrameworkImplementation with specific framework implementations.
 
     """
 
+    Logger.info(f"Running quantization parameters search. "
+                f"This process might take some time, "
+                f"depending on the model size and the selected quantization methods.\n")
+
     # Create a list of nodes to compute their thresholds
     nodes_list: List[BaseNode] = nodes if specific_nodes else graph.nodes()
 
-    for n in nodes_list:  # iterate only nodes that we should compute their thresholds
+    for n in tqdm(nodes_list, "Calculating quantization params"):  # iterate only nodes that we should compute their thresholds
         for candidate_qc in n.candidates_quantization_cfg:
-            if n.is_weights_quantization_enabled():
-                # If node's weights should be quantized, we compute its weights' quantization parameters
-                output_channels_axis, _ = get_channels_axis(candidate_qc.weights_quantization_cfg, fw_info, n.type)
-                weights_params = get_weights_qparams(n.get_weights_by_keys(fw_impl.constants.KERNEL),
-                                                     candidate_qc.weights_quantization_cfg,
-                                                     output_channels_axis)
-                candidate_qc.weights_quantization_cfg.set_weights_quantization_param(weights_params)
-                candidate_qc.weights_quantization_cfg.weights_channels_axis = output_channels_axis
+            for attr in n.get_node_weights_attributes():
+                if n.is_weights_quantization_enabled(attr):
+                    # If the node's weights attribute should be quantized, we compute its quantization parameters
+                    attr_cfg = candidate_qc.weights_quantization_cfg.get_attr_config(attr)
+                    channels_axis = attr_cfg.weights_channels_axis
+                    if channels_axis is not None:
+                        output_channels_axis = channels_axis[0]
+                    else:
+                        output_channels_axis = None
+                    weights_params = get_weights_qparams(n.get_weights_by_keys(attr),
+                                                         candidate_qc.weights_quantization_cfg,
+                                                         attr_cfg,
+                                                         output_channels_axis)
+                    attr_cfg.set_weights_quantization_param(weights_params)
+
             if n.is_activation_quantization_enabled():
                 # If node's activations should be quantized as well, we compute its activation quantization parameters
                 activation_params = get_activations_qparams(
                     activation_quant_cfg=candidate_qc.activation_quantization_cfg,
                     nodes_prior_info=n.prior_info,
                     out_stats_container=graph.get_out_stats_collector(n))
                 # Create a NodeQuantizationConfig containing all quantization params and attach it to the node
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -722,13 +722,11 @@
 
     Args:
         error_function: error_function: Function to compute the error between the original and quantized tensors.
         float_tensor: Numpy array with float tensor's content.
         q_tensor: Numpy array with quantized tensor's content.
         in_params: Quantization params the tensor is quantized by (used in specific error functions only).
 
-    Returns: A list of error values per-channel for the quantized tensor, according to the error function.
+    Returns: An array of error values for each channel of the quantized tensor, as determined by the specified error function.
     """
-    _error_per_list = []
-    for j in range(float_tensor.shape[0]):  # iterate all channels of the tensor.
-        _error_per_list.append(error_function(float_tensor[j, :], q_tensor[j, :], in_params[j]))
-    return np.asarray(_error_per_list)
+    return error_function(float_tensor, q_tensor, in_params)
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,16 @@
 
     tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits)
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         threshold = get_init_threshold(min_threshold, tensor_max, per_channel)
     else:
         signed = True  # weights are always signed
-        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.SYMMETRIC, quant_error_method, p, norm=False, n_bits=n_bits, signed=signed)
+        axis = -1 if per_channel else None
+        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.SYMMETRIC, quant_error_method, p, axis=axis, norm=False, n_bits=n_bits, signed=signed)
         threshold = qparams_symmetric_selection_tensor_search(error_function,
                                                               tensor_data,
                                                               tensor_max,
                                                               n_bits,
                                                               per_channel,
                                                               channel_axis,
                                                               min_threshold=min_threshold,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,16 @@
     """
     tensor_min = get_tensor_min(tensor_data, per_channel, channel_axis)
     tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits, is_uniform_quantization=True)
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         mm = tensor_min, tensor_max
     else:
-        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.UNIFORM, quant_error_method, p, norm=False)
+        axis = -1 if per_channel else None
+        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.UNIFORM, quant_error_method, p, axis=axis, norm=False)
         mm = qparams_uniform_selection_tensor_search(error_function,
                                                      tensor_data,
                                                      tensor_min,
                                                      tensor_max,
                                                      n_bits,
                                                      per_channel,
                                                      channel_axis)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,44 +15,40 @@
 
 import copy
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
-from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
+from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_weights_attr_by_qc
 from model_compression_toolkit.logger import Logger
 
 
-def quantize_graph_weights(graph: Graph,
-                           fw_info: FrameworkInfo,
-                           fw_impl: FrameworkImplementation) -> Graph:
+def quantize_graph_weights(graph: Graph) -> Graph:
     """
     Get a graph representing a model, and quantize its nodes' weights.
     Each node is quantized according to the passed framework info and quantization configuration.
     If weights bias correction is enabled in the quantization configuration, a bias correction term
     is calculated and subtracted from the original node's bias. The graph is quantized in-place.
 
     Args:
         graph: Graph to quantize its nodes.
-        fw_info: Framework information needed for quantizing the graph's nodes' weights and activations.
-        fw_impl: FrameworkImplementation with specific framework implementations.
 
     """
     # Iterate over nodes in the graph and quantize each node's weights and activations
     # (according to operators groups in framework info).
     for n in graph.nodes():
+        for attr in n.get_node_weights_attributes():
+            if n.is_weights_quantization_enabled(attr):
+                quantized_attr, io_channels_axes = \
+                    get_quantized_weights_attr_by_qc(attr,
+                                                     n,
+                                                     n.final_weights_quantization_cfg.get_attr_config(attr))
+
+                Logger.debug(
+                    f'Weights attribute: {attr} of node name: {n.name} has the following quantization params: '
+                    f'{str(n.final_weights_quantization_cfg.get_attr_config(attr).weights_quantization_params)}')
 
-        if n.is_weights_quantization_enabled():
-            quantized_kernel, io_channels_axes = get_quantized_kernel_by_weights_qc(fw_info,
-                                                                                    n,
-                                                                                    n.final_weights_quantization_cfg,
-                                                                                    fw_impl=fw_impl)
-
-            Logger.debug(
-                f'Node name: {n.name} has the following quantization params: '
-                f'{str(n.final_weights_quantization_cfg.weights_quantization_params)}')
-
-            # Set the kernel node to be the quantized kernel.
-            n.set_weights_by_keys(fw_impl.constants.KERNEL, quantized_kernel)
+                # Set the attribute to be the quantized attribute.
+                n.set_weights_by_keys(attr, quantized_attr)
 
     return graph
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantize_node.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/exporter.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,55 +10,81 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from model_compression_toolkit.core import common
-from model_compression_toolkit.logger import Logger
+from typing import Tuple, Any
+
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation \
-    import \
-    get_channels_axis
+from model_compression_toolkit.core.common import FrameworkInfo
+from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
+from model_compression_toolkit.core.common.quantization.quantize_graph_weights import quantize_graph_weights
+
+from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
+from model_compression_toolkit.core.common.user_info import UserInformation
+
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
+
+
+def _quantize_model(tg: Graph,
+                    fw_info: FrameworkInfo,
+                    fw_impl: FrameworkImplementation,
+                    tb_w: TensorboardWriter) -> Tuple[Any, UserInformation]:
+    """
+    Quantize graph's weights, and build a quantized framework model from it.
+
+    Args:
+        tg: A prepared for quantization graph.
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tb_w: TensorBoardWriter object to log events.
+
+    Returns:
+        Quantized model in the input framework, and information the user may need in order to use the quantized model.
+    """
 
+    quantized_tg = quantize_graph_weights(tg)
+    if tb_w is not None:
+        tb_w.add_graph(quantized_tg, 'after_quantization')
+    ######################################
+    # Back2Framework
+    ######################################
+    # Before building a quantized model, first apply some substitutions.
+    quantized_tg = substitute(quantized_tg,
+                              fw_impl.get_substitutions_pre_build())
+
+    quantized_model, user_info = fw_impl.model_builder(quantized_tg,
+                                                       mode=ModelBuilderMode.QUANTIZED,
+                                                       fw_info=fw_info)
+    return quantized_model, user_info
+
+
+def export_model(tg,
+                 fw_info,
+                 fw_impl,
+                 tb_w,
+                 bit_widths_config):
 
-def get_quantized_kernel_by_weights_qc(fw_info: FrameworkInfo,
-                                       n: BaseNode,
-                                       weights_qc: NodeWeightsQuantizationConfig,
-                                       fw_impl: FrameworkImplementation):
     """
-    For a node and weights quantization configuration, compute
-    the quantized kernel of the node and return it and the input/output channels indices.
+    A function for quantizing the graph's weights and build a quantized framework model from it.
 
     Args:
-        fw_info: A FrameworkInfo object Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
-        n: Node to quantize its kernel.
-        weights_qc: Weight quantization configuration to use for the quantization.
-        fw_impl: FrameworkImplementation with specific framework implementations.
+        tg: A prepared for quantization graph.
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tb_w: TensorBoardWriter object to log events.
+        bit_widths_config: mixed-precision bit configuration to be added to model user_info
 
     Returns:
-        A quantized kernel of the node using a weights quantization configuration.
+        Quantized model in the input framework, and information the user may need in order to use the quantized model.
     """
+    quantized_model, user_info = _quantize_model(tg,
+                                                 fw_info,
+                                                 fw_impl,
+                                                 tb_w)
+    user_info.mixed_precision_cfg = bit_widths_config
 
-    # If weights should be quantized per-channel but a kernel channels mapping is missing.
-    if weights_qc.weights_per_channel_threshold and fw_info.kernel_channels_mapping is \
-            None:
-        Logger.warning(
-            'Weights Per Channel Quantization requires channel mapping function but framework info '
-            'does not contain one')
-    output_channels_axis, input_channels_axis = get_channels_axis(weights_qc,
-                                                                  fw_info,
-                                                                  n.type)
-
-    Logger.debug(f'quantizing {n.name} with {weights_qc.weights_n_bits} bits')
-    quantized_kernel = weights_qc.weights_quantization_fn(n.get_weights_by_keys(fw_impl.constants.KERNEL),
-                                                          n_bits=weights_qc.weights_n_bits,
-                                                          signed=True,
-                                                          quantization_params=weights_qc.weights_quantization_params,
-                                                          per_channel=weights_qc.weights_per_channel_threshold,
-                                                          output_channels_axis=output_channels_axis)
+    return quantized_model, user_info
 
-    return quantized_kernel, (input_channels_axis, output_channels_axis)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,45 +9,49 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from sklearn.cluster import KMeans
 import numpy as np
 
-from model_compression_toolkit.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL
-from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters
+from model_compression_toolkit.constants import LUT_VALUES, SCALE_PER_CHANNEL, \
+    LUT_VALUES_BITWIDTH
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters, \
+    get_quantized_tensor, int_quantization_with_threshold
 
 
-def kmeans_quantizer(tensor_data: np.ndarray,
+def lut_kmeans_quantizer(tensor_data: np.ndarray,
                         n_bits: int,
                         signed: bool,
                         quantization_params: dict,
                         per_channel: bool,
                         output_channels_axis: int) -> np.ndarray:
     """
-    Quantize a tensor according to k-means algorithm. This function assigns cluster centers
-    to the tensor data values.
+    Quantize a tensor with given cluster centers and thresholds-per-channel vector.
+    1. We divide tensor_data with the scale vector per channel.
+    2. We scale the result to the range [-2^(LUT_VALUES_BITWIDTH-1), 2^(LUT_VALUES_BITWIDTH-1)-1].
+    3. We assign cluster centers to every value, multiply by thresholds_per_channel and divide by 2^(LUT_VALUES_BITWIDTH-1).
+    The result is the quantized tensor.
+
 
     Args:
         tensor_data: Tensor values to quantize.
         n_bits: Number of bits to quantize the tensor.
         signed: Whether the tensor contains negative values or not.
         quantization_params: Dictionary of specific parameters for this quantization function.
         per_channel: Whether to use separate quantization per output channel.
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
-    eps = 1e-8
-    cluster_centers = quantization_params[CLUSTER_CENTERS]
-    scales_per_channel = quantization_params[SCALE_PER_CHANNEL]
-    tensor = (tensor_data / (scales_per_channel + eps))
+    lut_values = quantization_params[LUT_VALUES]
+    thresholds_per_channel = quantization_params[SCALE_PER_CHANNEL]
+    tensor = int_quantization_with_threshold(tensor_data, thresholds_per_channel, LUT_VALUES_BITWIDTH)
     shape_before_kmeans = tensor.shape
-    cluster_assignments = kmeans_assign_clusters(cluster_centers, tensor.reshape(-1, 1))
-    quant_tensor = cluster_centers[cluster_assignments].reshape(shape_before_kmeans)
-    if per_channel:
-        quant_tensor = (quant_tensor * scales_per_channel)
+    cluster_assignments = kmeans_assign_clusters(lut_values, tensor.reshape(-1, 1))
+    quant_tensor = get_quantized_tensor(lut_values[cluster_assignments].reshape(shape_before_kmeans),
+                                        thresholds_per_channel,
+                                        LUT_VALUES_BITWIDTH)
     return quant_tensor
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py`

 * *Files 1% similar despite different names*

```diff
@@ -147,30 +147,30 @@
     clipped_tensor = np.clip(tensor_data, a_min=a, a_max=b)
 
     # Quantize the data between min/max of quantization range.
     q = delta * np.round((clipped_tensor - a) / delta) + a
     return q
 
 
-def kmeans_assign_clusters(cluster_centers: np.ndarray,
+def kmeans_assign_clusters(lut_values: np.ndarray,
                            query: np.ndarray) -> np.ndarray:
     """
     Assign each data value in query with its closest cluster center point.
     Args:
-        cluster_centers: the cluster centers to assign the query values.
+        lut_values: the cluster centers to assign the query values.
         query: values for which to assign cluster centers.
 
     Returns: A tensor of indexes to the cluster centers that where assigned to each value in
              the query tensor.
 
     """
     d0 = query.shape[0]
-    d1 = cluster_centers.shape[0]
+    d1 = lut_values.shape[0]
     query_ = query.repeat(d1).reshape(d0, d1)
-    cluster_centers_ = cluster_centers.repeat(d0).reshape(d1, d0).transpose(1, 0)
+    cluster_centers_ = lut_values.repeat(d0).reshape(d1, d0).transpose(1, 0)
     return np.argmin(np.abs(query_ - cluster_centers_), axis=1)
 
 
 def int_quantization_with_threshold(data: np.ndarray,
                                     threshold: np.ndarray,
                                     n_bits: int,
                                     signed: bool = True,
@@ -234,15 +234,15 @@
         n_bits: number of bits the tensor will be quantized with
         is_uniform_quantization (bool): Whether the tensor will be quantized with uniform quantization (min-max)
 
     Returns: maximal value (or values).
 
     """
     if n_bits < 1:
-        Logger.error("n_bits must be positive")
+        Logger.critical(f"Parameter n_bits must be positive; however 'n_bits'={n_bits} was provided.")
     if is_uniform_quantization:
         expansion_factor = 1.0
     elif n_bits == 1:
         expansion_factor = 0.0
     else:
         expansion_factor = np.power(2.0, n_bits - 1) / (np.power(2.0, n_bits - 1) - 1)
     if per_channel:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py`

 * *Files 3% similar despite different names*

```diff
@@ -48,19 +48,21 @@
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
-        Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must be defined in 'quantization_params'")  # pragma: no cover
+
     if not threshold_is_power_of_two(threshold, per_channel):
-        Logger.error(f"Expects {THRESHOLD} parameter to be a power of two, but got {threshold}")  # pragma: no cover
+        Logger.critical(f"Expected '{THRESHOLD}' parameter to be a power of two, but received {threshold}.")# pragma: no cover
+
     if (per_channel and (threshold <= 0).any()) or ((not per_channel) and threshold <= 0):
-        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must positive")  # pragma: no cover
 
 
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
@@ -84,18 +86,18 @@
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
-        Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must be defined in 'quantization_params'")  # pragma: no cover
 
     if (per_channel and np.any(threshold <= 0)) or (not per_channel and threshold <= 0):
-        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must positive")  # pragma: no cover
 
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
 
@@ -118,10 +120,10 @@
 
     Returns:
         Quantized data.
     """
     range_min = quantization_params.get(RANGE_MIN)
     range_max = quantization_params.get(RANGE_MAX)
     if range_min is None or range_max is None:
-        Logger.error("'quantization range' parameters must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical("'quantization range' parameters must be defined in 'quantization_params'")  # pragma: no cover
 
     return uniform_quantize_tensor(tensor_data, range_min, range_max, n_bits)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
 import copy
-from typing import List
+from typing import List, Tuple
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
@@ -67,29 +67,33 @@
     Create and set quantization configurations to a node (for both weights and activation).
 
     Args:
         node: Node to set its quantization configurations.
         quant_config: Quantization configuration to generate the node's configurations from.
         fw_info: Information needed for quantization about the specific framework.
         tpc: TargetPlatformCapabilities to get default OpQuantizationConfig.
-        mixed_precision_enable: is mixed precision enabled
+        mixed_precision_enable: is mixed precision enabled.
     """
     node_qc_options = node.get_qco(tpc)
 
     # Create QC candidates for weights and activation combined
-    weight_channel_axis = fw_info.kernel_channels_mapping.get(node.type)[0]
+    weight_channel_axis = fw_info.kernel_channels_mapping.get(node.type)
     node.candidates_quantization_cfg = _create_node_candidates_qc(quant_config,
                                                                   fw_info,
                                                                   weight_channel_axis,
                                                                   node_qc_options,
+                                                                  node,
                                                                   mixed_precision_enable=mixed_precision_enable)
 
+    # sorting the candidates by kernel attribute weights number of bits first and then by activation number of bits
+    # (in reversed order). since only kernel attribute is quantized in weights mixed precision,
+    # if the node doesn't have a kernel attribute, we only sort by activation_n_bits.
+    node.sort_node_candidates(fw_info)
+
     for candidate_qc in node.candidates_quantization_cfg:
-        candidate_qc.weights_quantization_cfg.enable_weights_quantization = \
-            candidate_qc.weights_quantization_cfg.enable_weights_quantization and node.has_weights_to_quantize(fw_info)
         candidate_qc.activation_quantization_cfg.enable_activation_quantization = \
             candidate_qc.activation_quantization_cfg.enable_activation_quantization and node.get_has_activation()
 
 
 def create_node_activation_qc(qc: QuantizationConfig,
                               fw_info: FrameworkInfo,
                               op_cfg: OpQuantizationConfig) -> NodeActivationQuantizationConfig:
@@ -104,99 +108,106 @@
 
     Returns:
         Activation quantization configuration of a node.
     """
 
     activation_quantization_fn = fw_info.activation_quantizer_mapping.get(op_cfg.activation_quantization_method)
     if activation_quantization_fn is None:
-        Logger.critical('Unknown quantization method for activations')  # pragma: no cover
+        Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
     activation_quantization_params_fn = get_activation_quantization_params_fn(op_cfg.activation_quantization_method)
 
     return NodeActivationQuantizationConfig(qc,
                                             op_cfg,
                                             activation_quantization_fn,
                                             activation_quantization_params_fn)
 
 
-def create_node_qc_candidate(qc: QuantizationConfig,
-                             fw_info: FrameworkInfo,
-                             weight_channel_axis: int,
-                             op_cfg: OpQuantizationConfig) -> CandidateNodeQuantizationConfig:
+def _create_node_single_candidate_qc(qc: QuantizationConfig,
+                                     fw_info: FrameworkInfo,
+                                     weight_channel_axis: Tuple[int, int],
+                                     op_cfg: OpQuantizationConfig,
+                                     node_attrs_list: List[str]) -> CandidateNodeQuantizationConfig:
     """
     Create quantization configuration candidate from a QuantizationConfig object.
     Creates both weights and activation quantization configurations
     and initialize a candidate object that encapsulates both.
 
     Args:
         qc: QuantizationConfig to create the node's config from.
         fw_info: Information about the specific framework the node was created from (e.g., whether its
             weights/activations should be quantized)
-        weight_channel_axis: Output channel index of the node's kernel.
+        weight_channel_axis: (Output, Input) channel index of the node's kernel.
         op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
+        node_attrs_list: A list of the node's weights attributes names.
 
     Returns: a CandidateNodeQuantizationConfig object with both weights and activation quantization config objects.
 
     """
 
-    # get attributes for weights quantization
-    weights_quantization_fn = get_weights_quantization_fn(op_cfg.weights_quantization_method)
-
-    if weights_quantization_fn is None:
-        Logger.critical('Unknown quantization method for weights')  # pragma: no cover
+    # parameters for weights attributes quantization are set within  CandidateNodeQuantizationConfig initialization
 
-    weights_quantization_params_fn = get_weights_quantization_params_fn(op_cfg.weights_quantization_method)
-
-    # get attributes for activation quantization
+    # get parameters for activation quantization
     activation_quantization_fn = fw_info.activation_quantizer_mapping.get(op_cfg.activation_quantization_method)
     if activation_quantization_fn is None:
-        Logger.critical('Unknown quantization method for activations')  # pragma: no cover
+        Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
     activation_quantization_params_fn = get_activation_quantization_params_fn(op_cfg.activation_quantization_method)
 
+    # TODO: remove this validation and warning once enabling all attributes quantization by default
+    attrs_with_enabled_quantization = [attr for attr, cfg in op_cfg.attr_weights_configs_mapping.items()
+                                       if cfg.enable_weights_quantization]
+    if len(attrs_with_enabled_quantization) > 1:
+        Logger.warning(f"Multiple weights attributes quantization is enabled via the provided TPC."
+                       f"Quantizing any attribute other than the kernel is experimental "
+                       f"and may be subject to unstable behavior."
+                       f"Attributes with enabled weights quantization: {attrs_with_enabled_quantization}.")
+
     return CandidateNodeQuantizationConfig(qc=qc,
                                            op_cfg=op_cfg,
                                            activation_quantization_fn=activation_quantization_fn,
                                            activation_quantization_params_fn=activation_quantization_params_fn,
-                                           weights_quantization_fn=weights_quantization_fn,
-                                           weights_quantization_params_fn=weights_quantization_params_fn,
-                                           weight_channel_axis=weight_channel_axis)
+                                           weights_channels_axis=weight_channel_axis,
+                                           node_attrs_list=node_attrs_list)
 
 
 def _create_node_candidates_qc(qc: QuantizationConfig,
                                fw_info: FrameworkInfo,
-                               weight_channel_axis: int,
+                               weight_channel_axis: Tuple[int, int],
                                node_qc_options: QuantizationConfigOptions,
+                               node: BaseNode,
                                mixed_precision_enable: bool = False) -> List[CandidateNodeQuantizationConfig]:
     """
     Create a list of candidates of weights and activation quantization configurations for a node.
 
     Args:
         qc: Quantization configuration the quantization process should follow.
         fw_info: Framework information (e.g., which layers should have their kernels' quantized).
-        weight_channel_axis: Output channel index of the node's kernel.
+        weight_channel_axis: (Output, Input) channel index of the node's kernel.
         node_qc_options: QuantizationConfigOptions for the node with quantization candidates information.
+        node: A node to set quantization configuration candidates to.
         mixed_precision_enable: is mixed precision enabled
 
     Returns:
         List of candidates of weights quantization configurations to set for a node.
     """
 
     candidates = []
+    node_attrs_list = node.get_node_weights_attributes()
+
     if mixed_precision_enable:
         for op_cfg in node_qc_options.quantization_config_list:
-            candidate_nbits_qc = copy.deepcopy(qc)
-            candidates.append(create_node_qc_candidate(candidate_nbits_qc,
-                                                       fw_info,
-                                                       weight_channel_axis,
-                                                       op_cfg))
-        # sorting the candidates by weights number of bits first and then by activation number of bits
-        # (in reversed order)
-        candidates.sort(key=lambda c: (c.weights_quantization_cfg.weights_n_bits,
-                                       c.activation_quantization_cfg.activation_n_bits), reverse=True)
+            candidate_qc = copy.deepcopy(qc)
+            candidates.append(_create_node_single_candidate_qc(candidate_qc,
+                                                               fw_info,
+                                                               weight_channel_axis,
+                                                               op_cfg,
+                                                               node_attrs_list))
+
     else:
-        candidates.append(create_node_qc_candidate(qc,
-                                                   fw_info,
-                                                   weight_channel_axis,
-                                                   node_qc_options.base_config))
+        candidates.append(_create_node_single_candidate_qc(qc,
+                                                           fw_info,
+                                                           weight_channel_axis,
+                                                           node_qc_options.base_config,
+                                                           node_attrs_list))
 
     return candidates
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/similarity_analyzer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/similarity_analyzer.py`

 * *Files 20% similar despite different names*

```diff
@@ -47,27 +47,34 @@
     Returns:
         Lp norm per sample in batch of tensor x.
     """
 
     return (np.abs(x) ** p).sum(axis=-1) ** (1.0/p)
 
 
-def flatten_tensor(t: np.ndarray, batch: bool) -> np.ndarray:
+def flatten_tensor(t: np.ndarray, batch: bool, axis: int = None) -> np.ndarray:
     """
     Flattening the samples batch to allow similarity analysis computation per sample.
 
     Args:
         t: A tensor to be flattened.
         batch: Whether the similarity computation is per image or per tensor.
+        axis: Axis along which the operator has been computed.
 
     Returns: A flattened tensor which has the number of samples as is first dimension.
 
     """
 
-    if batch:
+    if axis is not None and batch:
+        t = np.moveaxis(t, axis, -1)
+        f_t = t.reshape([t.shape[0], -1, t.shape[-1]])
+    elif axis is not None:
+        t = np.moveaxis(t, axis, -1)
+        f_t = t.reshape([-1, t.shape[-1]])
+    elif batch:
         f_t = t.reshape([t.shape[0], -1])
     else:
         f_t = t.flatten()
 
     return f_t
 
 #########################
@@ -75,92 +82,98 @@
 #########################
 
 
 def compute_mse(float_tensor: np.ndarray,
                 fxp_tensor: np.ndarray,
                 norm: bool = False,
                 norm_eps: float = 1e-8,
-                batch: bool = False) -> float:
+                batch: bool = False,
+                axis: int = None) -> float:
     """
     Compute the mean square error between two numpy arrays.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The MSE distance between the two tensors.
     """
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = ((float_flat - fxp_flat) ** 2).mean(axis=-1)
     if norm:
         error /= ((float_flat ** 2).mean(axis=-1) + norm_eps)
 
     return error
 
 
 def compute_mae(float_tensor: np.ndarray,
                 fxp_tensor: np.ndarray,
                 norm: bool = False,
                 norm_eps: float = 1e-8,
-                batch: bool = False) -> float:
+                batch: bool = False,
+                axis: int = None) -> float:
     """
     Compute the mean average error function between two numpy arrays.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The mean average distance between the two tensors.
     """
 
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = np.abs(float_flat - fxp_flat).mean(axis=-1)
     if norm:
         error /= (np.abs(float_flat).mean(axis=-1) + norm_eps)
     return error
 
 
-def compute_cs(float_tensor: np.ndarray, fxp_tensor: np.ndarray, eps: float = 1e-8, batch: bool = False) -> float:
+def compute_cs(float_tensor: np.ndarray, fxp_tensor: np.ndarray, eps: float = 1e-8, batch: bool = False,
+               axis: int = None) -> float:
     """
     Compute the similarity between two tensor using cosine similarity.
     The returned values is between 0 to 1: the smaller returned value,
     the greater similarity there is between the two tensors.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         eps: Small value to avoid zero division.
         batch: Whether to run batch similarity analysis or not.
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The cosine similarity between two tensors.
     """
 
     validate_before_compute_similarity(float_tensor, fxp_tensor)
     if np.all(fxp_tensor == 0) and np.all(float_tensor == 0):
         return 1.0
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     float_norm = _similarity_tensor_norm(float_flat)
     fxp_norm = _similarity_tensor_norm(fxp_flat)
 
     # -1 <= cs <= 1
     axis = None if not batch else 1
     cs = np.sum(float_flat * fxp_flat, axis=axis) / ((float_norm * fxp_norm) + eps)
@@ -170,58 +183,65 @@
 
 
 def compute_lp_norm(float_tensor: np.ndarray,
                     fxp_tensor: np.ndarray,
                     p: int,
                     norm: bool = False,
                     norm_eps: float = 1e-8,
-                    batch: bool = False) -> float:
+                    batch: bool = False,
+                    axis: int = None) -> float:
     """
     Compute the error function between two numpy arrays.
     The error is computed based on Lp-norm distance of the tensors.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         p: p-norm to use for the Lp-norm distance.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The Lp-norm distance between the two tensors.
     """
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = (np.abs(float_flat - fxp_flat) ** p).mean(axis=-1)
     if norm:
         error /= ((np.abs(float_flat) ** p).mean(axis=-1) + norm_eps)
     return error
 
 
-def compute_kl_divergence(float_tensor: np.ndarray, fxp_tensor: np.ndarray, batch: bool = False) -> float:
+def compute_kl_divergence(float_tensor: np.ndarray, fxp_tensor: np.ndarray, batch: bool = False,
+                          axis: int = None) -> float:
     """
     Compute the similarity between two tensor using KL-divergence.
     The returned values is between 0 to 1: the smaller returned value,
     the greater similarity there is between the two tensors.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         batch: Whether to run batch similarity analysis or not.
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The KL-divergence between two tensors.
     """
 
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     non_zero_fxp_tensor = fxp_flat.copy()
     non_zero_fxp_tensor[non_zero_fxp_tensor == 0] = EPS
 
-    return np.sum(np.where(float_flat != 0, float_flat * np.log(float_flat / non_zero_fxp_tensor), 0), axis=-1)
+    prob_distance = np.where(float_flat != 0, float_flat * np.log(float_flat / non_zero_fxp_tensor), 0)
+    # The sum is part of the KL-Divergance function.
+    # The mean is to aggregate the distance between each output probability vectors.
+    return np.mean(np.sum(prob_distance, axis=-1), axis=-1)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,17 +10,20 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform import AttributeQuantizationConfig
 
 
 def apply_bias_correction_to_graph(graph_to_apply_bias_correction: Graph,
                                    core_config: CoreConfig,
                                    fw_impl: FrameworkImplementation) -> Graph:
     """
     Get a graph, where each node has a final weights quantization configuration (with a bias
@@ -33,37 +36,49 @@
 
     Returns:
         Graph with bias correction apply to it's nodes.
     """
 
     graph = copy.deepcopy(graph_to_apply_bias_correction)
     for n in graph.nodes:
-        if n.is_weights_quantization_enabled() and core_config.quantization_config.weights_bias_correction \
-                and not n.final_weights_quantization_cfg.weights_second_moment_correction:
+        # bias correction is only relevant for nodes with kernel op
+        kernel_attr = graph.fw_info.get_kernel_op_attributes(n.type)[0]
+        if core_config.quantization_config.weights_bias_correction and kernel_attr is not None and \
+            n.is_weights_quantization_enabled(kernel_attr) and \
+                not n.final_weights_quantization_cfg.weights_second_moment_correction:
             # If a kernel was quantized and weights bias correction is enabled in n.quantization_cfg,
             # a bias correction term was calculated during model preparation, and is used now in the node's bias term.
             if n.final_weights_quantization_cfg.weights_bias_correction:
-                _apply_bias_correction_to_node(n, fw_impl)
+                _apply_bias_correction_to_node(n, fw_impl, core_config.quantization_config)
     return graph
 
 
-def _apply_bias_correction_to_node(node:BaseNode,
-                                   fw_impl: FrameworkImplementation):
+def _apply_bias_correction_to_node(node: BaseNode,
+                                   fw_impl: FrameworkImplementation,
+                                   qc: QuantizationConfig):
     """
     Set new bias to node using the bias-correction term that is stored in the
     final weights quantization configuration.
 
     Args:
         node: Node to set its corrected bias after bias-correction.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        qc: QuantizationConfig containing parameters of how the model should be quantized.
 
     """
     correction = node.final_weights_quantization_cfg.bias_corrected
 
     bias = node.get_weights_by_keys(fw_impl.constants.BIAS)  # get original bias from node's weights
 
     if bias is not None:  # If the layer has bias, we subtract the correction from original bias
         node.set_weights_by_keys(fw_impl.constants.BIAS, node.get_weights_by_keys(fw_impl.constants.BIAS) - correction)
 
-    else:  # It the layer has no bias, we consider it as if it has and its value is 0.
+    else:
+        # If the layer has no bias, we consider it as if it has and its value is 0 and add a "dummy" attribute
+        # configuration with disabled quantization.
         node.set_weights_by_keys(fw_impl.constants.BIAS, - correction)
         node.framework_attr[fw_impl.constants.USE_BIAS] = True  # Mark the use_bias attribute of the node.
+        node.final_weights_quantization_cfg.set_attr_config(fw_impl.constants.BIAS,
+                                                            WeightsAttrQuantizationConfig(
+                                                                qc,
+                                                                AttributeQuantizationConfig(
+                                                                    enable_weights_quantization=False)))
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.model_collector import ModelCollector
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.quantization.quantization_analyzer import analyzer_graph
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantize_graph_weights import quantize_graph_weights
 from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
 
 
 def _collect_and_assign_act_threshold(graph: Graph,
@@ -42,22 +41,18 @@
         representative_data_gen (Callable): Dataset used for calibration.
         core_config (CoreConfig): Configuration object containing parameters of how the model should be
          quantized, including mixed precision parameters.
         fw_info: FrameworkInfo object with information about the specific framework's model.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
      """
 
-    analyzer_graph(fw_impl.attach_sc_to_node,
-                   graph,
-                   fw_info,
-                   core_config.quantization_config)  # Mark points for statistics collection
-
     mi = ModelCollector(graph,
                         fw_impl,
-                        fw_info)
+                        fw_info,
+                        core_config.quantization_config) # Mark points for statistics collection
 
     for _data in tqdm(representative_data_gen()):
         mi.infer(_data)
 
     for n in list(graph.nodes):
         if n.is_activation_quantization_enabled():
             activation_params = get_activations_qparams(
@@ -77,17 +72,15 @@
         graph: Graph to build the from.
         fw_info: FrameworkInfo object with information about the specific framework's model.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     Returns:
         Quantized model for second moment correction.
     """
-    quantized_tg = quantize_graph_weights(graph,
-                                          fw_info=fw_info,
-                                          fw_impl=fw_impl)
+    quantized_tg = quantize_graph_weights(graph)
 
     quantized_model, user_info = fw_impl.model_builder(quantized_tg,
                                                        mode=ModelBuilderMode.FLOAT,
                                                        fw_info=fw_info)
     return quantized_model
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,104 +9,107 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import copy
 from typing import Any
 
 import numpy as np
 
-from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode, Graph
-from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
+from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_weights_attr_by_qc
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.logger import Logger
 
 
 def compute_bias_correction_of_graph(graph: Graph,
-                                     core_config: CoreConfig,
                                      fw_info: FrameworkInfo,
                                      fw_impl: FrameworkImplementation) -> Graph:
     """
     For each node in a graph, and for each candidate weights quantization configuration,
     compute the bias-correction term, and store it in the candidate weights quantization configuration.
 
     Args:
         graph: Graph with nodes to compute the bias correction for
         each node's weights quantization configuration candidates.
-        core_config: CoreConfig containing parameters of how the model should be quantized.
         fw_info: Framework info like lists of nodes their kernel should quantized.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     Returns:
         Graph with bias correction for each weights quantization configuration candidate
         for each node.
     """
 
     for n in graph.nodes:
-        if n.is_weights_quantization_enabled() and core_config.quantization_config.weights_bias_correction:
-            _compute_bias_correction_per_candidate_qc(n,
-                                                      fw_info,
-                                                      graph.get_in_stats_collector(n),
-                                                      fw_impl=fw_impl)
+        # Bias correction is computed based on the quantized kernel, so we need to get the specific kernel attribute
+        # name out of all the weights attributes of the node.
+        if fw_info.is_kernel_op(n.type):
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if n.is_weights_quantization_enabled(kernel_attr):
+                _compute_bias_correction_per_candidate_qc(n,
+                                                          kernel_attr,
+                                                          fw_info,
+                                                          graph.get_in_stats_collector(n),
+                                                          fw_impl=fw_impl)
     return graph
 
 
 def _compute_bias_correction_per_candidate_qc(node: BaseNode,
+                                              kernel_attr: str,
                                               fw_info: FrameworkInfo,
                                               node_in_stats_collector: BaseStatsCollector,
                                               fw_impl: FrameworkImplementation):
     """
     For each candidate weights quantization configuration of a given node,
     compute the bias-correction term, and store it in the candidate weights quantization configuration.
 
     Args:
         node: Node to compute the bias correction for its different candidates.
+        kernel_attr: The name of the kernel attribute of the node.
         fw_info: Framework info like lists of nodes their kernel should quantized.
         node_in_stats_collector: Statistics collector of the node for the mean per-channel.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     """
 
     for candidate_qc in node.candidates_quantization_cfg:
-        if candidate_qc.weights_quantization_cfg.enable_weights_quantization and not \
+        if candidate_qc.weights_quantization_cfg.weights_bias_correction and not \
                 candidate_qc.weights_quantization_cfg.weights_second_moment_correction:
-            quantized_kernel, io_channels_axes = get_quantized_kernel_by_weights_qc(fw_info,
-                                                                                    node,
-                                                                                    candidate_qc.weights_quantization_cfg,
-                                                                                    fw_impl=fw_impl)
-
-            # If a kernel was quantized and weights bias correction is enabled in n.quantization_cfg,
-            # a bias correction term is being calculated and used in the node's bias term.
-            if candidate_qc.weights_quantization_cfg.weights_bias_correction:
-                bias_correction_term = _get_bias_correction_term_of_node(io_channels_axes[0],
-                                                                         node,
-                                                                         node_in_stats_collector,
-                                                                         io_channels_axes[1],
-                                                                         quantized_kernel,
-                                                                         fw_impl=fw_impl)
 
-                # Store the correction term to use it later,
-                candidate_qc.weights_quantization_cfg.bias_corrected = bias_correction_term
+            quantized_kernel, io_channels_axes = get_quantized_weights_attr_by_qc(kernel_attr,
+                                                                                  node,
+                                                                                  candidate_qc.weights_quantization_cfg
+                                                                                  .get_attr_config(kernel_attr))
+
+            bias_correction_term = _get_bias_correction_term_of_node(io_channels_axes[0],
+                                                                     node,
+                                                                     node_in_stats_collector,
+                                                                     io_channels_axes[1],
+                                                                     quantized_kernel,
+                                                                     fw_impl=fw_impl)
+
+            # Store the correction term to use it later,
+            candidate_qc.weights_quantization_cfg.bias_corrected = bias_correction_term
+
 
 def is_non_positive_integer(x: float) -> bool:
     """
     Check if a variable is positive integer or not
     Args:
         x: input float to check
     Returns:
         True if x is non-positive integer
     """
     return x < 1 or int(x) != x
 
+
 def _compute_bias_correction(kernel: np.ndarray,
                              quantized_kernel: np.ndarray,
                              in_statistics_container: BaseStatsCollector,
                              output_channels_axis: int,
                              input_channels_axis: int) -> Any:
     """
     Compute the bias correction term for the bias in the error on the layers output,
@@ -183,21 +186,19 @@
 
 
     Returns:
         Bias-correction term to subtract from the current node's bias.
     """
 
     if output_channels_axis is None:
-        Logger.error(
-            f'Unknown output channel axis for node named: {n.name},'
-            f' please update channel mapping function')
+        Logger.critical(
+            f'Unknown output channel axis for node: {n.name}. Please update the channel mapping function.')
     if input_channels_axis is None:
-        Logger.error(
-            f'Unknown input channel axis for node named: {n.name},'
-            f' please update channel mapping function')
+        Logger.critical(
+            f'Unknown input channel axis for node: {n.name}. Please update the channel mapping function')
     # Compute the bias correction term.
     correction = _compute_bias_correction(n.get_weights_by_keys(fw_impl.constants.KERNEL),
                                           quantized_kernel,
                                           node_in_stats_collector,
                                           output_channels_axis,
                                           input_channels_axis)
     return correction
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,15 +53,14 @@
     tg_with_bias = substitute(transformed_graph, fw_impl.get_substitutions_statistics_correction(
         core_config.quantization_config))
 
     ########################################################
     # Compute bias correction to nodes' config candidates
     ########################################################
     tg_with_bias = compute_bias_correction_of_graph(tg_with_bias,
-                                                    core_config,
                                                     fw_info,
                                                     fw_impl)
 
     if tb_w is not None:
         tb_w.add_graph(tg_with_bias, 'statistics_computation')
 
     return tg_with_bias
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/apply_substitutions.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py`

 * *Files 24% similar despite different names*

```diff
@@ -15,20 +15,23 @@
 
 
 import copy
 from typing import Callable
 
 import numpy as np
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod, \
+    AttributeQuantizationConfig
 
 
 class BatchNormalizationReconstruction(common.BaseSubstitution):
     """
     Reconstruct BatchNormalization after linear layers.
     """
     def __init__(self,
@@ -75,28 +78,33 @@
         """
 
         num_nodes_before_substitution = len(graph.nodes)
         num_edges_before_substitution = len(graph.edges)
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if source_node.reuse or source_node.reuse_group is not None:
+        if source_node.is_reused():
             for qc in source_node.candidates_quantization_cfg:
                 qc.weights_quantization_cfg.weights_second_moment_correction = False
             return graph
 
         # We apply only on nodes with folded BatchNormalization.
         if source_node.prior_info.std_output is None or source_node.prior_info.mean_output is None:
             for qc in source_node.candidates_quantization_cfg:
                 qc.weights_quantization_cfg.weights_second_moment_correction = False
             return graph
 
         # This feature disabled for models with weights quantization method of Power of 2
         for qc in source_node.candidates_quantization_cfg:
-            if qc.weights_quantization_cfg.weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
+            # this feature is relevant only for layers with kernel op
+            kernel_attr = graph.fw_info.get_kernel_op_attributes(source_node.type)
+            if kernel_attr is None:
+                Logger.error(f"Can't preform BatchNorm reconstruction on a node {source_node.name} without a kernel op.")
+            if (qc.weights_quantization_cfg.get_attr_config(kernel_attr[0]).weights_quantization_method
+                    == QuantizationMethod.POWER_OF_TWO):
                 Logger.warning("Second moment statistics correction feature disabled for models with weights "
                                "quantization method of Power of 2")
                 for qc_inner in source_node.candidates_quantization_cfg:
                     qc_inner.weights_quantization_cfg.weights_second_moment_correction = False
                 return graph
 
         eps = self.epsilon_val
@@ -115,16 +123,29 @@
         bn_node = self.create_bn_node(source_node, bn_node_weights)
 
         bn_node.prior_info = copy.deepcopy(source_node.prior_info)
 
         bn_node.candidates_quantization_cfg = copy.deepcopy(source_node.candidates_quantization_cfg)
 
         for qc in bn_node.candidates_quantization_cfg:
-            qc.weights_quantization_cfg.enable_weights_quantization = False
             qc.activation_quantization_cfg.enable_activation_quantization = False
+            for attr in bn_node.get_node_weights_attributes():
+                if qc.weights_quantization_cfg.has_attribute_config(attr):
+                    # we only create a BN layer to collect statistics, so we don't need to quantize anything,
+                    # but we do need to add the BN attributes to the reconstructed node.
+                    qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
+                else:
+                    # setting a "dummy" attribute configuration with disabled quantization.
+                    # TODO: once enabling BN attributes quantization, need to figure out if thie
+                    #  reconstructed node BN attributes need to be quantized and how.
+                    qc.weights_quantization_cfg.set_attr_config(attr,
+                                                                WeightsAttrQuantizationConfig(
+                                                                    QuantizationConfig(),
+                                                                    AttributeQuantizationConfig(
+                                                                        enable_weights_quantization=False)))
 
         graph.reconnect_out_edges(current_node=source_node, new_node=bn_node)
         graph.replace_output_node(current_node=source_node, new_node=bn_node)
         graph.add_node_with_in_edges(bn_node, [source_node])
 
         assert len(graph.nodes) - num_nodes_before_substitution == 1
         assert len(graph.edges) - num_edges_before_substitution == 1
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py`

 * *Files 8% similar despite different names*

```diff
@@ -98,24 +98,22 @@
 
         # We apply only on nodes with reconstructed BatchNormalization.
         if not source_node.final_weights_quantization_cfg.weights_second_moment_correction:
             return graph
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if source_node.reuse or source_node.reuse_group is not None:
-            Logger.exception("If the linear operator is part of a reused group we should skip the the BN folding "
-                             "substitution and SMC feature")  # pragma: no cover
+        if source_node.is_reused():
+            Logger.critical("BN folding substitution cannot proceed if the linear operator is part of a reused group.")  # pragma: no cover
 
         bn_node = edge_nodes[1]
 
         if len(graph.get_next_nodes(source_node)) > 1 or len(graph.get_prev_nodes(bn_node)) > 1:
-            Logger.exception(
-                "If the linear operator has multiple outputs or the bn layer has multiple inputs we should "
-                "skip the the BN folding substitution and SMC feature")  # pragma: no cover
+            Logger.critical(
+                "BN folding substitution cannot proceed if the linear operator has multiple outputs or the BN layer has multiple inputs.")  # pragma: no cover
 
         kernel = source_node.get_weights_by_keys(self.kernel_str)
         bias = source_node.get_weights_by_keys(self.bias_str)
         gamma = bn_node.get_weights_by_keys(self.gamma_str)
         beta = bn_node.get_weights_by_keys(self.beta_str)
         moving_mean = bn_node.get_weights_by_keys(self.moving_mean_str)
         moving_variance = bn_node.get_weights_by_keys(self.moving_variance_str)
@@ -155,44 +153,48 @@
         graph.set_out_stats_collector_to_node(conv_bn, out_stats)
         graph.node_to_in_stats_collector.update({conv_bn: in_stats})
 
         graph.remove_edge(source_node, bn_node)
         graph.remove_node(bn_node)
         graph.remove_node(source_node)
 
-        self._calc_weights_quantization_params(conv_bn, weights_scale)
+        self._calc_weights_quantization_params(conv_bn, weights_scale, graph.fw_info)
 
         assert num_nodes_before_substitution - len(graph.nodes) == 1
         assert num_edges_before_substitution - len(graph.edges) == 1
         return graph
 
     def _calc_weights_quantization_params(self,
                                           conv_bn: BaseNode,
-                                          weights_scale: np.ndarray):
+                                          weights_scale: np.ndarray,
+                                          fw_info):
         """
         Update node weights quantization params.
         Args:
             conv_bn: Convolution node to update the weights quantization params.
             weights_scale: Weight scale factor in which to multiply the conv node's weight.
+            fw_info: FrameworkInfo object with information about the specific framework's model
         """
+        # Conv layer is ensured to have a kernel attribute
+        kernel_attr = fw_info.get_kernel_op_attributes(conv_bn.type)[0]
+        conv_bn_kernel_cfg = conv_bn.final_weights_quantization_cfg.get_attr_config(kernel_attr)
         # In case of SYMMETRIC weight quantization method, we update the threshold by weights_scale
-        if conv_bn.final_weights_quantization_cfg.weights_quantization_method == QuantizationMethod.SYMMETRIC:
-            original_threshold = conv_bn.final_weights_quantization_cfg.weights_quantization_params[THRESHOLD]
-            corr_dict = copy.deepcopy(conv_bn.final_weights_quantization_cfg.weights_quantization_params)
+        if conv_bn_kernel_cfg.weights_quantization_method == QuantizationMethod.SYMMETRIC:
+            original_threshold = conv_bn_kernel_cfg.weights_quantization_params[THRESHOLD]
+            corr_dict = copy.deepcopy(conv_bn_kernel_cfg.weights_quantization_params)
             corr_threshold, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_threshold, weights_scale)
             corr_dict[THRESHOLD] = corr_threshold
-            conv_bn.final_weights_quantization_cfg.set_weights_quantization_param(corr_dict)
+            conv_bn_kernel_cfg.set_weights_quantization_param(corr_dict)
 
         # In case of UNIFORM weight quantization method, we update the range_min, range_max by weights_scale
-        elif conv_bn.final_weights_quantization_cfg.weights_quantization_method == QuantizationMethod.UNIFORM:
-            corr_dict = copy.deepcopy(conv_bn.final_weights_quantization_cfg.weights_quantization_params)
-            original_range_min = conv_bn.final_weights_quantization_cfg.weights_quantization_params[RANGE_MIN]
+        elif conv_bn_kernel_cfg.weights_quantization_method == QuantizationMethod.UNIFORM:
+            corr_dict = copy.deepcopy(conv_bn_kernel_cfg.weights_quantization_params)
+            original_range_min = conv_bn_kernel_cfg.weights_quantization_params[RANGE_MIN]
             corr_range_min, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_range_min, weights_scale)
-            original_range_max = conv_bn.final_weights_quantization_cfg.weights_quantization_params[RANGE_MAX]
+            original_range_max = conv_bn_kernel_cfg.weights_quantization_params[RANGE_MAX]
             corr_range_max, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_range_max, weights_scale)
             corr_dict[RANGE_MIN] = corr_range_min
             corr_dict[RANGE_MAX] = corr_range_max
-            conv_bn.final_weights_quantization_cfg.set_weights_quantization_param(corr_dict)
+            conv_bn_kernel_cfg.set_weights_quantization_param(corr_dict)
 
         else:
-            Logger.exception("Second moment statistics correction feature disabled for models with weights "
-                             "quantization method of Power of 2")  # pragma: no cover
+            Logger.critical("Second moment statistics correction feature is not supported for weights quantization methods other than 'SYMMETRIC' and 'UNIFORM'.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/linear_collapsing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py`

 * *Files 14% similar despite different names*

```diff
@@ -87,22 +87,19 @@
         Args:
             graph: Graph we apply the substitution on.
             edge_nodes: Tuple of two linear nodes
         Returns:
             Graph after applying the substitution.
         """
 
-        first_node = edge_nodes[0]
-        second_node = edge_nodes[1]
+        first_node, second_node, _ = edge_nodes
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if first_node.reuse or first_node.reuse_group is not None:
-            return graph
-        if second_node.reuse or second_node.reuse_group is not None:
+        if first_node.is_reused() or second_node.is_reused():
             return graph
 
         # If there is an extra connection between these two nodes skip the substitution
         if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) > 1:
             return graph
 
         # Skip if convolution's data format is 'channels_first'
@@ -178,7 +175,87 @@
         graph.remove_node(second_node)
 
         # Sanity check
         assert num_nodes_before_substition - len(graph.nodes) == 1
         assert num_edges_before_substition - len(graph.edges) == 1
 
         return graph
+
+
+class Op2DAddConstCollapsing(common.BaseSubstitution):
+    """
+    Collapse Add-const into preceding Op2D (Not non-linear activation between them)
+    """
+    def __init__(self,
+                 first_node: NodeOperationMatcher,
+                 second_node: NodeOperationMatcher,
+                 op2d_collapsing_fn: Callable,
+                 bias_str: str,
+                 use_bias_str: str,
+                 layer_name_str: str = None):
+        """
+        Collapsing Add-const node (2nd node) to Op2D node (first node).
+        Args:
+            first_node: Node matcher for Op2d type nodes.
+            second_node: Node matcher for add type nodes.
+            op2d_collapsing_fn: Function for updating the convolution kernel and bias
+            bias_str: The framework specific attribute name of the convolution layer's bias.
+            use_bias_str: The framework specific attribute name of the convolution layer's bias flag.
+            layer_name_str: The framework specific attribute name of layer's name.
+        """
+        super().__init__(matcher_instance=EdgeMatcher(first_node, second_node))
+        self.op2d_collapsing_fn = op2d_collapsing_fn
+        self.bias_str = bias_str
+        self.use_bias_str = use_bias_str
+        self.layer_name_str = layer_name_str
+
+    def substitute(self,
+                   graph: Graph,
+                   edge_nodes: Tuple[BaseNode, BaseNode]) -> Graph:
+        """
+        Collapse linear layer into preceding linear layers.
+        Convolution condition:
+        |-------------------------|      |------|
+        | Op2D | ---> | Add-const |  ->  | Op2D |
+        |-------------------------|      |------|
+        Args:
+            graph: Graph we apply the substitution on.
+            edge_nodes: Tuple of linear node and add nodes
+        Returns:
+            Graph after applying the substitution.
+        """
+
+        first_node, second_node, _ = edge_nodes
+
+        # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
+        # we should skip the substitution.
+        if first_node.is_reused() or second_node.is_reused():
+            return graph
+
+        # If there is an extra connection between these two nodes skip the substitution
+        if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) > 1:
+            return graph
+
+        # New collapsed bias
+        bias = self.op2d_collapsing_fn(first_node, second_node, self.bias_str)
+
+        # New collapsed node
+        op2d_collapsed = copy.deepcopy(first_node)
+        op2d_collapsed_name = first_node.name + '_collapsed'
+        op2d_collapsed.name = op2d_collapsed_name
+        op2d_collapsed.framework_attr[self.use_bias_str] = True
+        op2d_collapsed.set_weights_by_keys(self.bias_str, bias)
+
+        if self.layer_name_str is not None:
+            op2d_collapsed.framework_attr[self.layer_name_str] = op2d_collapsed_name
+
+        # Update graph
+        graph.add_node(op2d_collapsed)
+        graph.reconnect_out_edges(current_node=second_node, new_node=op2d_collapsed)
+        graph.reconnect_in_edges(current_node=first_node, new_node=op2d_collapsed)
+        graph.replace_output_node(current_node=second_node, new_node=op2d_collapsed)
+
+        graph.remove_edge(first_node, second_node)
+        graph.remove_node(first_node)
+        graph.remove_node(second_node)
+
+        return graph
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py`

 * *Files 7% similar despite different names*

```diff
@@ -26,14 +26,17 @@
     the matches are not valid anymore, and we can find new matches.
     Args:
         graph: Graph to transform.
         linear_collapsing_substitution: substitution to apply on the graph.
     Returns:
         Transformed graph after applying all linear collapsing substitutions.
     """
+    # TODO: remove this if after adding Op2d-add_const collapse substitution in PyTorch
+    if linear_collapsing_substitution is None:
+        return graph
     matched_nodes = graph.filter(linear_collapsing_substitution.matcher_instance)
     matched_nodes_list = []
     match_indicator = True
     while len(matched_nodes) > 0 and match_indicator:
         match_indicator = False
         for matched_node in matched_nodes:
             if matched_node not in matched_nodes_list:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/residual_collapsing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,17 +59,15 @@
         """
 
         first_node = edge_nodes[0]
         second_node = edge_nodes[1]
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if first_node.reuse or first_node.reuse_group is not None:
-            return graph
-        if second_node.reuse or second_node.reuse_group is not None:
+        if first_node.is_reused() or second_node.is_reused():
             return graph
 
         # Check if convolution and residual satisfy the collapsing conditions, otherwise skip substitution
         if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) != 2:
             return graph
 
         # Check if Add is residual connection, otherwise skip substitution
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/scale_equalization.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 from typing import List
 
 import numpy as np
 import scipy
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 
 
 # We assume to have Gaussian distribution before the RelU operation
 # Hence, the activations after the RelU operation have Rectified Gaussian distribution
 # We need to calculate the "fixed" mean and std of the "new" activations
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,19 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 import numpy as np
 from typing import List, Tuple, Any, Callable
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import FrameworkInfo, Graph, BaseNode
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, SHIFT_NEGATIVE_NON_LINEAR_NUM_BITS
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod, \
+    AttributeQuantizationConfig
 from model_compression_toolkit.core.common.quantization.set_node_quantization_config import create_node_activation_qc, \
     set_quantization_configs_to_node
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     _mse_error_histogram
@@ -59,14 +62,20 @@
         bias_flag_str: The framework specific attribute name of the bias flag.
     """
 
     bias = op2d_node.get_weights_by_keys(bias_str)
     if bias is None:
         bias = 0.0
         op2d_node.framework_attr[bias_flag_str] = True
+        # Add an attribute quantization configuration to the newly added bias attribute, with disabled quantization
+        for qc in op2d_node.candidates_quantization_cfg:
+            qc.weights_quantization_cfg.set_attr_config(bias_flag_str,
+                                                        WeightsAttrQuantizationConfig(QuantizationConfig(),
+                                                                                      AttributeQuantizationConfig(
+                                                                                          enable_weights_quantization=False)))
 
     # Each node adds a different noise due to the shifting. It depends on the
     # dimensions of the kernel, thus the correction term is a function of
     # the layer type.
     kernel = op2d_node.get_weights_by_keys(fw_info.kernel_ops_attributes_mapping.get(op2d_node.type)[0])
     if kernel is not None:
         output_channel_index, input_channel_index = fw_info.kernel_channels_mapping.get(op2d_node.type)
@@ -121,15 +130,15 @@
         node_to_insert: Node to add.
         first_node: Node to insert the new node after it.
 
     """
 
     last_nodes = graph.get_next_nodes(first_node)
     if len(last_nodes) != 1:
-        Logger.error('Can only insert if there is only one input')  # pragma: no cover
+        Logger.critical(f'Insertion requires exactly one successor node; {len(last_nodes)} successors found.')  # pragma: no cover
     last_node = last_nodes[0]
     insert_node_between_two_nodes(graph, node_to_insert, first_node, last_node)
 
 
 def insert_node_before_node(graph: Graph,
                             node_to_insert: BaseNode,
                             last_node: BaseNode):
@@ -143,15 +152,15 @@
         graph: Graph to add the new node to.
         node_to_insert: Node to add.
         last_node: Node to insert the new node after it.
 
     """
     first_nodes = graph.get_prev_nodes(last_node)
     if len(first_nodes) != 1:
-        Logger.error('Can only insert if there is only one input')  # pragma: no cover
+        Logger.critical('Insertion requires exactly one predecessor node; multiple or no predecessors found.')  # pragma: no cover
     first_node = first_nodes[0]
     insert_node_between_two_nodes(graph, node_to_insert, first_node, last_node)
 
 
 def remove_node_between_two_nodes(graph: Graph,
                                   node_to_remove: BaseNode,
                                   first_node: BaseNode,
@@ -187,14 +196,15 @@
                             get_padding_values: Callable,
                             create_pad_node: Callable,
                             padding_str: str,
                             bias_str: str,
                             bias_flag_str: str,
                             zero_padding_node: BaseNode = None,
                             bypass_nodes: List = None,
+                            params_search_quantization_fn: Callable = None
                             ) -> Graph:
     """
     Shift the output of a non-linear activation by its minimal output value (quantized) such
     that all values after the shifting are positive.
     The shifting happens only if the ratio between the shifting value and the threshold is small enough
     (the threshold to activate the shifting and correction is in the passed QuantizationConfig, qc).
     To correct the impact of such shifting, a correction to the next linear node is computed and
@@ -212,23 +222,24 @@
         create_add_node: Function to create an add node.
         get_padding_values: Function to compute the op2d node's padding values
         create_pad_node: Function to create an pad node.
         padding_str: The framework specific attribute name of the padding.
         bias_str: The framework specific attribute name of the bias.
         bias_flag_str: The framework specific attribute name of the bias flag.
         zero_padding_node: ZeroPadding2D node that may be in the graph before the linear layer.
+        params_search_quantization_fn: Function to quantize np tensor using a framework (tf/torch) quantization method. Needed for better param_search estimating the expected loss.
 
     Returns:
         Graph after applying the shifting and correction.
     """
 
     min_to_correct, max_value2compare = graph.get_out_stats_collector(non_linear_node).get_min_max_values()
 
     if not non_linear_node.is_all_activation_candidates_equal():
-        Logger.error("Shift negative correction is not supported for more than one activation quantization "
+        Logger.critical("Shift negative correction is not supported for more than one activation quantization "
                      "configuration candidate")  # pragma: no cover
 
     # all candidates have same activation config, so taking the first candidate for calculations
     non_linear_node_cfg_candidate = non_linear_node.candidates_quantization_cfg[0].activation_quantization_cfg
 
     # get the non-linear activation threshold
     activation_threshold = non_linear_node_cfg_candidate.activation_quantization_params.get(THRESHOLD)
@@ -247,29 +258,45 @@
         'float32')  # Change to type float32 to support tensorflow dtypes
 
     delta = q_points + min_to_correct
     delta[delta < 0] = np.inf
     shift_value = q_points[np.argmin(delta)]
 
     if core_config.quantization_config.shift_negative_params_search:
+
         hist_bins, hist_count = graph.get_out_stats_collector(non_linear_node).hc.get_histogram()
         hist_count = z_score_filter(non_linear_node_cfg_candidate.z_threshold,
                                     hist_bins, hist_count)
 
         min_mse, _th, _shift = np.inf, None, None
         for _activation_threshold in [activation_threshold, 2 * activation_threshold]:
             qparams = {THRESHOLD: _activation_threshold, SIGNED: False}
             _lsb = _activation_threshold / num_q_points
             _q_points = np.linspace(0, _activation_threshold - _lsb, num_q_points).astype(
                 'float32')  # Change to type float32 to support tensorflow dtypes
             for _shift_value in _q_points:
                 _hist_bins = hist_bins.astype(np.float32) + _shift_value
-                q_bins = non_linear_node_cfg_candidate.activation_quantization_fn(
-                    non_linear_node_cfg_candidate.activation_n_bits,
-                    qparams)(_hist_bins)
+                fw_quant_fn = non_linear_node_cfg_candidate.activation_quantization_fn(non_linear_node_cfg_candidate.activation_n_bits,qparams)
+                """
+                In SNC, when better shifting values are tested for better choice,
+                the histogram (which is a numpy object) is quantized using the non-linear node activation
+                quantization function (to estimate the expected mse comparing to the original histogram).
+                The quantization function is a framework function, which makes it fail since it
+                expects a fw tensor. The commmon part of SNC receives an argument which is a callable 
+                that receives two argument and returns one: it gets the fw activation quantization function
+                and the bins to quantize. The function (of each fw) responsible for doing (if needed) a preprocessing and postprocessing
+                to the bins which is a numpy object.
+                Only if this function is passed - common SNC will use this function. If not - it assumes
+                no processing is needed and simply uses the activation quantization quantizer to quantize the bins (like tf for example).
+                """
+                if params_search_quantization_fn is None:
+                    q_bins = fw_quant_fn(_hist_bins)
+                else:
+                    q_bins = params_search_quantization_fn(fw_quant_fn, _hist_bins)
+
                 mse = _mse_error_histogram(q_bins, None, _hist_bins, hist_count)
                 if mse < min_mse:
                     min_mse = mse
                     _th, _shift = _activation_threshold, _shift_value
 
         shift_value = _shift
         activation_threshold = _th
@@ -289,14 +316,20 @@
         if padding_values is not None:
             pad_top, pad_btm, pad_left, pad_right = padding_values
 
     # Insert Add node between non linear node to op2d, and fix op2d bias
     add_node = create_add_node(shift_value,
                                non_linear_node.name,
                                non_linear_node.input_shape)
+
+    # Create prior info for the add node
+    if non_linear_node.prior_info is not None:
+        add_prior_info = non_linear_node.prior_info.get_shifted_prior_info(shift_value)
+        add_node.prior_info = add_prior_info
+
     insert_node_after_node(graph,
                            node_to_insert=add_node,
                            first_node=non_linear_node)
     op2d_bias_correction(op2d_node,
                          shift_value,
                          fw_info,
                          bias_str,
@@ -320,16 +353,17 @@
         set_quantization_configs_to_node(fw_info=fw_info,
                                          node=pad_node,
                                          quant_config=core_config.quantization_config,
                                          tpc=graph.tpc,
                                          mixed_precision_enable=core_config.mixed_precision_enable)
 
         for candidate_qc in pad_node.candidates_quantization_cfg:
-            candidate_qc.weights_quantization_cfg.enable_weights_quantization = False
             candidate_qc.activation_quantization_cfg.enable_activation_quantization = False
+            for attr in pad_node.get_node_weights_attributes():
+                candidate_qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
 
         # Insert a pad node between the add node to the op2d, and create statistics for the pad node
         insert_node_before_node(graph,
                                 node_to_insert=pad_node,
                                 last_node=op2d_node)
 
         graph.set_out_stats_collector_to_node(pad_node,
@@ -354,15 +388,16 @@
             for bypass_candidate_qc in bypass_node.candidates_quantization_cfg:
                 if bypass_candidate_qc.activation_quantization_cfg:
                     bypass_candidate_qc.activation_quantization_cfg.activation_quantization_params[SIGNED] = False
                     graph.shift_stats_collector(bypass_node, np.array(shift_value))
 
     add_node_qco = add_node.get_qco(graph.tpc).quantization_config_list
     for op_qc_idx, candidate_qc in enumerate(add_node.candidates_quantization_cfg):
-        candidate_qc.weights_quantization_cfg.enable_weights_quantization = False
+        for attr in add_node.get_node_weights_attributes():
+            candidate_qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
 
         candidate_qc.activation_quantization_cfg = create_node_activation_qc(core_config.quantization_config,
                                                                              fw_info,
                                                                              add_node_qco[op_qc_idx])
 
         candidate_qc.activation_quantization_cfg.set_activation_quantization_param({THRESHOLD: activation_threshold,
                                                                                     SIGNED: False})
@@ -462,15 +497,16 @@
                                     pad_node_types: NodeOperationMatcher,
                                     create_add_node: Callable,
                                     get_padding_values: Callable,
                                     create_pad_node: Callable,
                                     is_padding_node_and_node_has_padding: Callable,
                                     padding_str: str,
                                     bias_str: str,
-                                    bias_flag_str: str) -> Graph:
+                                    bias_flag_str: str,
+                                    params_search_quantization_fn: Callable=None) -> Graph:
     """
     Apply the substitution even if the linear node is not immediately after
     the non-linear node, but there are intermediate nodes
 
     Args:
         graph: Graph to apply the substitution on.
         core_config: Quantization configuration to build the substitutions list according to.
@@ -517,9 +553,10 @@
                                                 create_add_node,
                                                 get_padding_values,
                                                 create_pad_node,
                                                 padding_str,
                                                 bias_str,
                                                 bias_flag_str,
                                                 zero_padding_node=pad_node,
-                                                bypass_nodes=bypass_nodes)
+                                                bypass_nodes=bypass_nodes,
+                                                params_search_quantization_fn=params_search_quantization_fn)
     return graph
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/softmax_shift.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,23 +44,24 @@
         if len(predecessors) != 1:
             return graph
 
         act_node = predecessors[0]
 
         if len(graph.out_edges(act_node)) > 1:
             Logger.warning(f"Node {act_node.name} has multiple outgoing edges, which is not supported with "
-                           f"mixed-precision bit-operations KPI, thus, edge {act_node.name} --> {weights_node.name} "
+                           f"mixed-precision bit-operations utilization, thus, edge {act_node.name} --> {weights_node.name} "
                            f"would not be counted in the bit-operations calculations.")
             return graph
 
         # Virtual composed activation-weights node
         # we pass a dummy initialization dict to initialize the super BaseNode class,
         # the actual arguments values are irrelevant because they are being overridden or not used
         v_node = VirtualActivationWeightsNode(act_node,
                                               weights_node,
+                                              fw_info=graph.fw_info,
                                               **weights_node.__dict__)
 
         # Update graph
         graph.add_node(v_node)
         graph.reconnect_in_edges(current_node=act_node, new_node=v_node)
         graph.reconnect_out_edges(current_node=weights_node, new_node=v_node)
         graph.replace_input_node(current_node=act_node, new_node=v_node)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/substitutions/weights_activation_split.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,29 +45,35 @@
         Args:
             graph: Graph we apply the substitution on.
             node: Node to split.
 
         Returns:
             Graph after applying the substitution.
         """
-
-        if not node.is_all_weights_candidates_equal() and not node.is_all_activation_candidates_equal():
+        # The decomposition works on linear nodes, that is, nodes with kernel ops
+        kernel_attr = graph.fw_info.get_kernel_op_attributes(node.type)[0]
+        if kernel_attr is None:
+            Logger.error(f"Trying to split node weights and activation, but node "
+                         f"{node.name} doesn't have a kernel attribute.")
+        if not node.is_all_weights_candidates_equal(kernel_attr) and not node.is_all_activation_candidates_equal():
             # Node has both different weights and different activation configuration candidates
-            weights_bits = [c.weights_quantization_cfg.weights_n_bits for c in node.get_unique_weights_candidates()]
+            weights_bits = [c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                            for c in node.get_unique_weights_candidates(kernel_attr)]
             activation_bits = [c.activation_quantization_cfg.activation_n_bits for c in node.get_unique_activation_candidates()]
             expected_candidates = list(itertools.product(weights_bits, activation_bits))
-            all_candidates_bits = [(c.weights_quantization_cfg.weights_n_bits,
-                                    c.activation_quantization_cfg.activation_n_bits) for c in node.candidates_quantization_cfg]
+            all_candidates_bits = [(c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
+                                    c.activation_quantization_cfg.activation_n_bits)
+                                   for c in node.candidates_quantization_cfg]
             if not set(expected_candidates).issubset(all_candidates_bits):
                 # Node is not composite, therefore, can't be split
-                Logger.critical(f"The graph contains a node {node.name} with non composite candidates."
-                                f"In order to run mixed-precision search with BOPS target KPI, "
-                                f"all model layers should be composite.")  # pragma: no cover
+                Logger.critical(f"The node {node.name} cannot be split as it has non-composite candidates. "
+                                f"For mixed-precision search with BOPS target resource utilization, "
+                                f"all model layers must be composite.")  # pragma: no cover
 
-        weights_node = VirtualSplitWeightsNode(node)
+        weights_node = VirtualSplitWeightsNode(node, kernel_attr)
         activation_node = VirtualSplitActivationNode(node, self.activation_layer_type, self.fw_attr)
 
         # Update graph
         graph.add_node(weights_node)
         graph.add_node(activation_node)
         graph.reconnect_in_edges(current_node=node, new_node=weights_node)
         graph.reconnect_out_edges(current_node=node, new_node=activation_node)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/user_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/user_info.py`

 * *Files 9% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     input scaling during the process).
     """
 
     def __init__(self):
         self.input_scale = 1
         self.gptq_info_dict = dict()
         self.mixed_precision_cfg = None
-        self.final_kpi = None
+        self.final_resource_utilization = None
 
     def set_input_scale(self, scale_value: float):
         """
         Set the UserInformation an input scale value.
 
         Args:
             scale_value: Scale factor to store in the UserInformation.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/final_config_visualizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/nn_visualizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -63,15 +63,15 @@
         Initialize a NNVisualizer object.
         Args:
             graph_float: Float version of the graph.
 
         """
 
         self.graph_float = graph_float
-        self.graph_quantized = quantize_graph_weights(graph_float, fw_info=fw_info, fw_impl=fw_impl)
+        self.graph_quantized = quantize_graph_weights(graph_float)
         self.fw_impl = fw_impl
         self.fw_info = fw_info
 
         # Get compare points of two graphs.
         self.compare_points, self.compare_points_name = _get_compare_points(self.graph_quantized)
         self.compare_points_float, self.compare_points_name_float = _get_compare_points(self.graph_float)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/common/visualization/tensorboard_writer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from copy import deepcopy
 
 import io
+import os
 import numpy as np
 from PIL import Image
 from matplotlib.figure import Figure
 from tensorboard.compat.proto.attr_value_pb2 import AttrValue
 from tensorboard.compat.proto.config_pb2 import RunMetadata
 from tensorboard.compat.proto.event_pb2 import Event, TaggedRunMetadata
 from tensorboard.compat.proto.graph_pb2 import GraphDef
@@ -30,14 +31,17 @@
 from tensorboard.compat.proto.tensor_shape_pb2 import TensorShapeProto
 from tensorboard.summary.writer.event_file_writer import EventFileWriter
 from typing import List, Any, Dict
 from networkx import topological_sort
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
+    WeightsFinalBitwidthConfigVisualizer, ActivationFinalBitwidthConfigVisualizer
 
 DEVICE_STEP_STATS = "/device:CPU:0"
 
 
 def get_node_properties(node_dict_to_log: dict,
                         output_shapes: List[tuple] = None) -> Dict[str, Any]:
     """
@@ -222,15 +226,15 @@
             # if they exist at all, as we can log the initial graph,
             # which its nodes do not have configurations yet.
             # Log final config or unified candidates, not both
             attr = dict()
             if n.final_weights_quantization_cfg is not None:
                 attr.update(n.final_weights_quantization_cfg.__dict__)
             elif n.candidates_quantization_cfg is not None:
-                attr.update(n.get_unified_weights_candidates_dict())
+                attr.update(n.get_unified_weights_candidates_dict(self.fw_info))
             return attr
 
         def __get_node_attr(n: BaseNode) -> Dict[str, Any]:
             """
             Create a dictionary to display as the node's attributes.
             The dictionary contains information from node's framework attributes and quantization attributes
 
@@ -482,7 +486,49 @@
 
         event = Event(summary=Summary(value=[Summary.Value(tag=figure_tag, image=img_summary)]))
 
         # Get the event writer for this tag name
         er = self.__get_event_writer_by_tag_name(main_tag_name)
         er.add_event(event)
         er.flush()
+
+
+def init_tensorboard_writer(fw_info: FrameworkInfo) -> TensorboardWriter:
+    """
+    Create a TensorBoardWriter object initialized with the logger dir path if it was set,
+    or None otherwise.
+
+    Args:
+        fw_info: FrameworkInfo object.
+
+    Returns:
+        A TensorBoardWriter object.
+    """
+    tb_w = None
+    if Logger.LOG_PATH is not None:
+        tb_log_dir = os.path.join(os.getcwd(), Logger.LOG_PATH, 'tensorboard_logs')
+        Logger.info(f'To use Tensorboard, please run: tensorboard --logdir {tb_log_dir}')
+        tb_w = TensorboardWriter(tb_log_dir, fw_info)
+    return tb_w
+
+
+def finalize_bitwidth_in_tb(tb_w: TensorboardWriter,
+                            weights_conf_nodes_bitwidth: List,
+                            activation_conf_nodes_bitwidth: List):
+    """
+    Set the final bit-width configuration of the quantized model in the provided TensorBoard object.
+
+    Args:
+        tb_w: A TensorBoard object.
+        weights_conf_nodes_bitwidth: Final weights bit-width configuration.
+        activation_conf_nodes_bitwidth: Final activation bit-width configuration.
+
+    """
+
+    if len(weights_conf_nodes_bitwidth) > 0:
+        visual = WeightsFinalBitwidthConfigVisualizer(weights_conf_nodes_bitwidth)
+        figure = visual.plot_config_bitwidth()
+        tb_w.add_figure(figure, f'Weights final bit-width config')
+    if len(activation_conf_nodes_bitwidth) > 0:
+        visual = ActivationFinalBitwidthConfigVisualizer(activation_conf_nodes_bitwidth)
+        figure = visual.plot_config_bitwidth()
+        tb_w.add_figure(figure, f'Activation final bit-width config')
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py`

 * *Files 13% similar despite different names*

```diff
@@ -34,13 +34,13 @@
         mode: Mode of the Keras model builder.
 
     Returns:
         Keras model builder for the given mode.
     """
 
     if not isinstance(mode, ModelBuilderMode):
-        Logger.error(f'get_keras_model_builder expects a mode of type ModelBuilderMode, but {type(mode)} was passed.')
+        Logger.critical(f"Expected a ModelBuilderMode type for 'mode', but received {type(mode)} instead.")
     if mode is None:
-        Logger.error(f'get_keras_model_builder received a mode which is None')
+        Logger.critical(f"get_keras_model_builder received 'mode' is None")
     if mode not in keras_model_builders.keys():
-        Logger.error(f'mode {mode} is not in keras model builders factory')
+        Logger.critical(f"'mode' {mode} is not recognized in the Keras model builders factory.")
     return keras_model_builders.get(mode)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/float_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/instance_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py`

 * *Files 11% similar despite different names*

```diff
@@ -20,14 +20,15 @@
 from networkx.algorithms.dag import topological_sort
 
 import tensorflow as tf
 from tensorflow.keras.layers import Layer, InputLayer
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.keras.constants import LAYER_NAME
+from model_compression_toolkit.logger import Logger
 
 
 class OperationHandler:
     """
     Class to handle conversions from graph nodes to Keras operators and retrieving them.
     """
 
@@ -80,15 +81,21 @@
 
     Returns:
         Keras layer that was built from the node.
     """
     framework_attr = copy.copy(n.framework_attr)
     _layer_class = n.layer_class
     framework_attr[LAYER_NAME] = n.name  # Overwrite framework name to identical graph node name
-    node_instance = _layer_class.from_config(framework_attr)  # Build layer from node's configuration.
+    try:
+        node_instance = _layer_class.from_config(framework_attr)  # Build layer from node's configuration.
+    except Exception as e:
+        Logger.info(e) # pragma: no cover
+        Logger.critical(
+            f"Keras can not de-serialize layer {_layer_class} in order to build a static graph representation. This is probably because "
+            f"your model contains custom layers which MCT doesn't support. Please provide a model without custom layers.") # pragma: no cover
     with tf.name_scope(n.name):
         # Add layer name to default weight name to avoid name duplications
         node_instance.build(n.input_shape)
     node_instance.set_weights(n.get_weights_list())
     node_instance.trainable = False  # Set all node as not trainable
     return node_instance
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,29 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import tensorflow as tf
-from keras.engine.input_layer import InputLayer
-from keras.models import Model, clone_model
+from keras.models import Model
 from packaging import version
 
-from model_compression_toolkit.constants import INPUT_BASE_NAME
 from model_compression_toolkit.core.common.back2framework.base_model_builder import BaseModelBuilder
 from model_compression_toolkit.core.common.user_info import UserInformation
-from mct_quantizers import KerasActivationQuantizationHolder
 
-# As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Input
-    from tensorflow.python.keras.layers.core import TFOpLambda
-    from tensorflow.python.keras.engine.base_layer import TensorFlowOpLayer
-    from tensorflow.python.keras.layers import Layer
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras import Input
+    from keras.src.layers.core import TFOpLambda
+    from keras.src.engine.base_layer import TensorFlowOpLayer, Layer
 else:
     from keras import Input
     from keras.layers.core import TFOpLambda
     from keras.engine.base_layer import TensorFlowOpLayer, Layer
 
 from typing import Any, Dict, List, Tuple, Callable
 from tensorflow.python.util.object_identity import Reference as TFReference
@@ -164,29 +159,26 @@
             output_list = [OutTensor(n, 0) for n in self.append2output]
         else:
             output_list = self.graph.get_outputs()
 
         # Hold a dictionary from an input node to its corresponding input tensor. It is needed for when
         # building the model. Initially input nodes with input tensors are added to the dictionary,
         # as they're not added later.
-        input_nodes_to_input_tensors = {inode: Input(inode.framework_attr[BATCH_INPUT_SHAPE][1:],
-                                                     name=f'{inode.name}_{INPUT_BASE_NAME}')
-                                        for
-                                        inode in self.graph.get_inputs()}
-
+        input_nodes_to_input_tensors = {inode: Input(inode.framework_attr[BATCH_INPUT_SHAPE][1:], name=inode.name)
+                                        for inode in self.graph.get_inputs()}
 
         # Build a list of the model's input tensors. Switching from a dictionary to a list
         # to keep the tensors input order, since inputs in Graph are ordered by their indices.
         inputs_list = []
         for input_node in self.graph.get_inputs():
             inputs_list.append(input_nodes_to_input_tensors.get(input_node))
 
         # Build a dictionary from node to its output tensors, by applying the layers sequentially.
         for n in self.oh.node_sort:
-            op_func = self.oh.get_node_op_function(n) # Get node operation function
+            op_func = self.oh.get_node_op_function(n)  # Get node operation function
 
             input_tensors = self._build_input_tensors_list(n,
                                                            node_to_output_tensors_dict)  # Fetch Node inputs
             out_tensors_of_n, out_tensors_of_n_float = self._run_operation(n,  # Run node operation and fetch outputs
                                                                            input_tensors,
                                                                            op_func,
                                                                            input_nodes_to_input_tensors)
@@ -234,15 +226,14 @@
         List[TFReference]]:
         """
         Given a node, build a list of input tensors the node gets. The list is built
         based on the node's incoming edges and previous nodes' output tensors.
 
         Args:
             node: Node to build its input tensors list.
-            graph: Graph the node is in.
             node_to_output_tensors_dict: A dictionary from a node to its output tensors.
 
         Returns:
             A list of the node's input tensors.
         """
 
         input_tensors = []
@@ -275,14 +266,15 @@
         """
         if len(input_tensors) == 0:  # Placeholder handling
             out_tensors_of_n_float = input_nodes_to_input_tensors[n]
             out_tensors_of_n = self._run_operation_activation_quantization(n,
                                                                            out_tensors_of_n_float)
         else:
             input_tensors = [tensor for tensor_list in input_tensors for tensor in tensor_list]  # flat list of lists
+            input_tensors = n.insert_positional_weights_to_input_list(input_tensors)
             # Build a functional node using its args
             if isinstance(n, FunctionalNode):
                 if n.inputs_as_list:  # If the first argument should be a list of tensors:
                     out_tensors_of_n_float = op_func(input_tensors, *n.op_call_args, **n.op_call_kwargs)
                 else:  # If the input tensors should not be a list but iterated:
                     out_tensors_of_n_float = op_func(*input_tensors, *n.op_call_args, **n.op_call_kwargs)
             else:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/default_framework_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/default_framework_info.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,57 +15,57 @@
 
 
 import tensorflow as tf
 
 from model_compression_toolkit.core.keras.quantizer.lut_fake_quant import activation_lut_kmean_quantizer
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.keras.constants import SOFTMAX, LINEAR, RELU, SWISH, SIGMOID, IDENTITY, TANH, SELU, \
     KERNEL, DEPTHWISE_KERNEL
 from model_compression_toolkit.core.keras.quantizer.fake_quant_builder import power_of_two_quantization, symmetric_quantization, uniform_quantization
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
 If a layer that is not listed here is queried, [None] is returned.
 """
 KERNEL_ATTRIBUTES = DefaultDict({Conv2D: [KERNEL],
                                  DepthwiseConv2D: [DEPTHWISE_KERNEL],
                                  Dense: [KERNEL],
-                                 Conv2DTranspose: [KERNEL]}, lambda: [None])
+                                 Conv2DTranspose: [KERNEL]}, [None])
 
 
 """
 Map a layer to its kernel's output and input channels indices.
 Map's values are tuples of (output_channel_index, input_channel_index).
 Default value is returned for layers that are not included.
 """
 DEFAULT_CHANNEL_AXIS_DICT = DefaultDict({Conv2D: (3, 2),
                                          DepthwiseConv2D: (2, 2),
                                          Dense: (1, 0),
-                                         Conv2DTranspose: (2, 3)}, lambda: (None, None))
+                                         Conv2DTranspose: (2, 3)}, (None, None))
 
 
 """
 Map a layer to its output channel axis. 
 Where axis=-1 is the last axis
 """
 DEFAULT_OUT_CHANNEL_AXIS_DICT = DefaultDict({Conv2D: -1,
                                              DepthwiseConv2D: -1,
                                              Dense: -1,
                                              Conv2DTranspose: -1},
-                                            lambda: -1)
+                                            -1)
 
 
 """
 Map from an activation function to its min/max output values (if known).
 The values are used for tensor min/max values initialization.
 """
 ACTIVATION2MINMAX = {SOFTMAX: (0, SOFTMAX_THRESHOLD),
@@ -85,15 +85,15 @@
 LAYER2MINMAX = {Softmax: (0, SOFTMAX_THRESHOLD),
                 ELU: (-1, None),
                 tf.nn.silu: (-0.279, None),
                 tf.nn.swish: (-0.279, None),
                 tf.nn.sigmoid: (0, 1),
                 tf.nn.tanh: (-1, 1),
                 tf.nn.relu: (0, None),
-                tf.nn.relu6: (0, 6),
+                tf.nn.relu6: (0, None),
                 tf.nn.gelu: (-0.17, None),
                 tf.nn.elu: (-1, None),
                 tf.nn.selu: (-1.76, None),
                 tf.nn.softplus: (0, None),
                 tf.nn.softmax: (0, SOFTMAX_THRESHOLD),
                 }
 """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,25 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-
+import keras.layers
 from tensorflow.keras.layers import Dense, DepthwiseConv2D, Conv2D, Conv2DTranspose, Activation, SeparableConv2D
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.constants import FLOAT_32, DATA_TYPE
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.keras.constants import LINEAR, ACTIVATION, TRAINABLE, LAYER_NAME
+from model_compression_toolkit.core.keras.constants import LINEAR, ACTIVATION, TRAINABLE, LAYER_NAME, SOFTMAX, AXIS, \
+    SOFTMAX_AXIS_DEFAULT
 
 
 class ActivationDecomposition(common.BaseSubstitution):
     """
     Replace a linear layer that has an activation function, with two nodes: same linear layer without
     an activation function, and a new activation layer to replace the function the linear node had.
     """
@@ -58,28 +59,44 @@
             graph: Graph we apply the substitution on.
             op2d_node: Node to extract its activation function.
 
         Returns:
             Graph after applying the substitution.
         """
 
+        if ACTIVATION not in op2d_node.framework_attr:
+            Logger.warning(f'Op2d node {op2d_node.name} of type {op2d_node.type} is missing an "{ACTIVATION}"'
+                           f' attribute -> Skipping substitution ActivationDecomposition')  # pragma: no cover
+            return graph
+
         activation_node_name = op2d_node.name + '_post_activation'
 
-        activation_fw_attr = {
-            LAYER_NAME: activation_node_name,
-            TRAINABLE: False,
-            DATA_TYPE: FLOAT_32,
-            ACTIVATION: op2d_node.framework_attr.get(ACTIVATION)}
-
-        activation_node = common.graph.BaseNode(activation_node_name,
-                                                activation_fw_attr,
-                                                op2d_node.output_shape,
-                                                op2d_node.output_shape,
-                                                {},
-                                                Activation)
+        # Softmax is a special case where we need to know the default axis parameter used
+        # and for this reason we create a Softmax layer and not Activation layer.
+        if op2d_node.framework_attr.get(ACTIVATION) == SOFTMAX:
+            activation_fw_attr = {AXIS: SOFTMAX_AXIS_DEFAULT}
+            activation_node = common.graph.BaseNode(activation_node_name,
+                                                    activation_fw_attr,
+                                                    op2d_node.output_shape,
+                                                    op2d_node.output_shape,
+                                                    {},
+                                                    keras.layers.Softmax)
+        else:
+            activation_fw_attr = {
+                LAYER_NAME: activation_node_name,
+                TRAINABLE: False,
+                DATA_TYPE: FLOAT_32,
+                ACTIVATION: op2d_node.framework_attr.get(ACTIVATION)}
+
+            activation_node = common.graph.BaseNode(activation_node_name,
+                                                    activation_fw_attr,
+                                                    op2d_node.output_shape,
+                                                    op2d_node.output_shape,
+                                                    {},
+                                                    Activation)
 
         graph.add_node(activation_node)
         graph.reconnect_out_edges(current_node=op2d_node,
                                   new_node=activation_node)
         graph.add_edge(op2d_node,
                        activation_node,
                        source_index=0,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, EdgeMatcher, WalkMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.constants import THRESHOLD
 from model_compression_toolkit.core.keras.constants import KERNEL
+from model_compression_toolkit.logger import Logger
 
 input_node = NodeOperationMatcher(InputLayer)
 zeropad_node = NodeOperationMatcher(ZeroPadding2D)
 op2d_node = NodeOperationMatcher(Dense) | \
             NodeOperationMatcher(Conv2D) | \
             NodeOperationMatcher(DepthwiseConv2D) | \
             NodeOperationMatcher(Conv2DTranspose)
@@ -76,38 +77,41 @@
             Graph after applying the substitution.
         """
 
         input_layer = nodes_list[0]
         linear_layer = nodes_list[-1]
 
         if not input_layer.is_all_activation_candidates_equal():
-            raise Exception("Input scaling is not supported for more than one activation quantization configuration "
-                            "candidate")
+            Logger.critical("Input scaling is not supported for nodes with more than one activation quantization configuration "
+                            "candidate.")
 
         # all candidates have same activation config, so taking the first candidate for calculations
         threshold = input_layer.candidates_quantization_cfg[0].activation_quantization_cfg.activation_quantization_params.get(THRESHOLD)
 
         if threshold is None:
             return graph
 
         min_value, max_value = graph.get_out_stats_collector(input_layer).get_min_max_values()
         threshold_float = max(abs(min_value), max_value)
 
         if threshold > threshold_float:
             scale_factor = threshold_float / threshold
             graph.user_info.set_input_scale(1 / scale_factor)
 
-            w1_fixed = linear_layer.get_weights_by_keys(KERNEL) * scale_factor
-            linear_layer.set_weights_by_keys(KERNEL, w1_fixed)
+            kernel_attr = graph.fw_info.get_kernel_op_attributes(linear_layer.type)[0]
+
+            w1_fixed = linear_layer.get_weights_by_keys(kernel_attr) * scale_factor
+            linear_layer.set_weights_by_keys(kernel_attr, w1_fixed)
 
             graph.scale_stats_collector(input_layer, 1 / scale_factor)
 
             # After scaling weights may have different thresholds so it needs to be recalculated
             for nqc in linear_layer.candidates_quantization_cfg:
-                nqc.weights_quantization_cfg.calculate_and_set_weights_params(w1_fixed)
+                nqc.weights_quantization_cfg.get_attr_config(kernel_attr).calculate_and_set_weights_params(w1_fixed,
+                                                                                                           nqc.weights_quantization_cfg.min_threshold)
 
         return graph
 
 
 class InputScaling(BaseInputScaling):
     """
     Substitution extends BaseInputScaling to the case of Input-->Linear
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,18 +11,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Tuple
 import numpy as np
 import tensorflow as tf
-from tensorflow.keras.layers import Conv2D
+if tf.__version__ < "2.6":
+    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense
+else:
+    from keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense
+
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, NodeFrameworkAttrMatcher
-from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing
+from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing, Op2DAddConstCollapsing
 from model_compression_toolkit.core.keras.constants import KERNEL, KERNEL_SIZE, STRIDES, DILATIONS, LINEAR, \
     ACTIVATION, BIAS, USE_BIAS, LAYER_NAME, FILTERS, PADDING, GROUPS, DATA_FORMAT
 from model_compression_toolkit.logger import Logger
 
 
 def linear_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
@@ -96,15 +100,15 @@
             if bias2 is not None:
                 bias_collapsed += bias2
         elif bias2 is not None:
             bias_collapsed = bias2
 
         return kernel_collapsed, bias_collapsed
     else:
-        Logger.error("No supported layer collapsing of {} and {}".format(first_node.type, second_node.type))
+        Logger.critical(f"Layer collapsing unsupported for combination: {first_node.type} and {second_node.type}.")
 
 
 def keras_linear_collapsing() -> Conv2DCollapsing:
     """
     Returns:
         A Conv2DCollapsing initialized for Keras models.
     """
@@ -119,7 +123,62 @@
                             STRIDES,
                             PADDING,
                             DILATIONS,
                             GROUPS,
                             FILTERS,
                             data_format_str=DATA_FORMAT,
                             layer_name_str=LAYER_NAME)
+
+
+def op2d_add_const_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
+    """
+    Function generates matchers for matching:
+    (Op2D, Add(const)) -> Op2D.  (Op2D is one of [DepthwiseConv2D, Conv2D, Conv2DTranspose, Dense)
+    Returns:
+        Matcher for Op2D followed by Add const
+    """
+    first_node = NodeOperationMatcher(DepthwiseConv2D) | \
+                 NodeOperationMatcher(Conv2D) | \
+                 NodeOperationMatcher(Conv2DTranspose) | \
+                 NodeOperationMatcher(Dense)
+    second_node = NodeOperationMatcher(tf.math.add)
+    return first_node, second_node
+
+
+def op2d_add_const_collapsing_fn(op2d_node: BaseNode,
+                                 add_node: BaseNode,
+                                 bias_str: str) -> np.ndarray:
+    """
+    Collapsing Add-Const to previous node's bias
+    Args:
+        op2d_node: Op2d layer node
+        add_node: Add layer to collapse
+        bias_str: The framework specific attribute name of the convolution layer's bias.
+    Returns:
+        The modified conv layer node's bias
+    """
+    bias = op2d_node.get_weights_by_keys(bias_str)
+
+    # read constant from add node (either 1st or 2nd positional weight)
+    const = add_node.weights.get(0, add_node.weights.get(1))
+    if const is None:
+        Logger.critical(f'Failed to read constant from add node: {add_node.name}.')  # pragma: no cover
+
+    # return new bias
+    if bias is None:
+        return const
+    else:
+        return const + bias
+
+
+def keras_op2d_add_const_collapsing() -> Op2DAddConstCollapsing:
+    """
+    Returns:
+        An Op2DCollapsing initialized for Keras models.
+    """
+    first_node, second_node = op2d_add_const_collapsing_node_matchers()
+    return Op2DAddConstCollapsing(first_node,
+                                  second_node,
+                                  op2d_add_const_collapsing_fn,
+                                  BIAS,
+                                  USE_BIAS,
+                                  layer_name_str=LAYER_NAME)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,17 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import numpy as np
 import tensorflow as tf
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.layers.core import TFOpLambda
-    from tensorflow.keras.layers import MultiHeadAttention, Conv2D, Softmax, Concatenate, Reshape, Permute
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers.core import TFOpLambda
+    from keras.src.layers import MultiHeadAttention, Conv2D, Softmax, Concatenate, Reshape, Permute
 else:
     from keras.layers.core import TFOpLambda
     from keras.layers import MultiHeadAttention, Conv2D, Softmax, Concatenate, Reshape, Permute
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph, BaseNode, OutTensor
@@ -444,15 +444,15 @@
             mha_node: MultiHeadAttention node to replace.
 
         Returns:
             Graph after applying the substitution.
         """
 
         if mha_node.reuse:
-            Logger.error("MCT doesn't support reuse of MultiHeadAttention layer")  # pragma: no cover
+            Logger.critical("Reuse of MultiHeadAttention layers is currently not supported.")  # pragma: no cover
         params = MHAParams(mha_node)
 
         mha_in_edges = graph.in_edges(mha_node)
 
         # input permutation and reshape to standard shape: (batch, iterations, sequence, channels)
         q_permute_node, k_permute_node, v_permute_node, \
         q_reshape_node, k_reshape_node, v_reshape_node = \
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 8% similar despite different names*

```diff
@@ -58,15 +58,15 @@
         idxH = (kH - 1) // 2
         idxW = (kW - 1) // 2
         for i in range(Cout):
             kernel[idxH, idxW, i, i] += 1
 
         return kernel
     else:
-        Logger.error("No supported add residual collapsing for {}".format(first_node.type))
+        Logger.critical(f"Residual collapsing is unsupported for {first_node.type} node types.")
 
 
 def keras_residual_collapsing() -> ResidualCollapsing:
     """
     Returns:
         A ResidualCollapsing initialized for Keras models.
     """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from tensorflow.keras.layers import SeparableConv2D, Conv2D, DepthwiseConv2D
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.keras.constants import KERNEL, DEPTHWISE_KERNEL, BIAS, KERNEL_SIZE, PADDING, \
-    STRIDES, USE_BIAS, LINEAR, ACTIVATION, TRAINABLE, FILTERS, PAD_VALID
+    STRIDES, USE_BIAS, LINEAR, ACTIVATION, TRAINABLE, FILTERS, PAD_VALID, GROUPS
 
 POINTWISE_KERNEL = 'pointwise_kernel'
 DEPTH_MULTIPLIER = 'depth_multiplier'
 DATA_FORMAT = 'data_format'
 DILATION_RATE = 'dilation_rate'
 DEPTHWISE_INITIALIZER = 'depthwise_initializer'
 DEPTHWISE_REGULARIZER = 'depthwise_regularizer'
@@ -71,29 +71,31 @@
         """
 
         dw_kernel = separable_node.get_weights_by_keys(DEPTHWISE_KERNEL)
         pw_kernel = separable_node.get_weights_by_keys(POINTWISE_KERNEL)
         pw_bias = separable_node.get_weights_by_keys(BIAS)
 
         dw_weights_dict = {DEPTHWISE_KERNEL: dw_kernel}
-        pw_weights_dict = {KERNEL: pw_kernel,
-                           BIAS: pw_bias}
+        pw_weights_dict = {KERNEL: pw_kernel}
+
+        if pw_bias is not None:
+            pw_weights_dict[BIAS] = pw_bias
 
         # Split separable node attributes into relevant attributes for each of the new nodes.
         # List of dw attributes that should take from separable as they are.
         dw_attr_list = [KERNEL_SIZE, STRIDES, PADDING, DEPTH_MULTIPLIER, DATA_FORMAT, DILATION_RATE,
                         DEPTHWISE_INITIALIZER, DEPTHWISE_REGULARIZER, DEPTHWISE_CONSTRAINT, TRAINABLE]
 
         dw_framework_attr = {attr: separable_node.framework_attr[attr] for attr in dw_attr_list}
         dw_framework_attr.update({ACTIVATION: LINEAR,
                                   USE_BIAS: False})
 
         # List of pw attributes that should take from separable as they are.
         pw_attr_list = [FILTERS, DATA_FORMAT, DILATION_RATE, ACTIVATION, USE_BIAS, BIAS_CONSTRAINT,
-                        BIAS_INITIALIZER, BIAS_REGULARIZER, TRAINABLE, ACTIVITY_REGULARIZER]
+                        BIAS_INITIALIZER, BIAS_REGULARIZER, TRAINABLE, ACTIVITY_REGULARIZER, GROUPS]
 
         pw_framework_attr = {attr: separable_node.framework_attr[attr] for attr in pw_attr_list}
 
         # Use more attributes that are not taken as are
         pw_framework_attr.update({KERNEL_SIZE: (1, 1),
                                   STRIDES: (1, 1),
                                   PADDING: PAD_VALID,
@@ -108,38 +110,40 @@
         # compute input/outpus shapes of new nodes
         dw_output_shape = tuple(dw_layer_class(**dw_framework_attr).compute_output_shape(separable_node.input_shape))
         pw_input_shape = dw_output_shape
 
         # If the SeparableConv2D is reused, we need to keep the depthwise node as reused as well,
         # so we keep the names convention with adding the suffix of "_reuse_X".
         dw_node_name = separable_node.name + '_dw' if not separable_node.reuse else '_'.join(separable_node.name.split('_')[:-2]) + '_dw_' + '_'.join(separable_node.name.split('_')[-2:])
+        reuse_group = separable_node.reuse_group if not separable_node.reuse_group else separable_node.reuse_group + '_dw'
+
 
         # create new nodes
         dw_node = common.graph.BaseNode(dw_node_name,
                                         dw_framework_attr,
                                         separable_node.input_shape,
                                         dw_output_shape,
                                         dw_weights_dict,
                                         dw_layer_class,
                                         reuse=separable_node.reuse,
-                                        reuse_group=separable_node.reuse_group)
+                                        reuse_group=reuse_group)
 
         # If the SeparableConv2D is reused, we need to keep the pointwise node as reused as well,
         # so we keep the names convention with adding the suffix of "_reuse_X".
         pw_node_name = separable_node.name + '_pw' if not separable_node.reuse else '_'.join(separable_node.name.split('_')[:-2]) + '_pw_' + '_'.join(separable_node.name.split('_')[-2:])
+        reuse_group = separable_node.reuse_group if not separable_node.reuse_group else separable_node.reuse_group + '_pw'
 
         pw_node = common.graph.BaseNode(pw_node_name,
                                         pw_framework_attr,
                                         pw_input_shape,
                                         separable_node.output_shape,
                                         pw_weights_dict,
                                         pw_layer_class,
                                         reuse=separable_node.reuse,
-                                        reuse_group=separable_node.reuse_group
-                                        )
+                                        reuse_group=reuse_group)
 
         graph.add_node(dw_node)
         graph.add_node(pw_node)
         graph.add_edge(dw_node,
                        pw_node,
                        source_index=0,
                        sink_index=0)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,19 +12,24 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Tuple, Any
 
 import numpy as np
-import tensorflow as tf
 
+from packaging import version
+import tensorflow as tf
 from tensorflow.python.keras.layers.core import TFOpLambda
-from tensorflow.keras.layers import Activation, Conv2D, Dense, DepthwiseConv2D, ZeroPadding2D, Reshape, \
-    GlobalAveragePooling2D, Dropout, ReLU, PReLU, ELU
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Activation, Conv2D, Dense, DepthwiseConv2D, ZeroPadding2D, Reshape, \
+        GlobalAveragePooling2D, Dropout, ReLU, PReLU, ELU
+else:
+    from tensorflow.keras.layers import Activation, Conv2D, Dense, DepthwiseConv2D, ZeroPadding2D, Reshape, \
+        GlobalAveragePooling2D, Dropout, ReLU, PReLU, ELU
 
 from model_compression_toolkit.core import CoreConfig, FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.substitutions.shift_negative_activation import \
@@ -100,15 +105,16 @@
                               {FUNCTION: ADD},
                               input_shape,
                               input_shape,
                               weights={},
                               quantization_attr={},
                               layer_class=TFOpLambda,
                               op_call_args=[np.array(add_value, dtype=np.float32).reshape([1] * len(input_shape))],
-                              op_call_kwargs={})
+                              op_call_kwargs={},
+                              functional_op=tf.add)
     return add_node
 
 
 def create_pad_node(next_node_name: str,
                     prev_node_name: str,
                     value_to_pad: float,
                     input_shape: tuple,
@@ -148,15 +154,16 @@
                               input_shape,
                               tuple(padded_shape),
                               weights={},
                               quantization_attr={},
                               layer_class=TFOpLambda,
                               op_call_args=[],
                               op_call_kwargs={'paddings': num_elements_to_pad,
-                                              'constant_values': value_to_pad})
+                                              'constant_values': value_to_pad},
+                              functional_op=tf.pad)
 
     return pad_node
 
 
 def compute_op2d_padding(op2d_node: BaseNode) -> Tuple[int, int, int, int]:
     """
     Compute the padding around an input tensor of a linear node.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_implementation.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,64 +8,79 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from functools import partial
 from typing import List, Any, Tuple, Callable, Dict
 
 import numpy as np
 import tensorflow as tf
+from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder
 from tensorflow.keras.models import Model
-from tensorflow.python.layers.base import Layer
 
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, HessianInfoService
+from model_compression_toolkit.core.keras.hessian.activation_trace_hessian_calculator_keras import \
+    ActivationTraceHessianCalculatorKeras
+from model_compression_toolkit.core.keras.hessian.weights_trace_hessian_calculator_keras import WeightsTraceHessianCalculatorKeras
+from model_compression_toolkit.exporter.model_wrapper.fw_agnostic.get_inferable_quantizers import \
+    get_inferable_quantizers
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
+    get_weights_quantizer_for_node, get_activations_quantizer_for_node
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
+from model_compression_toolkit.core.common.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
 from model_compression_toolkit.core.common.similarity_analyzer import compute_kl_divergence, compute_cs, compute_mse
-from model_compression_toolkit.core.keras.back2framework.model_gradients import \
-    keras_iterative_approx_jacobian_trace
-from model_compression_toolkit.core.keras.constants import ACTIVATION, SOFTMAX, SIGMOID, ARGMAX, LAYER_NAME
+from model_compression_toolkit.core.keras.constants import ACTIVATION, SOFTMAX, SIGMOID, ARGMAX, LAYER_NAME, \
+    COMBINED_NMS
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_reconstruction import \
     keras_batchnorm_reconstruction
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.virtual_activation_weights_composition import \
     VirtualActivationWeightsComposition
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.weights_activation_split import \
     WeightsActivationSplit
-from model_compression_toolkit.core.keras.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
+from model_compression_toolkit.core.keras.mixed_precision.configurable_activation_quantizer import \
+    ConfigurableActivationQuantizer
+from model_compression_toolkit.core.keras.mixed_precision.configurable_weights_quantizer import \
+    ConfigurableWeightsQuantizer
 from model_compression_toolkit.core.keras.statistics_correction.apply_second_moment_correction import \
     keras_apply_second_moment_correction
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, Concatenate, Add
-    from tensorflow.python.keras.layers.core import TFOpLambda
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, \
+        Concatenate, Add
+    from keras.src.layers.core import TFOpLambda
 else:
     from keras.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, \
         Concatenate, Add
     from keras.layers.core import TFOpLambda
 
-from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
-from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.activation_decomposition import \
     ActivationDecomposition
+from model_compression_toolkit.core.keras.graph_substitutions.substitutions.matmul_substitution import \
+    MatmulToDenseSubstitution
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.softmax_shift import \
     keras_softmax_shift
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_folding import \
-    keras_batchnorm_folding
+    keras_batchnorm_folding, keras_batchnorm_forward_folding
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_refusing import \
     keras_batchnorm_refusing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.linear_collapsing import \
-    keras_linear_collapsing
+    keras_linear_collapsing, keras_op2d_add_const_collapsing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.residual_collapsing import \
     keras_residual_collapsing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.input_scaling import InputScaling, \
     InputScalingWithPad
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.relu_bound_to_power_of_2 import \
     ReLUBoundToPowerOfTwo
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.remove_relu_upper_bound import \
@@ -74,26 +89,25 @@
     MultiHeadAttentionDecomposition
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.scale_equalization import \
     ScaleEqualization, ScaleEqualizationWithPad, ScaleEqualizationMidActivation, ScaleEqualizationMidActivationWithPad
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.separableconv_decomposition import \
     SeparableConvDecomposition, DEPTH_MULTIPLIER
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.shift_negative_activation import \
     keras_apply_shift_negative_correction
+from model_compression_toolkit.core.keras.graph_substitutions.substitutions.dwconv_to_conv import DwconvToConv
 from model_compression_toolkit.core.keras.keras_node_prior_info import create_node_prior_info
 from model_compression_toolkit.core.keras.reader.reader import model_reader
-from model_compression_toolkit.core.common.collectors.statistics_collector_generator import \
-    create_stats_collector_for_node
 import model_compression_toolkit.core.keras.constants as keras_constants
 from model_compression_toolkit.core.keras.tf_tensor_numpy import tf_tensor_to_numpy, to_tf_tensor
 from model_compression_toolkit.core.keras.back2framework import get_keras_model_builder
 
 
 class KerasImplementation(FrameworkImplementation):
     """
-    An class with implemented methods to support optimizing Keras models.
+    A class with implemented methods to support optimizing Keras models.
     """
 
     def __init__(self):
         super().__init__()
 
     @property
     def constants(self):
@@ -141,28 +155,28 @@
         return to_tf_tensor(tensor)
 
     def model_builder(self,
                       graph: Graph,
                       mode: ModelBuilderMode,
                       append2output: List[Any] = None,
                       fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                      return_float_outputs: bool = False) -> Tuple[Model, UserInformation]:
+                      return_float_outputs: bool = False) -> Tuple:
         """
         Build a Keras model from a graph.
         The mode determines how the model should be build. append2output is a list of Nodes
         to set as the model outputs.
 
         Args:
             graph: Graph to build the model from it.
             mode: Mode for how to build the model.
             append2output: List of Nodes to set as the model's outputs.
             fw_info: FrameworkInfo object with information about the specific framework's model
             return_float_outputs (bool): whether to return outputs before or after quantization nodes (default)
         Returns:
-            A tuple of the Keras model that was built and an UserInformation object.
+            A tuple with the model and additional relevant supporting objects.
         """
 
         keras_model_builder = get_keras_model_builder(mode)
         return keras_model_builder(graph=graph,
                                    append2output=append2output,
                                    fw_info=fw_info,
                                    return_float_outputs=return_float_outputs).build_model()
@@ -197,29 +211,14 @@
         Returns:
             Graph after SNC.
         """
         return keras_apply_shift_negative_correction(graph,
                                                      core_config,
                                                      fw_info)
 
-    def attach_sc_to_node(self,
-                          node: BaseNode,
-                          fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel)
-
-        Returns:
-            Statistics collector for the node.
-        """
-        return create_stats_collector_for_node(node, fw_info)
 
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
@@ -241,30 +240,33 @@
     def get_substitutions_prepare_graph(self, fw_info: FrameworkInfo = None) -> List[common.BaseSubstitution]:
         """
 
         Returns: A list of the framework substitutions used to prepare the graph.
 
         """
         return [SeparableConvDecomposition(),
+                MatmulToDenseSubstitution(),
                 MultiHeadAttentionDecomposition(),
-                ActivationDecomposition()]
+                ActivationDecomposition(),
+                DwconvToConv()]
 
     def get_substitutions_pre_statistics_collection(self, quant_config: QuantizationConfig) -> \
             List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used before we collect statistics.
 
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
 
         Returns:
             A list of the framework substitutions used before we collect statistics.
 
         """
-        substitutions_list = [keras_batchnorm_folding()]
+        substitutions_list = [keras_batchnorm_folding(),
+                              keras_batchnorm_forward_folding()]
         if quant_config.relu_bound_to_power_of_2:
             substitutions_list.append(ReLUBoundToPowerOfTwo())
         return substitutions_list
 
     def get_substitutions_statistics_correction(self, quant_config: QuantizationConfig) -> \
             List[common.BaseSubstitution]:
         """
@@ -290,14 +292,20 @@
 
     def get_linear_collapsing_substitution(self) -> common.BaseSubstitution:
         """
         Returns: linear collapsing substitution
         """
         return keras_linear_collapsing()
 
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: Op2d add-const collapsing substitution
+        """
+        return keras_op2d_add_const_collapsing()
+
     def get_substitutions_post_statistics_collection(self, quant_config: QuantizationConfig) \
             -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used after we collect statistics.
 
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
@@ -344,41 +352,47 @@
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(keras_batchnorm_refusing())
         return substitutions_list
 
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
-                                  disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
+                                  disable_activation_for_metric: bool = False,
+                                  hessian_info_service: HessianInfoService = None) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
         Returns:
             A SensitivityEvaluation object.
         """
 
         return SensitivityEvaluation(graph=graph,
                                      quant_config=quant_config,
                                      representative_data_gen=representative_data_gen,
                                      fw_info=fw_info,
                                      fw_impl=self,
-                                     set_layer_to_bitwidth=set_layer_to_bitwidth,
-                                     get_quant_node_name=lambda node_name: f'quant_{node_name}',
-                                     disable_activation_for_metric=disable_activation_for_metric)
+                                     set_layer_to_bitwidth=partial(set_layer_to_bitwidth,
+                                                                   weights_quantizer_type=ConfigurableWeightsQuantizer,
+                                                                   activation_quantizer_type=ConfigurableActivationQuantizer,
+                                                                   weights_quant_layer_type=KerasQuantizationWrapper,
+                                                                   activation_quant_layer_type=KerasActivationQuantizationHolder),
+                                     disable_activation_for_metric=disable_activation_for_metric,
+                                     hessian_info_service=hessian_info_service)
 
     def get_node_prior_info(self,
                             node: BaseNode,
                             fw_info: FrameworkInfo,
                             graph: Graph) -> NodePriorInfo:
         """
         Get a NodePriorInfo object for a node that represents a Keras layer.
@@ -403,135 +417,106 @@
         Returns: True if the node should be considered an interest point, False otherwise.
         """
 
         if node.type == Activation:
             node_type_name = node.framework_attr[keras_constants.ACTIVATION]
             if node_type_name in [keras_constants.SOFTMAX, keras_constants.SIGMOID]:
                 return True
-        elif node.type in [tf.nn.softmax, tf.nn.sigmoid, Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, Concatenate,
-                           Add, tf.add]:
+        elif node.type in [tf.nn.softmax, tf.keras.layers.Softmax, tf.nn.sigmoid, Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, Concatenate,
+                           tf.concat, Add, tf.add]:
             return True
 
         return False
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         if compute_distance_fn is not None:
             return compute_distance_fn
 
         if layer_class == Activation:
             node_type_name = framework_attrs[ACTIVATION]
-            if node_type_name == SOFTMAX:
+            if node_type_name == SOFTMAX and axis is not None:
                 return compute_kl_divergence
             elif node_type_name == SIGMOID:
                 return compute_cs
-        elif layer_class == tf.nn.softmax:
+        elif axis is not None and (layer_class == tf.nn.softmax or layer_class == tf.keras.layers.Softmax
+                                   or (layer_class == TFOpLambda and
+                                       SOFTMAX in framework_attrs[keras_constants.FUNCTION])):
             return compute_kl_divergence
         elif layer_class == tf.nn.sigmoid:
             return compute_cs
         elif layer_class == Dense:
             return compute_cs
         return compute_mse
 
-    def get_model_layers_names(self,
-                               model: Model) -> List[str]:
-        """
-        Returns a list of the given model's layers names.
-
-        Args:
-            model: A Keras model.
-
-        Returns: List of layers' names.
-
-        """
-
-        return [layer.name for layer in model.layers]
-
-    def get_model_layer_by_name(self,
-                                model: Model,
-                                layer_name: str) -> Layer:
-        """
-        Returns a Keras model's layer by its name.
-
-        Args:
-            model: A Keras model to retrieve a layer from.
-            layer_name: The requested layer's name.
-
-        Returns: A Keras layer object.
-
-        """
-
-        return model.get_layer(name=layer_name)
-
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, np.ndarray],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a Keras model gradient calculation function, which computes the jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list
-            alpha:A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated  jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
-
-        """
-
-        return keras_iterative_approx_jacobian_trace(graph_float, model_input_tensors, interest_points, output_list,
-                                                     all_outputs_indices, alpha, n_iter, norm_weights=norm_weights)
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get Keras trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculatorKeras to use for the trace hessian approximation computation for this request.
+
+        """
+        if trace_hessian_request.mode == HessianMode.ACTIVATION:
+            return ActivationTraceHessianCalculatorKeras(graph=graph,
+                                                         trace_hessian_request=trace_hessian_request,
+                                                         input_images=input_images,
+                                                         fw_impl=self,
+                                                         num_iterations_for_approximation=num_iterations_for_approximation)
+        elif trace_hessian_request.mode == HessianMode.WEIGHTS:
+            return WeightsTraceHessianCalculatorKeras(graph=graph,
+                                                      trace_hessian_request=trace_hessian_request,
+                                                      input_images=input_images,
+                                                      fw_impl=self,
+                                                      num_iterations_for_approximation=num_iterations_for_approximation)
+        else:
+            Logger.critical(f"Unsupported Hessian mode for Keras: {trace_hessian_request.mode}.")
 
-    def is_node_compatible_for_metric_outputs(self,
-                                              node: BaseNode) -> Any:
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> Any:
         """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
         if node.layer_class == TFOpLambda:
             node_attr = getattr(node, 'framework_attr', None)
-            if node_attr is not None and (ARGMAX in node_attr[LAYER_NAME] or SOFTMAX in node_attr[LAYER_NAME]):
+            if node_attr is not None and (ARGMAX in node_attr[LAYER_NAME]
+                                          or COMBINED_NMS in node_attr[LAYER_NAME]):
                 return False
-        elif node.layer_class == tf.nn.softmax or node.layer_class == tf.math.argmax:
+        elif node.layer_class in [tf.math.argmax]:
             return False
 
         return True
 
     def get_node_mac_operations(self,
                                 node: BaseNode,
                                 fw_info: FrameworkInfo) -> float:
@@ -541,29 +526,28 @@
         Args:
             node: A graph node that wraps the operation for which the MAC count is computed.
             fw_info: FrameworkInfo object with information about the Keras model.
 
         Returns: The MAC count og the operation
         """
 
-        input_shape = node.input_shape
         output_shape = node.output_shape
         kernel_shape = node.get_weights_by_keys(fw_info.get_kernel_op_attributes(node.type)[0]).shape
         output_channel_axis, input_channel_axis = fw_info.kernel_channels_mapping.get(node.type)
 
         if node.type is Conv2D or node.type is Conv2DTranspose:
             # (C_out * W_out * H_out) * C_in * (W_kernel * H_kernel)
             return np.prod([x for x in output_shape if x is not None]) * \
-                   input_shape[input_channel_axis] * \
+                   kernel_shape[input_channel_axis] * \
                    (kernel_shape[0] * kernel_shape[1])
         elif node.type is DepthwiseConv2D:
             # Depth * (W_out * H_out) * C_in * (W_kernel * H_kernel)
             return node.framework_attr.get(DEPTH_MULTIPLIER) * \
                    np.prod([x for x in output_shape if x is not None]) / output_shape[output_channel_axis] * \
-                   input_shape[input_channel_axis] * \
+                   kernel_shape[input_channel_axis] * \
                    (kernel_shape[0] * kernel_shape[1])
         elif node.type is Dense:
             # IN * OUT
             return kernel_shape[0] * kernel_shape[1]
         else:
             return 0
 
@@ -599,7 +583,43 @@
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
 
         return model(inputs)
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of Keras compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        def _weight_name(w: str) -> str:
+            """
+            Extracts the weight name from the full TensorFlow variable name.
+
+            For example, returns 'kernel' for 'dense_2/kernel:0'.
+
+            Args:
+              w: TensorFlow variable name.
+
+            Returns:
+              Extracted weight name.
+            """
+
+            return w.split(':')[0].split('/')[-1]
+
+        attribute_names = [_weight_name(wn) for wn in node.get_node_weights_attributes()
+                           if node.is_weights_quantization_enabled(wn)]
+
+        return get_inferable_quantizers(node,
+                                        get_weights_quantizer_for_node,
+                                        get_activations_quantizer_for_node,
+                                        attribute_names)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_model_validation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_model_validation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/keras_node_prior_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/keras_node_prior_info.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import Any, Tuple
 import numpy as np
 import tensorflow as tf
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Activation, ReLU, BatchNormalization
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Activation, ReLU, BatchNormalization
 else:
     from keras.layers import Activation, ReLU, BatchNormalization
 
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.keras.constants import ACTIVATION, RELU_MAX_VALUE, NEGATIVE_SLOPE, THRESHOLD, \
@@ -54,15 +54,14 @@
     Returns:
         Min/max output values if known.
     """
     min_output, max_output = None, None
 
     if node.type == ReLU:
         min_output = node.framework_attr[THRESHOLD] if node.framework_attr[NEGATIVE_SLOPE] == 0 else None
-        max_output = node.framework_attr[RELU_MAX_VALUE]
 
     elif fw_info.layers_has_min_max(node.type):
         min_output, max_output = fw_info.layer_min_max_mapping[node.type]
 
     elif node.type == Activation and fw_info.activation_has_min_max(node.framework_attr[ACTIVATION]):
         min_output, max_output = fw_info.activation_min_max_mapping[node.framework_attr[ACTIVATION]]
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/kpi_data_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,158 +8,125 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 from typing import Callable
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfig, CoreConfig, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.constants import TENSORFLOW
+from model_compression_toolkit.constants import PYTORCH, FOUND_TORCH
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    DEFAULT_MIXEDPRECISION_CONFIG
-from model_compression_toolkit.constants import FOUND_TF
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+from model_compression_toolkit.core.exporter import export_model
+from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 
-if FOUND_TF:
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
-    from tensorflow.keras.models import Model
 
+if FOUND_TORCH:
+    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from torch.nn import Module
+    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     from model_compression_toolkit import get_target_platform_capabilities
 
-    KERAS_DEFAULT_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
-
+    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
-    def keras_kpi_data(in_model: Model,
-                       representative_data_gen: Callable,
-                       quant_config: MixedPrecisionQuantizationConfig = DEFAULT_MIXEDPRECISION_CONFIG,
-                       fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                       target_platform_capabilities: TargetPlatformCapabilities = KERAS_DEFAULT_TPC) -> KPI:
+    def pytorch_post_training_quantization(in_module: Module,
+                                           representative_data_gen: Callable,
+                                           target_resource_utilization: ResourceUtilization = None,
+                                           core_config: CoreConfig = CoreConfig(),
+                                           target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform modeling, and uses it to compute the KPI data.
+        Quantize a trained Pytorch module using post-training quantization.
+        By default, the module is quantized using a symmetric constraint quantization thresholds
+        (power of two) as defined in the default TargetPlatformCapabilities.
+        The module is first optimized using several transformations (e.g. BatchNormalization folding to
+        preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+        being collected for each layer's output (and input, depends on the quantization configuration).
+        Thresholds are then being calculated using the collected statistics and the module is quantized
+        (both coefficients and activations by default).
+        If gptq_config is passed, the quantized weights are optimized using gradient based post
+        training quantization by comparing points between the float and quantized modules, and minimizing the
+        observed loss.
 
         Args:
-            in_model (Model): Keras model to quantize.
+            in_module (Module): Pytorch module to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+            core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
 
         Returns:
-            A KPI object with total weights parameters sum, max activation tensor and total kpi.
+            A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
-            Import a Keras model:
+            Import a Pytorch module:
 
-            >>> from tensorflow.keras.applications.mobilenet import MobileNet
-            >>> model = MobileNet()
+            >>> from torchvision import models
+            >>> module = models.mobilenet_v2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
 
             >>> import numpy as np
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 224, 224, 3))]
+            >>>         yield [np.random.random((4, 3, 224, 224))]
 
-            Import MCT and call for KPI data calculation:
+            Import MCT and pass the module with the representative dataset generator to get a quantized module
+            Set number of clibration iterations to 1:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
-
+            >>> quantized_module, quantization_info = mct.ptq.pytorch_post_training_quantization(module, repr_datagen)
 
         """
 
-        if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
-
-        fw_impl = KerasImplementation()
-
-        quantization_config, mp_config = quant_config.separate_configs()
-        core_config = CoreConfig(quantization_config=quantization_config,
-                                 mixed_precision_config=mp_config)
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
-
-
-    def keras_kpi_data_experimental(in_model: Model,
-                                    representative_data_gen: Callable,
-                                    core_config: CoreConfig,
-                                    fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                    target_platform_capabilities: TargetPlatformCapabilities = KERAS_DEFAULT_TPC) -> KPI:
-        """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and hw modeling, and uses it to compute the KPI data.
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use "
+                             "pytorch_post_training_quantization API, or pass a valid mixed precision "
+                             "configuration.")  # pragma: no cover
+
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+
+        fw_impl = PytorchImplementation()
+
+        # Ignore hessian info service as it is not used here yet.
+        tg, bit_widths_config, _ = core_runner(in_model=in_module,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_PYTORCH_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+
+        if core_config.debug_config.analyze_similarity:
+            analyzer_model_quantization(representative_data_gen,
+                                        tb_w,
+                                        tg,
+                                        fw_impl,
+                                        DEFAULT_PYTORCH_INFO)
 
-        Args:
-            in_model (Model): Keras model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-
-        Returns:
-
-            A KPI object with total weights parameters sum and max activation tensor.
-
-        Examples:
+        return get_exportable_pytorch_model(tg)
 
-            Import a Keras model:
-
-            >>> from tensorflow.keras.applications.mobilenet import MobileNet
-            >>> model = MobileNet()
-
-            Create a random dataset generator:
-
-            >>> import numpy as np
-            >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
-
-            Import MCT and call for KPI data calculation:
-
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
-
-        """
-
-        if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
-
-        fw_impl = KerasImplementation()
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
 
 else:
-    # If tensorflow or tensorflow_model_optimization are not installed,
-    # we raise an exception when trying to use this function.
-    def keras_kpi_data(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
-
-
-    def keras_kpi_data_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    # If torch is not installed,
+    # we raise an exception when trying to use these functions.
+    def pytorch_post_training_quantization(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_post_training_quantization_experimental'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/mixed_precision/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/base_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,22 +9,21 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer
 from typing import List, Any, Dict
 from tensorflow import Tensor
 import six, abc
 
 
 @six.add_metaclass(abc.ABCMeta)
-class BaseTrainableQuantizer(Quantizer):
+class BaseTrainableQuantizer:
     """
     Base trainable quantizer to define extra methods needed by the GPTQ post-processing.
     """
 
     @abc.abstractmethod
     def get_quant_config(self, layer) -> Dict[str, Any]:
         """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,89 +1,81 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Callable
+import torch
 
-
-from typing import Tuple, Callable
-
-import tensorflow as tf
-import numpy as np
-from tensorflow.python.util.object_identity import Reference as TFReference
-
-from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 
 
-def quantizer_min_max_calculator(threshold: np.ndarray,
-                                 num_bits: int,
-                                 signed: bool) -> Tuple[float, float]:
+def get_symmetric_quantization_range_and_scale(activation_is_signed: bool,
+                                               activation_n_bits: int,
+                                               activation_threshold: float):
     """
-    Compute quantization range's min/max values given a threshold, number of bits,
-     and whether it's signed or not.
+    Calculates lower and upper bounds on the quantization range, along with quantization scale,
+    for symmetric quantization (used for the symmetric and power-of-two quantizers),
+    according to whether the quantization is signed or unsigned.
 
     Args:
-        threshold: Threshold for quantization range values.
-        num_bits: Number of bits to use for quantization.
-        signed: Whether the quantization range should include negative values or not.
+        activation_is_signed: Whether the quantization is signed or not.
+        activation_n_bits: Number of bits to use for quantization.
+        activation_threshold: The quantization threshold.
 
-    Returns:
-        Min and max values for quantization range.
-    """
+    Returns: range lower bound, range upper bound and quantization scale.
 
-    if signed:
-        delta = threshold / (2 ** (num_bits - 1))
-        min_value = -threshold
+    """
+    if activation_is_signed:
+        min_value = -2 ** (activation_n_bits - 1)
+        max_value = 2 ** (activation_n_bits - 1) - 1
+        scale = activation_threshold / 2 ** (activation_n_bits - 1)
     else:
-        delta = threshold / (2 ** (num_bits))
         min_value = 0
+        max_value = (2 ** activation_n_bits) - 1
+        scale = activation_threshold / 2 ** activation_n_bits
 
-    max_value = threshold - delta
-    return min_value, max_value
+    return min_value, max_value, scale
 
 
 def power_of_two_quantization(activation_n_bits: int,
                               quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
-    build and return a fake-quantization node with power-of-two quantization.
-
+    build and return a fake-quantization node, quantized with a power-of-two threshold.
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
-
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None:
-        Logger.error("Activation threshold is None")  # pragma: no cover
-    if activation_is_signed is None:
-        Logger.error("activation_is_signed is None")  # pragma: no cover
+    if activation_threshold is None or activation_is_signed is None:
+        return None # pragma: no cover
     if not threshold_is_power_of_two(activation_threshold, per_channel=False):
-        Logger.error("Activation threshold is not power of two")  # pragma: no cover
+        return None # pragma: no cover
 
-    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
-                                                        activation_n_bits,
-                                                        activation_is_signed)
+    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
+                                                                             activation_n_bits,
+                                                                             activation_threshold)
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    return lambda x: q(x, min_value, max_value, scale)
 
 
 def symmetric_quantization(activation_n_bits: int,
                            quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a symmetric fake-quantization node.
@@ -94,24 +86,22 @@
 
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None:
-        Logger.error("Activation threshold is None")  # pragma: no cover
-    if activation_is_signed is None:
-        Logger.error("activation_is_signed is None")  # pragma: no cover
-
-    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
-                                                        activation_n_bits,
-                                                        activation_is_signed)
+    if activation_threshold is None or activation_is_signed is None:
+        return None # pragma: no cover
+
+    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
+                                                                             activation_n_bits,
+                                                                             activation_threshold)
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    return lambda x: q(x, min_value, max_value, scale)
 
 
 def uniform_quantization(activation_n_bits: int,
                          quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a uniform fake-quantization node.
@@ -119,38 +109,47 @@
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
 
     Returns:
         A fake quantization node.
     """
-    min_value, max_value = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
+    a, b = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
 
-    if min_value is None:
-        Logger.error("Min value is None")  # pragma: no cover
-    if max_value is None:
-        Logger.error("Max value is None")  # pragma: no cover
+    if a is None or b is None:
+        return None # pragma: no cover
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    # fixing quantization range to include 0
+    a = 0 if a > 0 else a
+    b = 0 if b < 0 else b
+    a, b = fix_range_to_include_zero(a, b, activation_n_bits)
 
+    min_value = 0
+    max_value = 2 ** activation_n_bits - 1
+    scale = (b - a) / ((2 ** activation_n_bits) - 1)
+    zero_point = -round(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
 
-def q(x: TFReference, min_value, max_value, activation_n_bits) -> TFReference:
-    """
-    Fake-quantize the input tensor x, using a tensorflow fake-quantization node.
+    return lambda x: q(x, min_value, max_value, scale, zero_point)
 
-    Args:
-        x: Input tensor to quantize.
-        min_value: quantization range lower bound.
-        max_value: quantization range upper bound.
-        activation_n_bits: Number of bits to use for quantization.
 
+def q(x: torch.Tensor,
+      min_value: int,
+      max_value: int,
+      scale: float,
+      zero_point: int = 0) -> torch.Tensor:
+    """
+    Fake-quantize the input tensor x, using a pytorch fake-quantization node.
+    Args:
+        x: input tensor to quantize.
+        min_value: lower bound of the quantized domain.
+        max_value: upper bound of the quantized domain.
+        scale: quantization scale.
+        zero_point: quantization zero_point
     Returns:
         The fake-quantized input tensor.
     """
-    if x.dtype != tf.float32:
-        x = tf.cast(x, dtype=tf.float32)  # pragma: no cover
 
-    # fake_quant_with_min_max_vars expects to get x of float32
-    return tf.quantization.fake_quant_with_min_max_vars(x,
-                                                        min=min_value,
-                                                        max=max_value,
-                                                        num_bits=activation_n_bits)
+    return torch.fake_quantize_per_tensor_affine(x,
+                                                 scale=scale,
+                                                 zero_point=zero_point,
+                                                 quant_min=min_value,
+                                                 quant_max=max_value)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from typing import Tuple, Dict, Callable
 
 import numpy as np
 import tensorflow as tf
 from keras.layers import Layer
 from tensorflow.python.util.object_identity import Reference as TFReference
 
-from model_compression_toolkit.constants import SIGNED, CLUSTER_CENTERS, EPS, \
-    MULTIPLIER_N_BITS, THRESHOLD
+from model_compression_toolkit.constants import SIGNED, LUT_VALUES, EPS, \
+    LUT_VALUES_BITWIDTH, THRESHOLD
 
 
 def activation_lut_kmean_quantizer(activation_n_bits: int,
                                    quantization_params: Dict[str, np.ndarray]) -> Callable:
     """
     Builds a LUT quantizer for layer's activation using the provided params (threshold and clusters).
     It initiates a fake custom LUT layer that provides the quantizer function.
@@ -25,22 +25,22 @@
 
     lut_fake_quant = LUTFakeQuant(quantization_params=quantization_params)
     return lambda x: lut_fake_quant(x)
 
 
 class LUTFakeQuant(Layer):
     """
-    A custom Keras layer for quantizing activation tensor with non-uniform quantization (using lookup table clustering).
+    A custom Keras layer for quantizing activation tensor with non-uniform quantization (using lookup table values).
     """
 
     def __init__(self, quantization_params: Dict[str, np.ndarray], **kwargs):
         super(LUTFakeQuant, self).__init__(**kwargs)
         self.quantization_params = quantization_params
         self.activation_is_signed = self.quantization_params.get(SIGNED)
-        self.cluster_centers = self.quantization_params.get(CLUSTER_CENTERS)
+        self.lut_values = self.quantization_params.get(LUT_VALUES)
         self.threshold = self.quantization_params.get(THRESHOLD)
 
     def build(self, input_shape: Tuple[int]):
         """
         Builds the layer.
 
         Args:
@@ -55,15 +55,15 @@
         Args:
             input_data: A Keras input tensor.
             **kwargs: Optional arguments' dictionary.
 
         Returns: KerasTensor after applying a non-uniform fake quantization.
 
         """
-        if self.activation_is_signed is None or self.cluster_centers is None or self.threshold is None:
+        if self.activation_is_signed is None or self.lut_values is None or self.threshold is None:
             return None  # pragma: no cover
 
         _quant_output = self.lut_kmeans_quantizer(input_data)
         return _quant_output
 
     def lut_kmeans_quantizer(self, tensor_data: TFReference) -> TFReference:
         """
@@ -75,22 +75,22 @@
 
         Args:
             tensor_data: Input activation tensor.
 
         Returns: Quantized tensor.
         """
 
-        tensor = self.int_quantization_with_threshold(tensor_data, MULTIPLIER_N_BITS)
+        tensor = self.int_quantization_with_threshold(tensor_data, LUT_VALUES_BITWIDTH)
         tensor = tf.expand_dims(tensor, -1)
 
-        expanded_cluster_centers = self.cluster_centers.reshape([*[1 for _ in range(len(tensor.shape)-1)], -1])
-        cluster_assignments = tf.argmin(tf.abs(tensor - expanded_cluster_centers), axis=-1)
-        centers = tf.gather(self.cluster_centers.flatten(), cluster_assignments)
+        expanded_lut_values = self.lut_values.reshape([*[1 for _ in range(len(tensor.shape)-1)], -1])
+        lut_values_assignments = tf.argmin(tf.abs(tensor - expanded_lut_values), axis=-1)
+        centers = tf.gather(self.lut_values.flatten(), lut_values_assignments)
 
-        quant_tensor = (centers / (2 ** (MULTIPLIER_N_BITS - int(self.activation_is_signed)))) * self.threshold
+        quant_tensor = (centers / (2 ** (LUT_VALUES_BITWIDTH - int(self.activation_is_signed)))) * self.threshold
 
         return quant_tensor
 
     def int_quantization_with_threshold(self,
                                         data: TFReference,
                                         n_bits: int,
                                         eps: float = EPS) -> TFReference:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,7 +8,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,162 +1,134 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from functools import partial
+from typing import Dict, Any, List
 
-import numpy as np
-import tensorflow as tf
-from tensorflow.python.framework.tensor_shape import TensorShape
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
-from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer
-from typing import Dict, Any, List, Callable
-
+from model_compression_toolkit.core.common.mixed_precision.configurable_quantizer_utils import \
+    verify_candidates_descending_order, init_quantized_weights
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
-from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeActivationQuantizationConfig
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from mct_quantizers import mark_quantizer
 
+import tensorflow as tf
+from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import \
+    ConfigurableQuantizerIdentifier
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
 
-class SelectiveActivationQuantizer(Quantizer):
+
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC,
+                                     QuantizationMethod.UNIFORM, QuantizationMethod.LUT_POT_QUANTIZER,
+                                     QuantizationMethod.LUT_SYM_QUANTIZER],
+                identifier=ConfigurableQuantizerIdentifier.CONFIGURABLE_ID)
+class ConfigurableWeightsQuantizer(BaseKerasInferableQuantizer):
     """
-    Quantizer that can use different quantized weights on-the-fly.
+    Configurable weights quantizer for Keras mixed precision search.
+    The quantizer holds a set of quantized layer's weights for each of the given bit-width candidates, provided by the
+    node's quantization config. This allows to use different quantized weights on-the-fly.
+
     The general idea behind this kind of quantizer is that it gets the float tensor to quantize
-    when initialize, it quantizes the float tensor in different bitwidths, and every time it need to return a
+    when initialized, it quantizes the float tensor in different bitwidths, and every time it need to return a
     quantized version of the float weight, it returns only one quantized weight according to an "active"
     index - the index of a candidate weight quantization configuration from a list of candidates that was passed
-    to the SelectiveWeightsQuantizer when it was initialized.
-    The "active" index can be configured as part of the SelectiveWeightsQuantizer's API, so a different quantized
-    weight can be returned in another time.
+    to the quantizer when it was initialized.
     """
 
     def __init__(self,
                  node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 max_candidate_idx: int):
+                 float_weights: tf.Tensor,
+                 kernel_attr: str,
+                 max_candidate_idx: int = 0):
         """
-        Init a selective quantizer.
+        Initializes a configurable quantizer.
 
         Args:
-            node_q_cfg: Quantization configuration candidate of the node that generated the layer that will
+            node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
                 use this quantizer.
+            float_weights: Float weights of the layer.
+            kernel_attr: The kernel attribute name of the node. Only layers with kernel op can be configured.
             max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
+
         """
+
+        super(ConfigurableWeightsQuantizer, self).__init__()
+
         self.node_q_cfg = node_q_cfg
-        self.active_quantization_config_index = max_candidate_idx  # initialize with first config as default
-        self.activation_quantizers = []
-        self._store_activation_quantizers()
+        self.float_weights = float_weights
+        self.max_candidate_idx = max_candidate_idx
+        self.kernel_attr = kernel_attr
+
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
+
+        for qc in self.node_q_cfg:
+            if qc.weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization != \
+                    self.node_q_cfg[0].weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization:
+                Logger.critical("Mixing candidates with varying weights quantization states (enabled/disabled) is not supported.")
+
+        # Initialize quantized weights for each weight that should be quantized.
+        self.quantized_weights = init_quantized_weights(node_q_cfg=self.node_q_cfg,
+                                                        float_weights=self.float_weights,
+                                                        fw_tensor_convert_func=partial(tf.convert_to_tensor,
+                                                                                       dtype=tf.float32),
+                                                        kernel_attr=self.kernel_attr)
 
-    def _get_qc_quantizer(self, index: int) -> NodeActivationQuantizationConfig:
+        self.active_quantization_config_index = self.max_candidate_idx
+
+    def set_weights_bit_width_index(self,
+                                    index: int):
         """
-        Quantize the quantizer float weight using a candidate quantization configuration.
+        Change the "active" bitwidth index the configurable quantizer uses, so a different quantized weight
+        will be used.
 
         Args:
-            index: Index of the candidate to use for the quantization.
+            index: Quantization configuration candidate index to use.
 
-        Returns:
-            Quantized weight.
         """
-        qc = self.node_q_cfg[index].activation_quantization_cfg
-        return qc
 
-    def _store_activation_quantizers(self):
-        """
-        Go over all candidates configurations, quantize the quantizer float weight according to each one
-        of them, and store the quantized weights in a list quantized_weights the quantizer holds.
-        """
-        for i in range(len(self.node_q_cfg)):
-            q_activation = self._get_qc_quantizer(i)
-            self.activation_quantizers.append(q_activation.quantize_node_output)
+        if index >= len(self.node_q_cfg):
+            Logger.critical(f'Quantizer supports only {len(self.node_q_cfg)} bit width configurations; index {index} is out of range.')# pragma: no cover
+        self.active_quantization_config_index = index
 
-    def build(self,
-              tensor_shape: TensorShape,
-              name: str,
-              layer: QuantizeWrapper) -> Dict[str, tf.Variable]:
-        """
-        The build method has to be implemented as part of the Keras framework,
-        but there is no need to use it here as we do not train any new variable.
-        Hence, it returns an empty dictionary.
+    def __call__(self,
+                 inputs: tf.Tensor) -> tf.Tensor:
         """
+        Method to return the quantized weight. This method is called when the framework needs to quantize a
+        float weight, and is expected to return the quantized weight. Since we already quantized the weight in
+        all possible bitwidths, we do not quantize it again, and simply return the quantized weight according
+        to the current active_quantization_config_index.
 
-        return {}
-
-    def __call__(self,
-                 inputs: tf.Tensor,
-                 training: bool,
-                 weights: Dict[str, tf.Variable],
-                 **kwargs: Dict[str, Any]) -> np.ndarray:
-        """
-        Method to return the quantized weight. This method is called
-        when the framework needs to quantize a float weight, and is expected to return the quantized
-        weight. Since we already quantized the weight in all possible bitwidths, we do not
-        quantize it again, and simply return the quantized weight according to the current
-        active_quantization_config_index.
+        Args:
+            inputs: Input tensor (not relevant since the weights are already quantized).
 
         Returns:
             Quantized weight, that was quantized using number of bits that is in a
             specific quantization configuration candidate (the candidate's index is the
             index that is in active_quantization_config_index the quantizer holds).
         """
-        return self.activation_quantizers[self.active_quantization_config_index](inputs)
-
-    def set_active_quantization_config_index(self, index: int):
-        """
-        Set an index to use for the quantized weight the quantizer returns
-        when requested.
 
-        Args:
-            index: Index of a candidate quantization configuration to use its quantized
-            version of the float weight.
-        """
-        assert index < len(
-            self.node_q_cfg), f'Quantizer has {len(self.node_q_cfg)} ' \
-                                      f'possible nbits. Can not set ' \
-                                      f'index {index}'
-        self.active_quantization_config_index = index
+        return self.quantized_weights[self.active_quantization_config_index]
 
     def get_config(self) -> Dict[str, Any]:  # pragma: no cover
         """
-        Returns: Configuration of TrainableQuantizer.
+        Returns: The ConfigurableWeightsQuantizer configuration.
         """
 
         return {
+            'float_weights': self.float_weights,
             'node_q_cfg': self.node_q_cfg,
-            'active_quantization_config_index': self.active_quantization_config_index,
-            'activation_quantizers': self.activation_quantizers
+            'active_quantization_config_index': self.active_quantization_config_index
         }
-
-    def __eq__(self, other: Any) -> bool:  # pragma: no cover
-        """
-        Check if equals to another object.
-
-        Args:
-            other: Other object to compare.
-
-        Returns:
-            Whether they are equal or not.
-        """
-        if not isinstance(other, SelectiveActivationQuantizer):
-            return False
-
-        return self.node_q_cfg == other.node_q_cfg and \
-               self.active_quantization_config_index == other.node_q_cfg and \
-               self.activation_quantizers == other.activation_quantizers
-
-    def __ne__(self, other: Any) -> bool:  # pragma: no cover
-        """
-        Check if not equals to another object.
-
-        Args:
-            other: Other object to compare.
-
-        Returns:
-            Whether they are differ or not.
-        """
-        return not self.__eq__(other)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,171 +9,173 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
+from typing import Dict, Any
+
 import numpy as np
 import tensorflow as tf
-from tensorflow.python.framework.tensor_shape import TensorShape
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
-from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer
-from typing import Dict, Any, List
-
-from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
-    CandidateNodeQuantizationConfig
-
 
-class SelectiveWeightsQuantizer(Quantizer):
-    """
-    Quantizer that can use different quantized weights on-the-fly.
-    The general idea behind this kind of quantizer is that it gets the float tensor to quantize
-    when initialize, it quantizes the float tensor in different bitwidths, and every time it need to return a
-    quantized version of the float weight, it returns only one quantized weight according to an "active"
-    index - the index of a candidate weight quantization configuration from a list of candidates that was passed
-    to the SelectiveWeightsQuantizer when it was initialized.
-    The "active" index can be configured as part of the SelectiveWeightsQuantizer's API, so a different quantized
-    weight can be returned in another time.
+from model_compression_toolkit.gptq import RoundingType
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from model_compression_toolkit.gptq.common.gptq_constants import AUXVAR, PTQ_THRESHOLD
+from model_compression_toolkit.gptq.keras.quantizer import quant_utils as qutils
+from model_compression_toolkit.constants import THRESHOLD
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
+    get_threshold_reshape_shape
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+
+
+def pertubation_symmetric_quantizer(input_tensor: tf.Tensor,
+                                    auxvar_tensor: tf.Variable,
+                                    max_tensor: tf.Tensor,
+                                    num_bits: int,
+                                    signed: bool,
+                                    power_of_two: bool,
+                                    max_lsbs_change: int = 1) -> tf.Tensor:
     """
+    Quantize a tensor symmetrically with maximum LSBs shift.
 
-    def __init__(self,
-                 node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 float_weight: np.ndarray,
-                 max_candidate_idx: int):
-        """
-        Init a selective quantizer.
+    Args:
+        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
+        auxvar_tensor: Tensor that manifests the bit shift the weight due to gptq
+        max_tensor: Tensor with max values to compute the threshold.
+        num_bits: Num of bits to use.
+        signed: Signedness of the quantization range.
+        power_of_two: Whether the threshold should be constrained or not.
+        max_lsbs_change: maximum number of LSBs that the auxvar is allowed to change
 
-        Args:
-            node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
-                use this quantizer.
-            float_weight: Float weights of the layer.
-            max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
-        """
-
-        self.node_q_cfg = node_q_cfg
-        self.quantizer_fn_list = [qc.weights_quantization_cfg.weights_quantization_fn for qc in self.node_q_cfg]
-        self.float_weight = float_weight
-        self.quantized_weights = []
-        self.active_quantization_config_index = max_candidate_idx
-        self._store_quantized_weights()
+    Returns:
+        A quantized tensor.
+    """
 
-    def _quantize_by_qc(self, index: int) -> np.ndarray:
-        """
-        Quantize the quantizer float weight using a candidate quantization configuration.
+    if power_of_two:
+        max_tensor = qutils.power_of_two_max(max_tensor)
+    delta = qutils.calculate_delta(max_tensor, num_bits, signed)
+    input_tensor_int = tf.stop_gradient(tf.round(input_tensor / delta))
+    tensor_q = qutils.ste_round(
+        input_tensor_int + qutils.ste_clip(auxvar_tensor, max_val=max_lsbs_change * delta) / delta)
+    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
+    max_int = (2 ** (num_bits - int(signed))) - 1
+    return delta * qutils.ste_clip(tensor_q, max_val=max_int, min_val=min_int)
 
-        Args:
-            index: Index of the candidate to use for the quantization.
 
-        Returns:
-            Quantized weight.
-        """
-        qc = self.node_q_cfg[index].weights_quantization_cfg
-        return self.quantizer_fn_list[index](self.float_weight,
-                                             qc.weights_n_bits,
-                                             True,
-                                             qc.weights_quantization_params,
-                                             qc.weights_per_channel_threshold,
-                                             qc.weights_channels_axis)
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
+                identifier=RoundingType.STE)
+class STEWeightGPTQQuantizer(BaseKerasGPTQTrainableQuantizer):
+    """
+    Trainable symmetric quantizer to quantize a layer weights.
+    """
 
-    def _store_quantized_weights(self):
+    def __init__(self,
+                 quantization_config: TrainableQuantizerWeightsConfig,
+                 max_lsbs_change_map: dict = DefaultDict(default_value=1)):
         """
+        Initialize a STEWeightGPTQQuantizer object with parameters to use for the quantization.
 
-        Go over all candidates configurations, quantize the quantizer float weight according to each one
-        of them, and store the quantized weights in a list quantized_weights the quantizer holds.
-
+        Args:
+            quantization_config: Trainable weights quantizer config.
+            max_lsbs_change_map: a mapping between number of bits to max lsb change.
         """
-        for i in range(len(self.node_q_cfg)):
-            q_weight = self._quantize_by_qc(i)
-            self.quantized_weights.append(tf.Variable(q_weight,
-                                                      trainable=False,
-                                                      dtype=tf.float32))
-
-    def build(self,
-              tensor_shape: TensorShape,
-              name: str,
-              layer: QuantizeWrapper) -> Dict[str, tf.Variable]:
+        super().__init__(quantization_config)
+        self.num_bits = quantization_config.weights_n_bits
+        self.per_channel = quantization_config.weights_per_channel_threshold
+
+        threshold_values = quantization_config.weights_quantization_params[THRESHOLD]
+        self.threshold_shape = np.asarray(threshold_values).shape
+        self.threshold_values = np.reshape(np.asarray(threshold_values), [-1]) if self.per_channel else float(
+            threshold_values)
+
+        self.quantization_axis = quantization_config.weights_channels_axis
+        self.power_of_two = quantization_config.weights_quantization_method == QuantizationMethod.POWER_OF_TWO
+        self.max_lsbs_change = max_lsbs_change_map.get(self.num_bits)
+
+    def initialize_quantization(self,
+                                tensor_shape: Any,
+                                name: str,
+                                layer: Any):
         """
-        The build method has to be implemented as part of the Keras framework,
-        but there is no need to use it here as we do not train any new variable.
-        Hence, it returns an empty dictionary.
+        Add quantizer parameters to the quantizer parameters dictionary
 
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+
+        ptq_threshold_tensor = layer.add_weight(
+            f"{name}_{PTQ_THRESHOLD}",
+            shape=len(self.threshold_values) if self.per_channel else (),
+            initializer=tf.keras.initializers.Constant(1.0),
+            trainable=False)
+        ptq_threshold_tensor.assign(self.threshold_values)
+
+        w = getattr(layer.layer, name)
+        auxvar_tensor = layer.add_weight(
+            f"{name}_{AUXVAR}",
+            shape=list(w.shape),
+            initializer=tf.keras.initializers.Constant(0.0),
+            trainable=True)
+
+        # save the quantizer added parameters for later calculations
+        self.add_quantizer_variable(PTQ_THRESHOLD, ptq_threshold_tensor, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(AUXVAR, auxvar_tensor, VariableGroup.WEIGHTS)
+
+    def __call__(self,
+                 inputs: tf.Tensor,
+                 training: bool):
         """
+        Quantize a tensor.
 
-        return {}
-
-    def __call__(self, inputs: tf.Tensor,
-                 training: bool,
-                 weights: Dict[str, tf.Variable],
-                 **kwargs: Dict[str, Any]) -> np.ndarray:
-        """
-        Method to return the quantized weight. This method is called
-        when the framework needs to quantize a float weight, and is expected to return the quantized
-        weight. Since we already quantized the weight in all possible bitwidths, we do not
-        quantize it again, and simply return the quantized weight according to the current
-        active_quantization_config_index.
+        Args:
+            inputs: Input tensor to quantize.
+            training: Whether the graph is in training mode.
 
         Returns:
-            Quantized weight, that was quantized using number of bits that is in a
-            specific quantization configuration candidate (the candidate's index is the
-            index that is in active_quantization_config_index the quantizer holds).
+            The quantized tensor.
         """
 
-        return self.quantized_weights[self.active_quantization_config_index]
-
-    def set_active_quantization_config_index(self, index: int):
-        """
-        Set an index to use for the quantized weight the quantizer returns
-        when requested.
-
-        Args:
-            index: Index of a candidate quantization configuration to use its quantized
-            version of the float weight.
-
-        """
-        assert index < len(
-            self.node_q_cfg), f'Quantizer has {len(self.node_q_cfg)} ' \
-                                      f'possible nbits. Can not set ' \
-                                      f'index {index}'
-        self.active_quantization_config_index = index
+        auxvar = self.get_quantizer_variable(AUXVAR)
+        ptq_threshold_tensor = self.get_quantizer_variable(PTQ_THRESHOLD)
 
-    def get_config(self) -> Dict[str, Any]:  # pragma: no cover
-        """
-        Returns: Configuration of TrainableQuantizer.
-        """
+        if self.per_channel:
+            reshape_shape = get_threshold_reshape_shape(inputs.shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=-1)
+            ptq_threshold_tensor = tf.reshape(ptq_threshold_tensor, reshape_shape)
+            q_tensor = pertubation_symmetric_quantizer(inputs,
+                                                       auxvar,
+                                                       ptq_threshold_tensor,
+                                                       self.num_bits,
+                                                       signed=True,
+                                                       power_of_two=self.power_of_two,
+                                                       max_lsbs_change=self.max_lsbs_change)
+            return q_tensor
+        else:
+            return pertubation_symmetric_quantizer(inputs,
+                                                   auxvar,
+                                                   ptq_threshold_tensor,
+                                                   self.num_bits,
+                                                   signed=True,
+                                                   power_of_two=self.power_of_two)
 
-        return {
-            'node_q_cfg': self.node_q_cfg,
-            'float_weight': self.float_weight,
-            'quantizer_fn_list': self.quantizer_fn_list,
-            'quantized_weights': self.quantized_weights,
-            'active_quantization_config_index': self.active_quantization_config_index
-        }
 
-    def __eq__(self, other: Any) -> bool:  # pragma: no cover
+    def get_quant_config(self) -> Dict[str, np.ndarray]:
         """
-        Check if equals to another object.
-        Args:
-            other: Other object to compare.
+        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
 
         Returns:
-            Whether they are equal or not.
-        """
-        if not isinstance(other, SelectiveWeightsQuantizer):
-            return False
-
-        return (self.node_q_cfg == other.node_q_cfg and
-                self.float_weight == other.float_weight and
-                self.quantizer_fn_list == other.quantizer_fn_list and
-                self.self.quantized_weights == other.self.quantized_weights and
-                self.active_quantization_config_index == other.active_quantization_config_index)
+            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
+            Keys must match NodeQuantizationConfig attributes
 
-    def __ne__(self, other: Any) -> bool:  # pragma: no cover
-        """
-        Check if not equals to another object.
-        Args:
-            other: Other object to compare.
-
-        Returns:
-            Whether they are differ or not.
         """
-        return not self.__eq__(other)
+        old_threshold = self.get_quantizer_variable(PTQ_THRESHOLD)
+        return {THRESHOLD: old_threshold.numpy().reshape(self.threshold_shape)}
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/common.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/common.py`

 * *Files 19% similar despite different names*

```diff
@@ -13,20 +13,20 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import tensorflow as tf
 from packaging import version
 
-# As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.engine.node import Node as KerasNode
-    from tensorflow.keras.layers import InputLayer
-    from tensorflow.python.keras.engine.functional import Functional
-    from tensorflow.python.keras.engine.sequential import Sequential
+
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.input_layer import InputLayer
+    from keras.src.engine.node import Node as KerasNode
+    from keras.src.engine.functional import Functional
+    from keras.src.engine.sequential import Sequential
 else:
     from keras.engine.input_layer import InputLayer
     from keras.engine.node import Node as KerasNode
     from keras.engine.functional import Functional
     from keras.engine.sequential import Sequential
 
 from model_compression_toolkit.logger import Logger
@@ -43,15 +43,15 @@
         Whether the node represents an input layer or not.
     """
     if isinstance(node, BaseNode):
         return node.type == InputLayer
     elif isinstance(node, KerasNode):
         return isinstance(node.layer, InputLayer)
     else:
-        Logger.error('Node to check has to be either a graph node or a keras node')  # pragma: no cover
+        Logger.critical('Node must be a graph node or a Keras node for input layer check.')  # pragma: no cover
 
 
 def is_node_a_model(node: BaseNode) -> bool:
     """
     Checks if a node represents a Keras model.
     Args:
         node: Node to check if its a Keras model by itself.
@@ -60,9 +60,9 @@
         Whether the node represents a Keras model or not.
     """
     if isinstance(node, BaseNode):
         return node.type in [Functional, Sequential]
     elif isinstance(node, KerasNode):
         return isinstance(node.layer, Functional) or isinstance(node.layer, Sequential)
     else:
-        Logger.error('Node to check has to be either a graph node or a keras node')  # pragma: no cover
+        Logger.critical('Node must be a graph node or a Keras node.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/connectivity_handler.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,17 +13,16 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import tensorflow as tf
 from packaging import version
 
-# As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.engine.node import Node as KerasNode
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.node import Node as KerasNode
 else:
     from keras.engine.node import Node as KerasNode
 
 
 from tensorflow.python.util.object_identity import Reference as TFReference
 from typing import List, Tuple
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/node_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,132 +1,124 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Any
+from typing import Tuple
+import numpy as np
+import torch
+from torch.nn import Conv2d
+import torch.nn.functional as F
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy
+from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing
+from model_compression_toolkit.core.pytorch.constants import KERNEL, KERNEL_SIZE, STRIDES, DILATIONS, BIAS, USE_BIAS, FILTERS, PADDING, GROUPS
+from model_compression_toolkit.logger import Logger
 
-import tensorflow as tf
-from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.layers.core import TFOpLambda, SlicingOpLambda
-    from tensorflow.python.keras.engine.keras_tensor import KerasTensor
-    from tensorflow.python.keras.engine.node import Node as KerasNode
-else:
-    from keras.layers.core import TFOpLambda, SlicingOpLambda
-    from keras.engine.keras_tensor import KerasTensor
-    from keras.engine.node import Node as KerasNode
-
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
-
-keras = tf.keras
-layers = keras.layers
-
-REUSED_IDENTIFIER = '_reused_'
+def linear_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
+    """
+    Function generates matchers for matching:
+    (Conv2D, Conv2D)[activation=linear] -> Conv2D.
+    Returns:
+        Matcher for 2 consecutive linear convolution
+    """
+    first_node = NodeOperationMatcher(Conv2d)
+    second_node = NodeOperationMatcher(Conv2d)
+    return first_node, second_node
 
 
-def build_node(node: KerasNode,
-               node_name_to_node: dict) -> BaseNode:
+def conv2d_collapsing_fn(first_node: BaseNode,
+                         second_node: BaseNode,
+                         kernel_str: str,
+                         bias_str: str) -> Tuple[np.ndarray, np.ndarray]:
     """
-    Build a node from a Keras node. A node contains all information to reconstruct the layer it's representing
-    in a model:
-    operation, layer configuration, path for instantiating the Keras layer the node has, weights, group of other
-    nodes if it's a reused layer,
-    input/output shape.
+    Collapsing 2 convolutions to one convolution: Out = k2*(k1*x+b1)+b2 = k2*k1*x+k2*b1+b2 = k*x+b
+    We calculate k=k2*k1 (collapsed kernel) by injecting identity tensor to the convolutions and extract the output
+    We calculate b=k2*b1+b2 (collapsed bias) matrix multiplication
     Args:
-        node: Node in the graph of a Keras model.
-        node_name_to_node: Dictionary of already created nodes aims to identify reused layers.
-
+        first_node: First layer node to collapse to second layer node
+        second_node: Second layer node
+        kernel_str: The framework specific attribute name of the convolution layer's weight/kernel.
+        bias_str: The framework specific attribute name of the convolution layer's bias.
     Returns:
-        Graph node that was built from the Keras node.
+        The modified layer node's weights: kernel, bias
     """
-    keras_layer = node.layer  # get the layer the node represents.
-    layer_config = keras_layer.get_config()  # layer configuration to reconstruct it.
-    op_call_args = node.call_args
-    op_call_kwargs = node.call_kwargs
-    layer_class = type(keras_layer)  # class path to instantiating it in back2framework.
-    weights = {v.name: v.numpy() for v in keras_layer.weights}  # layer's weights
-
-    # If it's a node representing a reused layer, several nodes will contain the same layer instance.
-    # Thus, the first time a node of a reused layer is being created, it's being build as a node of a non-reused layer,
-    # while other nodes of this layer will be created as nodes of a reused layer with the suffix "_reused_i"
-    # where i is the input/output index of the layer.
-    is_reused = keras_layer.name in node_name_to_node
-    if is_reused:
-        # Mark the "base" node with its reused group.
-        node_name_to_node[keras_layer.name].reuse_group = keras_layer.name
-        io_index = 1
-        while keras_layer.name + REUSED_IDENTIFIER + str(io_index) in node_name_to_node:  # find next unused io index
-            io_index = io_index + 1
-        reuse_group = keras_layer.name  # by the layer name we can gather nodes of this reused layer
-        node_name = keras_layer.name + REUSED_IDENTIFIER + str(io_index)
-    else:
-        io_index = 0  # for non reused layers input/output index is 0
-        reuse_group = None
-        node_name = keras_layer.name
-    input_shape = keras_layer.get_input_shape_at(io_index)
-    output_shape = keras_layer.get_output_shape_at(io_index)
-
-    if layer_class in [TFOpLambda, SlicingOpLambda]:
-        # Some functional ops (such as tf.concat) should receive the input tensors as a list
-        # and some are not (such as tf.multiply), so each FunctionalNode holds
-        # a flag to indicate that.
-        inputs_as_list = __is_functional_inputs_a_list(op_call_args)
-        # Do not hold the tensors that are in op_call_args as they are
-        # not needed. Thus, if the first element in op_call_args is a list of
-        # Keras tensors, remove it from op_call_args.
-        op_call_args = op_call_args[int(inputs_as_list):]
-        node = FunctionalNode(node_name,
-                              layer_config,
-                              input_shape,
-                              output_shape,
-                              weights,
-                              layer_class,
-                              [arg for arg in op_call_args if not isinstance(arg, KerasTensor)],  # Do not hold the tensors that are in op_call_args
-                              {k: v for k, v in op_call_kwargs.items() if not isinstance(v, KerasTensor)}, # In TF2.5 tensors are in kwargs as well.
-                              is_reused,
-                              reuse_group,
-                              functional_op=keras_layer.function,
-                              inputs_as_list=inputs_as_list)
-    else:
-        node = BaseNode(node_name,
-                        layer_config,
-                        input_shape,
-                        output_shape,
-                        weights,
-                        layer_class,
-                        is_reused,
-                        reuse_group)
+    if first_node.type == Conv2d and second_node.type == Conv2d:
+        # Get nodes attributes
+        kernel1 = first_node.get_weights_by_keys(kernel_str)
+        kernel2 = second_node.get_weights_by_keys(kernel_str)
+        bias1 = first_node.get_weights_by_keys(bias_str)
+        bias2 = second_node.get_weights_by_keys(bias_str)
+        strides1 = first_node.framework_attr[STRIDES]
+        strides2 = second_node.framework_attr[STRIDES]
+
+        # --------------------------------------- #
+        # Kernel collapsing: k=k2*k1
+        # --------------------------------------- #
+        # Inspired by https://arxiv.org/pdf/2103.09404.pdf - Algorithm1
+
+        # Generate identity input with padding
+        kx, ky = kernel1.shape[2] + kernel2.shape[2] - 1, kernel1.shape[3] + kernel2.shape[3] - 1
+        x_pad, y_pad = 2 * kx - 1, 2 * ky - 1
+        in_tensor = torch.eye(kernel1.shape[1])
+        in_tensor = torch.unsqueeze(torch.unsqueeze(in_tensor, 2), 3)
+        in_tensor = F.pad(in_tensor, (int(np.ceil((x_pad - 1) / 2)),
+                                      int(np.floor((x_pad - 1) / 2)),
+                                      int(np.ceil((y_pad - 1) / 2)),
+                                      int(np.floor((y_pad - 1) / 2))))
+
+        # Run first Conv2D
+        conv1_out = F.conv2d(input=to_torch_tensor(in_tensor), weight=to_torch_tensor(kernel1), stride=strides1, padding=(0,0))
+
+        # Run second Conv2D
+        kernel2_torch = to_torch_tensor(kernel2)
+        conv2_out = F.conv2d(input=conv1_out, weight=kernel2_torch, stride=strides2)
+
+        # Extract collapsed kernel from output: the collapsed kernel is the output of the convolution after fixing the dimension
+        kernel_collapsed = torch_tensor_to_numpy(torch.permute(torch.flip(conv2_out,[3,2]), dims=[1,0,2,3]))
+
+        # --------------------------------------- #
+        # Bias collapsing: b=k2*b1+b2
+        # --------------------------------------- #
+        bias_collapsed = None
+        if bias1 is not None:
+            bias1_torch = to_torch_tensor(bias1)
+            bias_collapsed = torch_tensor_to_numpy(torch.matmul(torch.sum(kernel2_torch,dim=(2, 3)), bias1_torch))
+            if bias2 is not None:
+                bias_collapsed += bias2
+        elif bias2 is not None:
+            bias_collapsed = bias2
 
-    node_name_to_node[node_name] = node
-    return node
+        return kernel_collapsed, bias_collapsed
+    else:
+        Logger.critical(f"Layer collapsing is not supported for the combination of {first_node.type} and {second_node.type}.")
 
 
-def __is_functional_inputs_a_list(op_call_args: Any) -> bool:
+def pytorch_linear_collapsing() -> Conv2DCollapsing:
     """
-    Check whether the input tensors should be passed as a list
-    or not.
-
-    Args:
-        op_call_args: Arguments list to check.
-
     Returns:
-        Whether the input tensors should be passed as a list or not.
+        A Conv2DCollapsing initialized for pytorch models.
     """
-
-    if len(op_call_args) > 0 and isinstance(op_call_args[0], list):
-        inputs_as_list = True
-        for arg in op_call_args[0]:
-            inputs_as_list = inputs_as_list and isinstance(arg, KerasTensor)
-        return inputs_as_list
-    return False
+    first_node, second_node = linear_collapsing_node_matchers()
+    return Conv2DCollapsing(first_node,
+                            second_node,
+                            conv2d_collapsing_fn,
+                            KERNEL,
+                            KERNEL_SIZE,
+                            BIAS,
+                            USE_BIAS,
+                            STRIDES,
+                            PADDING,
+                            DILATIONS,
+                            GROUPS,
+                            FILTERS)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/reader/reader.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/reader/reader.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/statistics_correction/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/tf_tensor_numpy.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,17 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Union, List, Tuple
 import tensorflow as tf
 import numpy as np
 
+from model_compression_toolkit.logger import Logger
+
 
 def to_tf_tensor(tensor):
     """
     Convert a Numpy array to a TF tensor.
     Args:
         tensor: Numpy array.
 
@@ -30,29 +33,38 @@
     elif isinstance(tensor, list):
         return [to_tf_tensor(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return (to_tf_tensor(t) for t in tensor)
     elif isinstance(tensor, np.ndarray):
         return tf.convert_to_tensor(tensor.astype(np.float32))
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(tf.Tensor)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to TF tensor: {type(tensor)}.')
 
 
-def tf_tensor_to_numpy(tensor: tf.Tensor) -> np.ndarray:
+def tf_tensor_to_numpy(tensor: Union[List, Tuple, np.ndarray, tf.Tensor],
+                       is_single_tensor=False) -> np.ndarray:
     """
     Convert a TF tensor to a Numpy array.
     Args:
         tensor: TF tensor.
+        is_single_tensor: whether input is a value to be converted to a single tensor.
+                          if False, recurse the lists and tuples
 
     Returns:
         Numpy array converted from the input tensor.
     """
     if isinstance(tensor, np.ndarray):
         return tensor
     elif isinstance(tensor, list):
-        return [tf_tensor_to_numpy(t) for t in tensor]
+        if is_single_tensor:
+            return np.array(tensor)
+        else:
+            return [tf_tensor_to_numpy(t) for t in tensor]
     elif isinstance(tensor, tuple):
-        return (tf_tensor_to_numpy(t) for t in tensor)
+        if is_single_tensor:
+            return np.array(tensor)
+        else:
+            return (tf_tensor_to_numpy(t) for t in tensor)
     elif isinstance(tensor, tf.Tensor):
         return tensor.numpy()
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(np.ndarray)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Numpy array: {type(tensor)}.')
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/keras/visualization/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/importance_metrics/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,13 +34,13 @@
         mode: Mode of the PyTorch model builder.
 
     Returns:
         PyTorch model builder for the given mode.
     """
 
     if not isinstance(mode, ModelBuilderMode):
-        Logger.error(f'get_pytorch_model_builder expects a mode of type ModelBuilderMode, but {type(mode)} was passed.')
+        Logger.critical(f"Expected a ModelBuilderMode type for 'mode' parameter; received {type(mode)} instead.")
     if mode is None:
-        Logger.error(f'get_pytorch_model_builder received a mode which is None')
+        Logger.critical(f"Received 'mode' parameter is None.")
     if mode not in pytorch_model_builders.keys():
-        Logger.error(f'mode {mode} is not in pytorch model builders factory')
+        Logger.critical(f"'mode' parameter {mode} is not supported by the PyTorch model builders factory.")
     return pytorch_model_builders.get(mode)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 
     Returns:
         Pytorch module that was built from the node.
     """
 
     framework_attr = copy.copy(n.framework_attr)
     node_instance = n.type(**framework_attr)
-    node_instance.load_state_dict({k: torch.Tensor(v) for k, v in n.weights.items()}, strict=False)
+    node_instance.load_state_dict({k: torch.tensor(v) for k, v in n.weights.items()}, strict=False)
     set_model(node_instance)
     return node_instance
 
 
 # todo: remove. It is not used anymore
 def identity_wrapper(node: BaseNode,
                      module: Module,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,102 +9,68 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import List, Any, Tuple
+from typing import List, Tuple
 
 import torch
 
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder
 from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder, \
     PytorchModel
-
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-from model_compression_toolkit.core.pytorch.mixed_precision.mixed_precision_wrapper import PytorchMixedPrecisionWrapper
 
 
-class MixedPrecisionPyTorchModel(PytorchModel):
+class QuantizedPyTorchModel(PytorchModel):
+    """
+    Quantized PyTorch model.
+    """
 
     def __init__(self,
                  graph: common.Graph,
                  append2output=None):
         """
 
         Args:
             graph: Graph to build its corresponding Pytorch model.
             append2output: List of nodes or OutTensor objects.
         """
 
         super().__init__(graph,
                          append2output)
 
-
-    def _add_modules(self):
-        configurable_nodes = self.graph.get_configurable_sorted_nodes()
-        for n in self.node_sort:
-            if n in configurable_nodes:
-                self.add_module(n.name, PytorchMixedPrecisionWrapper(n,
-                                                                     DEFAULT_PYTORCH_INFO))
-            else:
-                if not isinstance(n, FunctionalNode):
-                    self.add_module(n.name, node_builder(n))
-
     def _quantize_node_activations(self,
                                    node: BaseNode,
                                    input_tensors: List[torch.Tensor]) -> List[torch.Tensor]:
         """
         Quantize node's activation given input tensors.
 
         Args:
             node: Node to quantize its outputs.
             input_tensors: Input tensors of the node.
 
         Returns:
             Output of the node.
 
         """
-        if node.is_all_activation_candidates_equal():
-            # otherwise, we want to use the float tensor when building the model for MP search
-            input_tensors = node.candidates_quantization_cfg[0].activation_quantization_cfg.quantize_node_output(input_tensors)
+        if node.is_activation_quantization_enabled():
+            if isinstance(input_tensors, list):
+                input_tensors = torch.cat(input_tensors, dim=0)
+            return node.final_activation_quantization_cfg.quantize_node_output(input_tensors)
         return input_tensors
 
 
-    def _get_op_func(self,
-                     node: BaseNode,
-                     configurable_nodes_names: List[str]) -> Any:
-        """
-        Gets the operation function that runs the actual inference of the nodes compatible layer.
-
-        Args:
-            node: The corresponding node of the layer it runs.
-            configurable_nodes_names: A list of names of configurable nodes in the quantized model.
-
-        Returns: Module/functional to apply to the input tensors.
-
-        """
-        if node.name in configurable_nodes_names:
-            return getattr(self, node.name)
-        else:
-            return node.type if isinstance(node, FunctionalNode) else getattr(self, node.name)
-
+class QuantizedPyTorchModelBuilder(PyTorchModelBuilder):
 
-
-
-class MixedPrecisionPyTorchModelBuilder(PyTorchModelBuilder):
-    """
-    Mixed-precision PyTorch model.
-    """
     def __init__(self,
                  graph: common.Graph,
                  append2output=None,
                  fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
                  return_float_outputs: bool = False):
         """
 
@@ -118,13 +84,13 @@
         super().__init__(graph,
                          append2output,
                          fw_info,
                          return_float_outputs)
 
     def build_model(self) -> Tuple[PytorchModel, UserInformation]:
         """
-        Build a PyTorch float model and return it.
-        Returns: Float PyTorch model and user information.
+        Build a PyTorch quantized model and return it.
+        Returns: Quantized PyTorch model and user information.
 
         """
-        return MixedPrecisionPyTorchModel(self.graph,
-                                          self.append2output), self.graph.user_info
+        return QuantizedPyTorchModel(self.graph,
+                                     self.append2output), self.graph.user_info
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,28 +13,29 @@
 # limitations under the License.
 # ==============================================================================
 from abc import abstractmethod
 from functools import partial
 from typing import Tuple, Any, Dict, List, Union, Callable
 
 import torch
+import numpy as np
 from networkx import topological_sort
 
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.back2framework.base_model_builder import BaseModelBuilder
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder, identity_wrapper
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, BufferHolder
-from model_compression_toolkit.core.pytorch.utils import get_working_device
-from model_compression_toolkit.core.pytorch.constants import BUFFER
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
+from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from mct_quantizers.common.constants import ACTIVATION_HOLDER_QUANTIZER
 
 
 def _build_input_tensors_list(node: BaseNode,
                               graph: Graph,
                               inputs: Tuple[Any],
                               node_to_output_tensors_dict: Dict[BaseNode, List]) -> List[List]:
@@ -57,17 +58,44 @@
         input_tensors = []
         # Go over a sorted list of the node's incoming edges, and for each source node get its output tensors.
         # Append them in a result list.
         for ie in graph.incoming_edges(node, sort_by_attr=EDGE_SINK_INDEX):
             _input_tensors = node_to_output_tensors_dict[ie.source_node]
             input_tensors.append(_input_tensors)
         input_tensors = [tensor for tensor_list in input_tensors for tensor in tensor_list]  # flat list of lists
+        input_tensors = node.insert_positional_weights_to_input_list(input_tensors)
+        # convert inputs from positional weights (numpy arrays) to tensors. Must handle each element in the
+        # list separately, because in FX the tensors are FX objects and fail to_torch_tensor
+        input_tensors = [to_torch_tensor(t) if isinstance(t, np.ndarray) else t
+                         for t in input_tensors]
     return input_tensors
 
 
+def _merge_inputs(_node, input_tensors: List, op_call_args: List) -> List:
+    """
+    Merge input tensors list with op_call_args, according to correct order
+
+    Args:
+        _node: The node the inputs are for
+        input_tensors: activation input tensors to node.
+        op_call_args: framework node call args
+    Returns:
+        Combined list of input_tensors and op_call_args
+    """
+    if isinstance(_node, FunctionalNode) and _node.tensor_input_indices:
+        assert len(_node.tensor_input_indices) == len(input_tensors), 'Mismatch between input tensors and indices'
+        _input_list = op_call_args.copy()
+        for i, t in zip(_node.tensor_input_indices, input_tensors):
+            _input_list.insert(i, t)
+    else:
+        _input_list = input_tensors + op_call_args
+
+    return _input_list
+
+
 def _run_operation(n: BaseNode,
                    input_tensors: List,
                    op_func: Any,
                    quantize_node_activation_fn,
                    use_activation_quantization: bool) -> Tuple[Union[List, torch.Tensor], Union[List, torch.Tensor]]:
     """
     Applying the layer (op_func) to the input tensors (input_tensors).
@@ -86,15 +114,15 @@
     """
 
     op_call_args = n.op_call_args if isinstance(n, FunctionalNode) else []
     functional_kwargs = n.op_call_kwargs if isinstance(n, FunctionalNode) else {}
     if isinstance(n, FunctionalNode) and n.inputs_as_list:
         out_tensors_of_n_float = op_func(input_tensors, *op_call_args, **functional_kwargs)
     else:
-        out_tensors_of_n_float = op_func(*input_tensors + op_call_args, **functional_kwargs)
+        out_tensors_of_n_float = op_func(*_merge_inputs(n, input_tensors, op_call_args), **functional_kwargs)
 
     # Add a fake quant node if the node has an activation threshold.
     out_tensors_of_n = out_tensors_of_n_float
     if use_activation_quantization:
         if isinstance(out_tensors_of_n_float, list):
             out_tensors_of_n_float = torch.cat(out_tensors_of_n_float, dim=0)
         out_tensors_of_n = quantize_node_activation_fn(out_tensors_of_n_float)
@@ -209,15 +237,15 @@
         """
         if isinstance(node, FunctionalNode):
             if self.wrapper is None:
                 node_op = node.type
             else:
                 node_op = self.wrapper(node, node.type)
         else:
-            if self.wrapper is None or node.type == BufferHolder:
+            if self.wrapper is None:
                 node_op = node_builder(node)
             else:
                 node_op = self.wrapper(node, node_builder(node))
         return node_op
 
     def _add_modules(self):
         """
@@ -226,18 +254,14 @@
         for node in self.node_sort:
             node_op = self.wrap(node)
             if isinstance(node, FunctionalNode):
                 # for functional layers
                 setattr(self, node.name, node_op)
             else:
                 self.add_module(node.name, node_op)
-                if node.type == BufferHolder:
-                    self.get_submodule(node.name). \
-                        register_buffer(node.name,
-                                        torch.Tensor(node.get_weights_by_keys(BUFFER)).to(get_working_device()))
 
             # Add activation quantization modules if an activation holder is configured for this node
             if node.is_activation_quantization_enabled() and self.get_activation_quantizer_holder is not None:
                 activation_quantizer_holder = self.get_activation_quantizer_holder(node)
                 if activation_quantizer_holder is not None:
                     self.add_module(node.name + '_' + ACTIVATION_HOLDER_QUANTIZER, activation_quantizer_holder)
                     self.node_to_activation_quantization_holder.update(
@@ -249,15 +273,15 @@
         Args:
             args: argument input tensors to model.
         Returns:
             torch Tensor/s which is/are the output of the model logic.
         """
         node_to_output_tensors_dict = dict()
         node_to_output_tensors_dict_float = dict()
-        configurable_nodes = self.graph.get_configurable_sorted_nodes_names()
+        configurable_nodes = self.graph.get_configurable_sorted_nodes_names(DEFAULT_PYTORCH_INFO)
         for node in self.node_sort:
             input_tensors = _build_input_tensors_list(node,
                                                       self.graph,
                                                       args,
                                                       node_to_output_tensors_dict)
 
             op_func = self._get_op_func(node, configurable_nodes)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/constants.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/constants.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,16 +13,14 @@
 # limitations under the License.
 # ==============================================================================
 
 
 # # Layer type constants:
 PLACEHOLDER = 'placeholder'
 OUTPUT = 'output'
-CONSTANT = 'constant'
-BUFFER = 'buffer'
 
 # # Module operation type
 CALL_FUNCTION = 'call_function'
 CALL_METHOD = 'call_method'
 GET_ATTR = 'get_attr'
 
 # # Layers attributes constants:
@@ -38,33 +36,39 @@
 TYPE = 'type'
 PAD = 'pad'
 VALUE = 'value'
 FUNCTIONAL_OP = 'functional_op'
 OP_CALL_ARGS = 'op_call_args'
 OP_CALL_KWARGS = 'op_call_kwargs'
 INPUTS_AS_LIST = 'inputs_as_list'
+TENSOR_INPUT_INDICES = 'tensor_input_indices'
 INPLACE = 'inplace'
 HARDTANH_MIN_VAL = 'min_val'
 HARDTANH_MAX_VAL = 'max_val'
 
 # # Layers variables names:
 KERNEL = 'weight'
 BIAS = 'bias'
 GAMMA = 'weight'
 BETA = 'bias'
+WEIGHT = 'weight'
 MOVING_MEAN = 'running_mean'
 MOVING_VARIANCE = 'running_var'
 EPSILON = 'eps'
 EPSILON_VAL = 1e-5
 MOMENTUM = 'momentum'
 MOMENTUM_VAL = 0.1
+NORMALIZED_SHAPE = 'normalized_shape'
 DIM = 'dim'
 IN_CHANNELS = 'in_channels'
 OUT_CHANNELS = 'out_channels'
 NUM_FEATURES = 'num_features'
+NUM_PARAMETERS = 'num_parameters'
+IN_FEATURES = 'in_features'
+OUT_FEATURES = 'out_features'
 
 # torch devices
 CUDA = 'cuda'
 CPU = 'cpu'
 
 # ReLU bound constants
 RELU_POT_BOUND = 8.0
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/default_framework_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/default_framework_info.py`

 * *Files 9% similar despite different names*

```diff
@@ -13,50 +13,50 @@
 # limitations under the License.
 # ==============================================================================
 from torch.nn import Hardsigmoid, ReLU, ReLU6, Softmax, Sigmoid
 from torch.nn.functional import hardsigmoid, relu, relu6, softmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch import sigmoid
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.pytorch.constants import KERNEL
 from model_compression_toolkit.core.pytorch.quantizer.fake_quant_builder import power_of_two_quantization, \
     symmetric_quantization, uniform_quantization
 from model_compression_toolkit.core.pytorch.quantizer.lut_fake_quant import activation_lut_kmean_quantizer
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
 If a layer that is not listed here is queried, [None] is returned.
 """
 KERNEL_ATTRIBUTES = DefaultDict({Conv2d: [KERNEL],
                                  ConvTranspose2d: [KERNEL],
                                  Linear: [KERNEL]},
-                                lambda: [None])
+                                [None])
 
 """
 Map a layer to its kernel's output and input channels indices.
 Map's values are tuples of (output_channel_index, input_channel_index).
 Default value is returned for layers that are not included.
 """
 DEFAULT_CHANNEL_AXIS_DICT = DefaultDict({Conv2d: (0, 1),
                                          Linear: (0, 1),
                                          ConvTranspose2d: (1, 0)},
-                                        lambda: (None, None))
+                                        (None, None))
 
 """
 Map a layer to its output channel axis.
 Where axis=-1 is the last axis
 """
 DEFAULT_OUT_CHANNEL_AXIS_DICT = DefaultDict({Conv2d: 1,
                                              Linear: -1,
                                              ConvTranspose2d: 1},
-                                            lambda: 1)
+                                            1)
 
 
 """
 Map from an activation function to its min/max output values (if known).
 The values are used for tensor min/max values initialization.
 """
 ACTIVATION2MINMAX = {}  # should be an empty dict in Pytorch
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,66 +8,40 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import numpy as np
 from torch.nn import BatchNorm2d, Conv2d, ConvTranspose2d
 
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.substitutions.batchnorm_folding import BatchNormalizationFolding
+from model_compression_toolkit.core.common.substitutions.batchnorm_refusing import BatchNormalizationRefusing
 from model_compression_toolkit.core.pytorch.constants import KERNEL, BIAS, GAMMA, BETA, MOVING_MEAN, MOVING_VARIANCE, \
     EPSILON, USE_BIAS
+from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_folding import \
+    update_kernel_for_bn_folding_fn
 
 
-def batchnorm_folding_node_matchers():
+def batchnorm_refusing_node_matchers():
     """
     Function generates matchers for matching:
     (Conv2d, ConvTranspose2d)-> BatchNorm2d.
 
     Returns:
         Matcher for batch norm nodes, and source nodes.
     """
     bn_node = NodeOperationMatcher(BatchNorm2d)
     source_node = NodeOperationMatcher(Conv2d) | \
                   NodeOperationMatcher(ConvTranspose2d)
     return bn_node, source_node
 
 
-def update_kernel_for_bn_folding_fn(conv_node: BaseNode,
-                                    kernel: np.ndarray,
-                                    weights_scale: np.ndarray):
+def pytorch_batchnorm_refusing() -> BatchNormalizationRefusing:
     """
-    Args:
-        conv_node: Convolution node to update the weight/kernel.
-        kernel: The Convolution node's weight
-        weights_scale: Weight scale factor in which to multiply the conv node's weight.
-
-    Returns:
-        The modified convolution node's weight/kernel/
-    """
-    return kernel * weights_scale[:, None, None, None], KERNEL
-
-
-def pytorch_batchnorm_folding() -> BatchNormalizationFolding:
-    """
-
+    Re-fuse BatchNormalization into preceding linear layers.
     Returns:
-        A BatchNormalizationFolding initialized for Pytorch models.
+        A BatchNormalizationRefusing initialized for Pytorch models.
     """
-    bn_node, source_node = batchnorm_folding_node_matchers()
-    return BatchNormalizationFolding(source_node,
-                                      bn_node,
-                                      update_kernel_for_bn_folding_fn,
-                                      KERNEL,
-                                      BIAS,
-                                      GAMMA,
-                                      BETA,
-                                      MOVING_MEAN,
-                                      MOVING_VARIANCE,
-                                      EPSILON,
-                                      USE_BIAS,
-                                      layer_name_str=None,  # torch.nn.Modules don't have an attribute 'name'
-                                      )
+    bn_node, source_node = batchnorm_refusing_node_matchers()
+    return BatchNormalizationRefusing(source_node, bn_node, update_kernel_for_bn_folding_fn, KERNEL, BIAS, GAMMA, BETA,
+                                      MOVING_MEAN, MOVING_VARIANCE, EPSILON, USE_BIAS, layer_name_str=None)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,27 +43,27 @@
         Args:
             mha_node: MHA node
         """
 
         # Only batch first network is supported
         if BATCH_FIRST in mha_node.framework_attr.keys():
             if mha_node.framework_attr[BATCH_FIRST] is not True:
-                Logger.error('Only batch first network is supported')  # pragma: no cover
+                Logger.critical('Only networks with batch first cofiguration are supported.')  # pragma: no cover
         else:
-            Logger.error('Only batch first network is supported')  # pragma: no cover
+            Logger.critical('Only networks with batch first cofiguration are supported.')  # pragma: no cover
 
         # Add Zero Attn feature is Not Implemented
         if ADD_ZERO_ATTN in mha_node.framework_attr.keys():
             if mha_node.framework_attr[ADD_ZERO_ATTN] is not False:
-                Logger.error('Add Zero Attn feature is Not Implemented')  # pragma: no cover
+                Logger.critical('Add Zero Attention (Add Zero Attn) feature is not implemented.')  # pragma: no cover
 
         # Check if Add Bias KV feature is Active
         if BIAS_K and BIAS_V in mha_node.weights.keys():
             if mha_node.weights[BIAS_K] is not None and mha_node.weights[BIAS_V] is not None:
-                Logger.error('Add BIAS_KV feature is Not Implemented')  # pragma: no cover
+                Logger.critical('Add Bias to Key/Value (BIAS_KV) feature is not implemented.')  # pragma: no cover
 
         self.embed_dim = mha_node.framework_attr[EMBED_DIM]
         self.num_heads = mha_node.framework_attr[NUM_HEADS]
 
         self.kdim = mha_node.framework_attr[KEY_DIM]
 
         self.vdim = mha_node.framework_attr[VALUE_DIM]
@@ -698,15 +698,15 @@
             graph: input graph
             mha_node: MHA node to substitute inputs and outputs with
         Returns:
             Graph after applying the substitution.
         """
 
         if mha_node.reuse:
-            raise Exception("MCT doesn't support reuse of MultiHeadAttention layer")  # pragma: no cover
+            Logger.critical("Reuse of MultiHeadAttention layers is currently not supported.")  # pragma: no cover
         params = MHAParams(mha_node)
 
         # project
         # (B, q_seq, q_dim*n_h) --> (B, q_dim*n_h, q_seq)
         # (B, kv_seq, k_dim) --> (B, q_dim*n_h, kv_seq)
         # (B, kv_seq, v_dim) --> (B, q_dim*n_h, kv_seq)
         q_transpose_node, k_transpose_node, v_transpose_node, \
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files 2% similar despite different names*

```diff
@@ -99,15 +99,15 @@
                     (np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]).astype(int) -
                      np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]) == 0):
                 scale_factor = non_linear_node.framework_attr[HARDTANH_MAX_VAL] / self.threshold
                 non_linear_node.functional_op.__defaults__ = (0.0, self.threshold, non_linear_node.framework_attr[INPLACE])
             else:
                 return graph
         else:
-            Logger.error(f"In substitution with wrong matched pattern")
+            Logger.critical(f"Encountered an unexpected non-linearity type not supported for this substitution: {non_linear_node.type}.")
         Logger.debug(
             f"Node named:{non_linear_node.name} changed "
             f"to:{non_linear_node.type}")
 
         w2_fixed = scale_factor * second_op2d_node.get_weights_by_keys(KERNEL)
         w1_fixed = first_op2d_node.get_weights_by_keys(KERNEL) / scale_factor
         b1_fixed = first_op2d_node.get_weights_by_keys(BIAS) / scale_factor
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py`

 * *Files 15% similar despite different names*

```diff
@@ -35,34 +35,45 @@
         nodes = NodeOperationMatcher(reshape) | NodeOperationMatcher(torch.Tensor.view)
         super().__init__(matcher_instance=nodes)
 
     def substitute(self,
                    graph: Graph,
                    node: BaseNode) -> Graph:
         """
-        Replaces the 'size' attribute to 'reshape' or 'view' operators to be a list of integers,
+        Replaces the 'size' attribute for 'reshape' or 'view' operators to be a list of integers,
         determined by their intended output shape. This replaces 'size' attributes that come from
         nodes in the graph. We delete nodes for which that was their sole purpose.
 
         Args:
             graph: Graph we apply the substitution on.
             node: node that match the pattern in the substitution init.
 
         Returns:
             Graph after applying the substitution.
         """
+        # skip substitution if the reshape is applied to weights (not activations). In this case, the node is the first source and doesn't have an input shape.
+        if len(node.input_shape) == 0:
+            return graph
+
         # we want the batch size value to infer from the length of the array and remaining dimensions
         if len(node.output_shape) == 1:
             node.output_shape[0][0] = BATCH_DIM_VALUE
         else:
-            Logger.error('Reshape or view nodes should have a single output shape')  # pragma: no cover
+            Logger.critical("This substitution handles 'reshape' or 'view' nodes with a single output shape.")  # pragma: no cover
 
         # configure the new static output shape attribute
         node.op_call_args = node.output_shape
 
+        # When a "reshape" is called with multiple arguments (e.g. x.reshape(-1, channels, height, width)
+        # this substitution converts it x.reshape((-1, channels, height, width)), so need to update the
+        # tensor_input_indices attribute.
+        # scalar argument's shape is [1] so remove those indices from tensor_input_indices
+        # node.input_shape example: [[1, 32, 4, 32], [1], [1], [1]]
+        node.tensor_input_indices = node.tensor_input_indices[:sum([i != [1] for i in node.input_shape])]
+
         # modify the node input info
         node.input_shape = [node.input_shape[0]]
 
         # the first input is the tensor to be reshaped, we want his batch size value to infer
         # from the length of the array and remaining dimensions
         node.input_shape[0][0] = BATCH_DIM_VALUE
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 3% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         # Collapsing residual by adding "1" to kernel diagonal
         idxH = (kH - 1) // 2
         idxW = (kW - 1) // 2
         for i in range(Cout):
             kernel[i, i, idxH, idxW] += 1
         return kernel
     else:
-        Logger.error("No supported add residual collapsing for {}".format(first_node.type))
+        Logger.critical(f"Residual collapsing not supported for node type: {first_node.type}")
 
 
 def pytorch_residual_collapsing() -> ResidualCollapsing:
     """
     Returns:
         A ResidualCollapsing initialized for pytorch models.
     """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,41 +9,60 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import operator
-from typing import Tuple, Any
+from typing import Tuple, Any, Callable
 
+import numpy as np
 import torch.nn.functional
 from torch.nn import Conv2d, Linear, PReLU, ELU, Hardswish, Dropout, ZeroPad2d, SiLU
 from torch import reshape
 from torch.nn.functional import hardswish, silu, prelu, elu
 from torch.nn.functional import avg_pool2d
 
 from model_compression_toolkit.core import CoreConfig, FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import EdgeMatcher
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common.substitutions.shift_negative_activation import apply_shift_negative_correction
 from model_compression_toolkit.core.pytorch.constants import PAD, VALUE, PADDING, BIAS, USE_BIAS
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 
 """
 This substitution aims to solve an issue of activation with negative outputs where
 the portion of the negative range is relatively small. In a symmetric quantization this causes 
 of bit loosing as the entire negative quantization range does not contain
 any values. To solve it, we shift the output of the activation by the minimal output value (quantized) such
 that all values after the shifting are positive. To correct the impact of such shifting, a correction
 to the next linear node is computed and added to its bias term.
 If the linear node pads the input tensor with zeros, we modify the padded value as well.  
 """
 
 
+def params_search_quantization_fn(quantization_fw:Callable,
+                                  bins_to_quantize:np.ndarray) -> np.ndarray:
+    """
+    This function receives a quantization function and the bins of a histogram
+    which is a numpy array. The function preprocessd postprocess the bins tensor according
+    to the quantization_fw expected input/output.
+
+    Args:
+        quantization_fw: Quantization fn to use to quantize the bins.
+        bins_to_quantize: Bins to quantize.
+
+    Returns:
+        Quantized np array bins.
+
+    """
+    return quantization_fw(to_torch_tensor(bins_to_quantize)).cpu().numpy()
+
 def shift_negative_activation_node_matchers():
     # Match activation nodes with negative outputs.
     snc_node = NodeOperationMatcher(PReLU) | \
                NodeOperationMatcher(prelu) | \
                NodeOperationMatcher(ELU) | \
                NodeOperationMatcher(elu) | \
                NodeOperationMatcher(Hardswish) | \
@@ -215,9 +234,10 @@
                                            pad_node,
                                            create_add_node,
                                            get_padding_values,
                                            create_pad_node,
                                            is_padding_node_and_node_has_padding,
                                            PADDING,
                                            BIAS,
-                                           USE_BIAS
+                                           USE_BIAS,
+                                           params_search_quantization_fn=params_search_quantization_fn
                                            )
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/kpi_data_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/keras/quantization_facade.py`

 * *Files 15% similar despite different names*

```diff
@@ -11,152 +11,150 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core.analyzer import analyzer_model_quantization
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.constants import PYTORCH
-from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
+from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.constants import FOUND_TORCH
-
-if FOUND_TORCH:
-    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.core.exporter import export_model
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+
+if FOUND_TF:
+    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
+    from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
+    from tensorflow.keras.models import Model
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from torch.nn import Module
+    from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
 
     from model_compression_toolkit import get_target_platform_capabilities
-
-    PYTORCH_DEFAULT_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
+    DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
-    def pytorch_kpi_data(in_model: Module,
-                         representative_data_gen: Callable,
-                         quant_config: MixedPrecisionQuantizationConfig = DEFAULT_MIXEDPRECISION_CONFIG,
-                         fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                         target_platform_capabilities: TargetPlatformCapabilities = PYTORCH_DEFAULT_TPC) -> KPI:
+    def keras_post_training_quantization(in_model: Model,
+                                         representative_data_gen: Callable,
+                                         target_resource_utilization: ResourceUtilization = None,
+                                         core_config: CoreConfig = CoreConfig(),
+                                         target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC):
         """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
+         Quantize a trained Keras model using post-training quantization. The model is quantized using a
+         symmetric constraint quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
+         (both coefficients and activations by default).
+         In order to limit the maximal model's size, a target ResourceUtilization need to be passed after weights_memory
+         is set (in bytes).
+
+         Args:
+             in_model (Model): Keras model to quantize.
+             representative_data_gen (Callable): Dataset used for calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+
+         Returns:
+
+             A quantized model and information the user may need to handle the quantized model.
 
-        Args:
-            in_model (Model): PyTorch model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+         Examples:
 
-        Returns:
-            A KPI object with total weights parameters sum, max activation tensor and total kpi.
+            Import MCT:
 
-        Examples:
+            >>> import model_compression_toolkit as mct
 
-            Import a Pytorch model:
+            Import a Keras model:
 
-            >>> from torchvision import models
-            >>> module = models.mobilenet_v2()
+            >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
+            >>> model = MobileNetV2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
 
             >>> import numpy as np
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 3, 224, 224))]
+            >>>         yield [np.random.random((4, 224, 224, 3))]
 
-            Import mct and call for KPI data calculation:
+            Create a MCT core config, containing the quantization configuration:
 
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
+            >>> config = mct.core.CoreConfig()
 
-        """
+            If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+            The candidates bitwidth for quantization should be defined in the target platform model.
+            In this example we use 1 image to search mixed-precision configuration:
 
-        if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
-
-        fw_impl = PytorchImplementation()
-
-        quantization_config, mp_config = quant_config.separate_configs()
-        core_config = CoreConfig(quantization_config=quantization_config,
-                                 mixed_precision_config=mp_config)
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
-
-
-    def pytorch_kpi_data_experimental(in_model: Module,
-                                      representative_data_gen: Callable,
-                                      core_config: CoreConfig = CoreConfig(),
-                                      fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                                      target_platform_capabilities: TargetPlatformCapabilities = PYTORCH_DEFAULT_TPC) -> KPI:
-        """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=1))
 
-        Args:
-            in_model (Model): PyTorch model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
+            For mixed-precision set a target ResourceUtilization object:
+            Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
+            that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
+            while the bias will not):
 
-        Returns:
+            >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
-            A KPI object with total weights parameters sum and max activation tensor.
+            Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+            quantized model:
 
-        Examples:
+            >>> quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(model, repr_datagen, ru, core_config=config)
 
-            Import a Pytorch model:
+            For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
 
-            >>> from torchvision import models
-            >>> module = models.mobilenet_v2()
+         """
 
-            Create a random dataset generator:
+        fw_info = DEFAULT_KERAS_INFO
 
-            >>> import numpy as np
-            >>> def repr_datagen(): yield [np.random.random((1, 3, 224, 224))]
+        KerasModelValidation(model=in_model,
+                             fw_info=fw_info).validate()
 
-            Import mct and call for KPI data calculation:
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                                    "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization "
+                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
 
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
+        tb_w = init_tensorboard_writer(fw_info)
 
-        """
+        fw_impl = KerasImplementation()
 
-        if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
-
-        fw_impl = PytorchImplementation()
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
+        # Ignore returned hessian service as PTQ does not use it
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=fw_info,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
 
-else:
-    # If torch is not installed,
-    # we raise an exception when trying to use this function.
-    def pytorch_kpi_data(*args, **kwargs):
-        Logger.critical('Installing torch is mandatory when using pytorch_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
 
+        if core_config.debug_config.analyze_similarity:
+            analyzer_model_quantization(representative_data_gen,
+                                        tb_w, tg,
+                                        fw_impl,
+                                        fw_info)
 
-    def pytorch_kpi_data_experimental(*args, **kwargs):
-        Logger.critical('Installing torch is mandatory when using pytorch_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        return get_exportable_keras_model(tg)
+
+
+
+else:
+    # If tensorflow is not installed,
+    # we raise an exception when trying to use these functions.
+    def keras_post_training_quantization(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_post_training_quantization. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/lsq/uniform_lsq.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,212 +1,223 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Union
+import numpy as np
+import torch
+import torch.nn as nn
 
-from typing import Any, List
+from model_compression_toolkit.constants import RANGE_MAX, RANGE_MIN
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
+
+from model_compression_toolkit.qat import TrainingMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
+from model_compression_toolkit import constants as C
+
+from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.qat.pytorch.quantizer.quantizer_utils import ste_round, grad_scale
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
+from mct_quantizers.pytorch.quantizers import \
+    WeightsUniformInferableQuantizer, ActivationUniformInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
+    TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+from model_compression_toolkit.qat.pytorch.quantizer.quantizer_utils import adjust_range_to_include_zero
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
+
+
+
+def uniform_lsq_quantizer(x: nn.Parameter,
+                      min_range: nn.Parameter,
+                      max_range: nn.Parameter,
+                      num_bits: int,
+                      min_int: int,
+                      max_int: int,
+                      scale_factor: float) -> Union[nn.Parameter, torch.Tensor]:
+    """
+    Uniform quantizer according to LSQ algorithm: https://arxiv.org/pdf/1902.08153.pdf
+    Args:
+        x: input to quantize
+        min_range: min range of quantization values
+        max_range: min range of quantization values
+        num_bits: number of bits for quantization
+        min_int: min clipping integer value
+        max_int: max clipping integer value
+        scale_factor: grad scale of LSQ algorithm
+    Returns:
+        A quantized tensor
+    """
+    a, b = adjust_range_to_include_zero(min_range, max_range, num_bits)
+    delta = (b - a) / (2 ** num_bits - 1)
+    delta_scaled = grad_scale(delta, scale_factor)
+    rounded = ste_round((x - a) / delta_scaled)
+    clipped = torch.clip(rounded, min=min_int, max=max_int)
+    quantized = delta_scaled * clipped + a
+    return quantized
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.UNIFORM],
+                identifier=TrainingMethod.LSQ)
+class LSQUniformWeightQATQuantizer(BasePytorchQATTrainableQuantizer):
+    """
+    Trainable constrained quantizer to quantize layer's weights.
+    """
+
+    def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
+        """
+        Initialize a LSQUniformWeightQATQuantizer object with parameters to use
+        for the quantization.
+
+        Args:
+            quantization_config: trainable quantizer config class
+        """
+        super().__init__(quantization_config)
+        self.num_bits = self.quantization_config.weights_n_bits
+        self.min_int = 0
+        self.max_int = 2 ** self.num_bits - 1
+        self.min_values = np.array(quantization_config.weights_quantization_params[RANGE_MIN])
+        self.max_values = np.array(quantization_config.weights_quantization_params[RANGE_MAX])
+        self.scale_factor = 1.0 / np.sqrt(self.max_int * self.min_values.size)
+
+    def initialize_quantization(self,
+                                tensor_shape: torch.Size,
+                                name: str,
+                                layer: PytorchQuantizationWrapper):
+        """
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+
+        # Add min and max variables to layer.
+        layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_values), requires_grad=True))
+        layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_values), requires_grad=True))
+
+        # Save the quantizer parameters for later calculations
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
 
-import torch
-import copy
 
-from model_compression_toolkit.core import FrameworkInfo
-from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
-from model_compression_toolkit.core.pytorch.utils import set_model, to_torch_tensor
-
-
-class PytorchMixedPrecisionWrapper(torch.nn.Module):
-    """
-    Class that wraps a Pytorch layer (nn.Module) to be used for mixed precision quantization.
-    Allows to maintain quantized weights tensors for each of the layer's attributes that we want to quantize,
-    and a list of activation quantizers for each quantization candidate,
-    for each of the candidate bitwidth options specified for the mixed precision model.
-    During MP search, it allows to activate the relevant quantized weights tensor and activation quantizer
-    according to a given configuration, and use it for inference.
-    """
-    def __init__(self,
-                 n: BaseNode,
-                 fw_info: FrameworkInfo):
-        """
-        Construct a Pytorch model that constitutes as a wrapper for a Pytorch layer, built from a given graph node.
-        Args:
-            n: Node to build its Pytorch layer.
-            fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
-        """
-        super(PytorchMixedPrecisionWrapper, self).__init__()
-
-        assert n.candidates_quantization_cfg is not None
-        self.node_q_cfg = n.candidates_quantization_cfg
-        if isinstance(n, FunctionalNode):
-            self.layer = n.type
-        else:
-            framework_attr = copy.copy(n.framework_attr)
-            self.layer = n.type(**framework_attr)
-
-        for qc in self.node_q_cfg:
-            assert qc.weights_quantization_cfg.enable_weights_quantization == \
-                   self.node_q_cfg[0].weights_quantization_cfg.enable_weights_quantization \
-                   and qc.activation_quantization_cfg.enable_activation_quantization == \
-                   self.node_q_cfg[0].activation_quantization_cfg.enable_activation_quantization, \
-                "Candidates with different weights/activation enabled properties is currently not supported"
-
-        self.enable_weights_quantization = \
-            self.node_q_cfg[0].weights_quantization_cfg.enable_weights_quantization and \
-            not n.is_all_weights_candidates_equal()
-        self.enable_activation_quantization = \
-            self.node_q_cfg[0].activation_quantization_cfg.enable_activation_quantization and \
-            not n.is_all_activation_candidates_equal()
-
-        max_cfg_candidates = n.find_max_candidates_indices()
-        assert len(max_cfg_candidates) == 1, \
-            f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
-        max_candidate_idx = max_cfg_candidates[0]
-
-        if not isinstance(n, FunctionalNode):
-            # loading the weights (if exists) from the graph node (weights of the trained model)
-            self.layer.load_state_dict({k: torch.Tensor(v) for k, v in n.weights.items()}, strict=False)
-            set_model(self.layer)
-
-        # Setting layers' weights
-        if self.enable_weights_quantization:
-            self.weight_attrs = fw_info.get_kernel_op_attributes(n.type)
-            # float_weights is a list of weights for each attribute that we want to quantize.
-            self.float_weights = [n.get_weights_by_keys(attr) for attr in
-                                  self.weight_attrs]
-
-            assert len(self.weight_attrs) == len(self.float_weights)
-            self.weights_quantizer_fn_list = [qc.weights_quantization_cfg.weights_quantization_fn
-                                              for qc in self.node_q_cfg]
-            self.quantized_weights = self._get_quantized_weights()
-            # Setting the model with the initial quantized weights (the highest precision)
-            self.set_active_weights(bitwidth_idx=max_candidate_idx)
-
-        # Setting layer's activation
-        if self.enable_activation_quantization:
-            self.activation_quantizers = self._get_activation_quantizers()
-            self.activation_bitwidth_idx = max_candidate_idx
+    def __call__(self,
+                 inputs: nn.Parameter,
+                 training: bool) -> torch.Tensor:
+        """
+        Quantize a tensor
+        Args:
+            inputs: Input tensor to quantize.
+            training: whether in training mode or not
+        Returns:
+            quantized tensor
+        """
+        min_range = self.get_quantizer_variable(FQ_MIN)
+        max_range = self.get_quantizer_variable(FQ_MAX)
+        weight_quantized = uniform_lsq_quantizer(inputs, min_range, max_range, self.num_bits, self.min_int, self.max_int, self.scale_factor)
+        return weight_quantized
+
+    def convert2inferable(self) -> WeightsUniformInferableQuantizer:
+        """
+        Convert quantizer to inferable quantizer.
 
-    def forward(self, x: Any, *args: Any, **kwargs: Any) -> Any:
+        Returns:
+            A pytorch inferable quanizer object.
+        """
+        min_range = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        max_range = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
+        min_range, max_range = fix_range_to_include_zero(min_range, max_range, self.num_bits)
+        return WeightsUniformInferableQuantizer(num_bits=self.num_bits,
+                                                min_range=min_range.tolist(),
+                                                max_range=max_range.tolist(),
+                                                per_channel=self.quantization_config.weights_per_channel_threshold,
+                                                channel_axis=self.quantization_config.weights_channels_axis)
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
+                quantization_method=[QuantizationMethod.UNIFORM],
+                identifier=TrainingMethod.LSQ)
+class LSQUniformActivationQATQuantizer(BasePytorchQATTrainableQuantizer):
+    """
+    Trainable constrained quantizer to quantize layer activations.
+    """
+
+    def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
+        Initialize a LSQUniformActivationQATQuantizer object with parameters to use
+        for uniform quantization.
+
         Args:
-            x: input tensors to layer.
+            quantization_config: trainable quantizer config class
+        """
+        super().__init__(quantization_config)
+        self.num_bits = self.quantization_config.activation_n_bits
+        self.min_int = 0
+        self.max_int = 2 ** self.num_bits - 1
+        self.min_range = np.array([quantization_config.activation_quantization_params[C.RANGE_MIN]])
+        self.max_range = np.array([quantization_config.activation_quantization_params[C.RANGE_MAX]])
+
+    def initialize_quantization(self,
+                                tensor_shape: torch.Size,
+                                name: str,
+                                layer: PytorchQuantizationWrapper):
+        """
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+        layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_range), requires_grad=True))
+        layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_range), requires_grad=True))
+
+        # Save the quantizer parameters for later calculations
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
+
+    def __call__(self,
+                 inputs: torch.Tensor,
+                 training: bool = True) -> torch.Tensor:
+        """
+        Quantize a tensor.
+        Args:
+            inputs: Input tensor to quantize.
+            training: Whether the graph is in training mode.
+
         Returns:
-            torch Tensor which is the output of the wrapped layer on the given input.
+            The quantized tensor.
+        """
+        min_range = self.get_quantizer_variable(FQ_MIN)
+        max_range = self.get_quantizer_variable(FQ_MAX)
+        n_channels = inputs.shape[1]
+        scale_factor = 1.0 / np.sqrt(self.max_int * n_channels)
+        inputs_quantized = uniform_lsq_quantizer(inputs, min_range, max_range, self.num_bits, self.min_int, self.max_int, scale_factor)
+        return inputs_quantized
+
+    def convert2inferable(self) -> ActivationUniformInferableQuantizer:
         """
-        outputs = self.layer(x, *args, **kwargs)
+        Convert quantizer to inferable quantizer.
 
-        if self.enable_activation_quantization:
-            # add fake quant to quantize activations with the active number of bits
-            if isinstance(outputs, list):
-                # we assume here that it can't be multiple outputs out of a quantized layer.
-                # We are not expecting to get here since we call this wrapper layer op only through the
-                # sensitivity evaluation inference pytorch method which unfolds the input list to separate arguments.
-                # But, in order to prevent possible issues (if someone will use this wrapper otherwise,
-                # we keep this line which handles an input as list, and exclude it from the test coverage report.
-                assert len(outputs) == 1, "Activation quantization for node with multiple outputs is not supported."  # pragma: no cover
-                outputs = torch.cat(outputs, dim=0)  # pragma: no cover
-
-            outputs = self.activation_quantizers[self.activation_bitwidth_idx](outputs)
-
-        return outputs
-
-    def _get_quantized_weights(self):
-        """
-        Calculates the quantized weights' tensors for each of the bitwidth candidates for quantization,
-        to be stored and used during MP search.
-        Returns: a list of quantized weights - for each bitwidth and layer's attribute to be quantized.
-        """
-        quantized_weights = []
-        for index, qc in enumerate(self.node_q_cfg):
-            # for each quantization configuration in mixed precision
-            # get quantized weights for each attribute and for each filter
-            quantized_per_attr = []
-            for float_weight in self.float_weights:
-                # for each attribute
-                quantized_per_attr.append(self.weights_quantizer_fn_list[index](tensor_data=float_weight,
-                                                                                n_bits=qc.weights_quantization_cfg.weights_n_bits,
-                                                                                signed=True,
-                                                                                quantization_params=qc.weights_quantization_cfg.weights_quantization_params,
-                                                                                per_channel=qc.weights_quantization_cfg.weights_per_channel_threshold,
-                                                                                output_channels_axis=qc.weights_quantization_cfg.weights_channels_axis))
-            quantized_weights.append(quantized_per_attr)
-
-        return quantized_weights
-
-    def _get_activation_quantizers(self) -> List[Any]:
-        """
-        Builds a list of quantizers for each of the bitwidth candidates for activation quantization,
-        to be stored and used during MP search.
-
-        Returns: a list of activation quantizers - for each bitwidth and layer's attribute to be quantized.
-        """
-        activation_quantizers = []
-        for index, qc in enumerate(self.node_q_cfg):
-            q_activation = self.node_q_cfg[index].activation_quantization_cfg
-            activation_quantizers.append(q_activation.quantize_node_output)
-
-        return activation_quantizers
-
-    def set_active_weights(self,
-                           bitwidth_idx: int,
-                           attr: str = None):
-        """
-        Set a weights' tensor to use by the layer wrapped by the module.
-        Args:
-            bitwidth_idx: Index of a candidate quantization configuration to use its quantized
-            version of the float weight.
-            attr: Attributes of the layer's weights to quantize
-        """
-        if self.enable_weights_quantization:
-            if attr is None:  # set bit width to all weights of the layer
-                attr_idxs = [attr_idx for attr_idx in range(len(self.quantized_weights[bitwidth_idx]))]
-                self._set_weights_bit_width_index(bitwidth_idx, attr_idxs)
-            else:  # set bit width to a specific attribute
-                attr_idx = self.weight_attrs.index(attr)
-                self._set_weights_bit_width_index(bitwidth_idx, [attr_idx])
-
-    def set_active_activation_quantizer(self,
-                                        bitwidth_idx: int):
-        """
-        Set an activation quantizer to use by the layer wrapped by the module.
-
-        Args:
-            bitwidth_idx: Index of a candidate quantization configuration to use its quantizer
-            for quantizing the activation.
-        """
-        if self.enable_activation_quantization:
-            self.activation_bitwidth_idx = bitwidth_idx
-
-    def _set_weights_bit_width_index(self,
-                                     bitwidth_idx: int,
-                                     attr_idxs: List[int]):
-        """
-        Sets the wrapped layer's weights state with quantized weights, according to the given configuration.
-        Args:
-            bitwidth_idx: Index of a candidate quantization configuration to use its quantized
-            version of the float weight.
-            attr_idxs: Indices list of attributes of the layer's weights to quantize
-        Returns: None (sets the new state of the layer inplace).
-        """
-        assert bitwidth_idx < len(self.quantized_weights), \
-            f"Index {bitwidth_idx} does not exist in current quantization candidates list"
-
-        loaded_weights = {k: torch.as_tensor(v) for k, v in self.layer.state_dict().items()}
-        with torch.no_grad():
-            for attr_idx in attr_idxs:
-                # need to prepare the weights' tensor - extract it from the maintained quantized_weights list
-                # and move it to the relevant device as the wrapped layer's weights.
-                weights_tensor = self.quantized_weights[bitwidth_idx][attr_idx]
-                weights_device = loaded_weights[self.weight_attrs[attr_idx]].device
-                active_weights = torch.nn.Parameter(torch.from_numpy(weights_tensor).to(weights_device))
-                loaded_weights[self.weight_attrs[attr_idx]] = active_weights
-            self.layer.load_state_dict(loaded_weights, strict=True)
+        Returns:
+            A pytorch inferable quanizer object.
+        """
+        min_range = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        max_range = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
+        min_range, max_range = fix_range_to_include_zero(min_range, max_range, self.num_bits)
+        return ActivationUniformInferableQuantizer(num_bits=self.num_bits,
+                                                   min_range=min_range.tolist(),
+                                                   max_range=max_range.tolist())
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/pytorch_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,53 +10,56 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import operator
 from copy import deepcopy
+from functools import partial
 from typing import List, Any, Tuple, Callable, Type, Dict
 
 import numpy as np
 import torch
+from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
 from torch import sigmoid, softmax, add, cat, argmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch.nn import Module, Sigmoid, Softmax
 
 import model_compression_toolkit.core.pytorch.constants as pytorch_constants
-from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
-from model_compression_toolkit.core.common.collectors.statistics_collector_generator import \
-    create_stats_collector_for_node
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, HessianInfoService
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
+from model_compression_toolkit.core.common.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.similarity_analyzer import compute_mse, compute_kl_divergence, compute_cs
-from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework import get_pytorch_model_builder
-from model_compression_toolkit.core.pytorch.back2framework.model_gradients import \
-    pytorch_iterative_approx_jacobian_trace
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_folding import \
-    pytorch_batchnorm_folding
+    pytorch_batchnorm_folding, pytorch_batchnorm_forward_folding
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_reconstruction import \
     pytorch_batchnorm_reconstruction
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_refusing import \
     pytorch_batchnorm_refusing
+from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.functional_batch_norm import \
+    FunctionalBatchNorm
+from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.functional_layer_norm import \
+    FunctionalLayerNorm
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.linear_collapsing import \
     pytorch_linear_collapsing
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.multi_head_attention_decomposition \
     import MultiHeadAttentionDecomposition
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.permute_call_method import \
     PermuteCallMethod
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.const_holder_conv import \
-    ConstantHolderConv
+    FunctionalConvSubstitution
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.relu_bound_to_power_of_2 import \
     ReLUBoundToPowerOfTwo
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.reshape_with_static_shapes import \
     ReshapeWithStaticShapes
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.residual_collapsing import \
     pytorch_residual_collapsing
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.scale_equalization import \
@@ -66,25 +69,37 @@
     pytorch_apply_shift_negative_correction
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.softmax_shift import \
     pytorch_softmax_shift
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.virtual_activation_weights_composition import \
     VirtualActivationWeightsComposition
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.weights_activation_split import \
     WeightsActivationSplit
-from model_compression_toolkit.core.pytorch.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
+from model_compression_toolkit.core.pytorch.hessian.activation_trace_hessian_calculator_pytorch import \
+    ActivationTraceHessianCalculatorPytorch
+from model_compression_toolkit.core.pytorch.hessian.weights_trace_hessian_calculator_pytorch import \
+    WeightsTraceHessianCalculatorPytorch
+from model_compression_toolkit.core.pytorch.mixed_precision.configurable_activation_quantizer import \
+    ConfigurableActivationQuantizer
+from model_compression_toolkit.core.pytorch.mixed_precision.configurable_weights_quantizer import \
+    ConfigurableWeightsQuantizer
 from model_compression_toolkit.core.pytorch.pytorch_node_prior_info import create_node_prior_info
 from model_compression_toolkit.core.pytorch.reader.reader import model_reader
 from model_compression_toolkit.core.pytorch.statistics_correction.apply_second_moment_correction import \
     pytorch_apply_second_moment_correction
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy, set_model
+from model_compression_toolkit.exporter.model_wrapper.fw_agnostic.get_inferable_quantizers import \
+    get_inferable_quantizers
+from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
+    get_weights_quantizer_for_node, get_activations_quantizer_for_node
+from model_compression_toolkit.logger import Logger
 
 
 class PytorchImplementation(FrameworkImplementation):
     """
-    An class with implemented methods to support optimizing Pytorch models.
+    A class with implemented methods to support optimizing Pytorch models.
     """
 
     def __init__(self):
         super().__init__()
 
     @property
     def constants(self):
@@ -130,29 +145,29 @@
         return model_reader(_module, representative_data_gen, self.to_numpy, self.to_tensor)
 
     def model_builder(self,
                       graph: Graph,
                       mode: ModelBuilderMode,
                       append2output: List[Any] = None,
                       fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                      return_float_outputs: bool = False) -> Tuple[Module, UserInformation]:
+                      return_float_outputs: bool = False) -> Tuple:
         """
         Build a Pytorch module from a graph.
         The mode determines how the module should be build. append2output is a list of Nodes
         to set as the module outputs.
 
         Args:
             graph: Graph to build the module from it.
             mode: Mode for how to build the module.
             append2output: List of Nodes to set as the module's outputs.
             fw_info: FrameworkInfo object with information about the specific framework's module
             return_float_outputs (bool): whether to return outputs before or after quantization nodes (default)
 
         Returns:
-            A tuple of the Pytorch module that was built and an UserInformation object.
+            A tuple with the model and additional relevant supporting objects.
         """
         pytorch_model_builder = get_pytorch_model_builder(mode)
         return pytorch_model_builder(graph=graph,
                                      append2output=append2output,
                                      fw_info=fw_info,
                                      return_float_outputs=return_float_outputs).build_model()
 
@@ -186,27 +201,14 @@
         Returns:
             Graph after SNC.
         """
         return pytorch_apply_shift_negative_correction(graph,
                                                        core_config,
                                                        fw_info)
 
-    def attach_sc_to_node(self,
-                          node: BaseNode,
-                          fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel)
-        Returns:
-            Statistics collector for the node.
-        """
-        return create_stats_collector_for_node(node, fw_info)
 
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
@@ -228,26 +230,29 @@
 
         Returns: A list of the framework substitutions used before we collect the prior information.
 
         """
         return [ReshapeWithStaticShapes(),
                 MultiHeadAttentionDecomposition(),
                 PermuteCallMethod(),
-                ConstantHolderConv(fw_info)]
+                FunctionalConvSubstitution(fw_info),
+                FunctionalBatchNorm(),
+                FunctionalLayerNorm()]
 
     def get_substitutions_pre_statistics_collection(self,
                                                     quant_config: QuantizationConfig
                                                     ) -> List[common.BaseSubstitution]:
         """
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
 
         Returns: A list of the framework substitutions used before we build a quantized module.
         """
-        substitutions_list = [pytorch_batchnorm_folding()]
+        substitutions_list = [pytorch_batchnorm_folding(),
+                              pytorch_batchnorm_forward_folding()]
         if quant_config.relu_bound_to_power_of_2:
             substitutions_list.append(ReLUBoundToPowerOfTwo())
         return substitutions_list
 
     def get_substitutions_statistics_correction(self, quant_config: QuantizationConfig
                                                 ) -> List[common.BaseSubstitution]:
         """
@@ -273,28 +278,34 @@
 
     def get_linear_collapsing_substitution(self) -> common.BaseSubstitution:
         """
         Returns: linear collapsing substitution
         """
         return pytorch_linear_collapsing()
 
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: None, as Op2d add-const substitution is not supported in torch yet
+        """
+        return None
+
     def get_substitutions_post_statistics_collection(self,
                                                      quant_config: QuantizationConfig) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used after we collect statistics.
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
         Returns:
             A list of the framework substitutions used after we collect statistics.
         """
         substitutions_list = []
         if quant_config.softmax_shift:
             substitutions_list.append(pytorch_softmax_shift())
         if quant_config.input_scaling:
-            raise Exception('Input scaling is currently not supported for Pytorch.')
+            Logger.critical('Input scaling is currently not supported for Pytorch.')
         return substitutions_list
 
     def get_substitutions_pre_build(self) -> List[common.BaseSubstitution]:
         """
         Returns: A list of the framework substitutions used before we build a quantized module.
         """
         return []
@@ -321,41 +332,48 @@
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(pytorch_batchnorm_refusing())
         return substitutions_list
 
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
-                                  disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
+                                  disable_activation_for_metric: bool = False,
+                                  hessian_info_service: HessianInfoService = None
+                                  ) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
         Returns:
             A SensitivityEvaluation object.
         """
 
         return SensitivityEvaluation(graph=graph,
                                      quant_config=quant_config,
                                      representative_data_gen=representative_data_gen,
                                      fw_info=fw_info,
                                      fw_impl=self,
-                                     set_layer_to_bitwidth=set_layer_to_bitwidth,
-                                     get_quant_node_name=lambda node_name: f'{node_name}',
-                                     disable_activation_for_metric=disable_activation_for_metric)
+                                     set_layer_to_bitwidth=partial(set_layer_to_bitwidth,
+                                                                   weights_quantizer_type=ConfigurableWeightsQuantizer,
+                                                                   activation_quantizer_type=ConfigurableActivationQuantizer,
+                                                                   weights_quant_layer_type=PytorchQuantizationWrapper,
+                                                                   activation_quant_layer_type=PytorchActivationQuantizationHolder),
+                                     disable_activation_for_metric=disable_activation_for_metric,
+                                     hessian_info_service=hessian_info_service)
 
     def get_node_prior_info(self,
                             node: BaseNode,
                             fw_info: FrameworkInfo,
                             graph: Graph) -> NodePriorInfo:
         """
         Get a NodePriorInfo object for a node that represents a Pytorch layer.
@@ -375,145 +393,84 @@
         """
         Returns whether a given node in considered as a potential interest point for mp metric computation purposes.
         Args:
             node: Node to indicate whether it needs to be part of the interest points set.
         Returns: True if the node should be considered an interest point, False otherwise.
         """
 
-        if node.type in [Conv2d, Linear, ConvTranspose2d, Sigmoid, sigmoid, Softmax, softmax, operator.add, add, cat]:
+        if node.type in [Conv2d, Linear, ConvTranspose2d, Sigmoid, sigmoid, Softmax, softmax, operator.add, add, cat,
+                         operator.concat]:
             return True
         return False
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         if compute_distance_fn is not None:
             return compute_distance_fn
 
-        elif layer_class in [Softmax, softmax]:
+        elif layer_class in [Softmax, softmax] and axis is not None:
             return compute_kl_divergence
         elif layer_class in [Sigmoid, sigmoid]:
             return compute_cs
         elif layer_class == Linear:
             return compute_cs
         return compute_mse
 
-    def get_model_layers_names(self,
-                               model: Module) -> List[str]:
-        """
-        Returns a list of the given model's layers names.
-
-        Args:
-            model: A Pytorch model.
-
-        Returns: List of layers' names.
-
-        """
-
-        return [layer[0] for layer in list(model.named_children())]
-
-    def get_model_layer_by_name(self,
-                                model: Module,
-                                layer_name: str) -> Module:
-        """
-        Returns a Pytorch model's layer by its name.
-
-        Args:
-            model: A Pytorch model to retrieve a layer from.
-            layer_name: The requested layer's name.
-
-        Returns: A Pytorch layer object.
-
-        """
-
-        return model.get_submodule(target=layer_name)
-
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, torch.Tensor],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],  # dummy - not used in pytorch
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a PyTorch specific model gradient calculation function, which computes the  jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list.
-            alpha: A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated  jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> bool:
         """
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
-        return pytorch_iterative_approx_jacobian_trace(graph_float, model_input_tensors, interest_points, output_list,
-                                                       all_outputs_indices, alpha, n_iter, norm_weights=norm_weights)
-
-    def is_node_compatible_for_metric_outputs(self,
-                                              node: BaseNode) -> bool:
-        """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
-        return node.layer_class not in [argmax, softmax]
+        return node.layer_class not in [argmax, softmax, Softmax]
 
     def get_node_mac_operations(self,
                                 node: BaseNode,
                                 fw_info: FrameworkInfo) -> float:
         """
         Gets the MAC operation count for a given operation.
 
         Args:
             node: A graph node that wraps the operation for which the MAC count is computed.
             fw_info: FrameworkInfo object with information about the Pytorch model.
 
         Returns: The MAC count of the operation
         """
 
-        input_shape = node.input_shape[0]
         output_shape = node.output_shape[0]
         kernel_shape = node.get_weights_by_keys(fw_info.get_kernel_op_attributes(node.type)[0]).shape
         output_channel_axis, input_channel_axis = fw_info.kernel_channels_mapping.get(node.type)
 
         if node.type is Conv2d or node.type is ConvTranspose2d:
             # (C_out * W_out * H_out) * C_in * (W_kernel * H_kernel)
             return np.prod([x for x in output_shape if x is not None]) * \
-                   input_shape[input_channel_axis] * \
+                   kernel_shape[input_channel_axis] * \
                    (kernel_shape[0] * kernel_shape[1])
         elif node.type is Linear:
             # IN * OUT
             return kernel_shape[0] * kernel_shape[1]
         else:
             return 0
 
@@ -549,8 +506,55 @@
             model: A Pytorch model to run inference for.
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
 
-        return model(*inputs)
+        return model(*inputs)
+
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get Pytorch trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculatorPytorch to use for the trace hessian approximation computation for this request.
+
+        """
+        if trace_hessian_request.mode == HessianMode.ACTIVATION:
+            return ActivationTraceHessianCalculatorPytorch(graph=graph,
+                                                           trace_hessian_request=trace_hessian_request,
+                                                           input_images=input_images,
+                                                           fw_impl=self,
+                                                           num_iterations_for_approximation=num_iterations_for_approximation)
+        elif trace_hessian_request.mode == HessianMode.WEIGHTS:
+            return WeightsTraceHessianCalculatorPytorch(graph=graph,
+                                                        trace_hessian_request=trace_hessian_request,
+                                                        input_images=input_images,
+                                                        fw_impl=self,
+                                                        num_iterations_for_approximation=num_iterations_for_approximation)
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of Pytorch compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        return get_inferable_quantizers(node,
+                                        get_weights_quantizer_for_node,
+                                        get_activations_quantizer_for_node,
+                                        node.get_node_weights_attributes())
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,81 +1,89 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable
-import torch
 
+
+from typing import Tuple, Callable
+
+import tensorflow as tf
+import numpy as np
+from tensorflow.python.util.object_identity import Reference as TFReference
+
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
-from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 
 
-def get_symmetric_quantization_range_and_scale(activation_is_signed: bool,
-                                               activation_n_bits: int,
-                                               activation_threshold: float):
+def quantizer_min_max_calculator(threshold: np.ndarray,
+                                 num_bits: int,
+                                 signed: bool) -> Tuple[float, float]:
     """
-    Calculates lower and upper bounds on the quantization range, along with quantization scale,
-    for symmetric quantization (used for the symmetric and power-of-two quantizers),
-    according to whether the quantization is signed or unsigned.
+    Compute quantization range's min/max values given a threshold, number of bits,
+     and whether it's signed or not.
 
     Args:
-        activation_is_signed: Whether the quantization is signed or not.
-        activation_n_bits: Number of bits to use for quantization.
-        activation_threshold: The quantization threshold.
-
-    Returns: range lower bound, range upper bound and quantization scale.
+        threshold: Threshold for quantization range values.
+        num_bits: Number of bits to use for quantization.
+        signed: Whether the quantization range should include negative values or not.
 
+    Returns:
+        Min and max values for quantization range.
     """
-    if activation_is_signed:
-        min_value = -2 ** (activation_n_bits - 1)
-        max_value = 2 ** (activation_n_bits - 1) - 1
-        scale = activation_threshold / 2 ** (activation_n_bits - 1)
+
+    if signed:
+        delta = threshold / (2 ** (num_bits - 1))
+        min_value = -threshold
     else:
+        delta = threshold / (2 ** (num_bits))
         min_value = 0
-        max_value = (2 ** activation_n_bits) - 1
-        scale = activation_threshold / 2 ** activation_n_bits
 
-    return min_value, max_value, scale
+    max_value = threshold - delta
+    return min_value, max_value
 
 
 def power_of_two_quantization(activation_n_bits: int,
                               quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
-    build and return a fake-quantization node, quantized with a power-of-two threshold.
+    build and return a fake-quantization node with power-of-two quantization.
+
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
+
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None or activation_is_signed is None:
-        return None # pragma: no cover
+    if activation_threshold is None:
+        Logger.critical("Activation threshold must be specified.")  # pragma: no cover
+    if activation_is_signed is None:
+        Logger.critical("Parameter 'activation_is_signed' must be specified.")  # pragma: no cover
     if not threshold_is_power_of_two(activation_threshold, per_channel=False):
-        return None # pragma: no cover
+        Logger.critical("Activation threshold must be a power of two.")  # pragma: no cover
 
-    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
-                                                                             activation_n_bits,
-                                                                             activation_threshold)
+    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
+                                                        activation_n_bits,
+                                                        activation_is_signed)
 
-    return lambda x: q(x, min_value, max_value, scale)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
 def symmetric_quantization(activation_n_bits: int,
                            quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a symmetric fake-quantization node.
@@ -86,22 +94,24 @@
 
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None or activation_is_signed is None:
-        return None # pragma: no cover
-
-    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
-                                                                             activation_n_bits,
-                                                                             activation_threshold)
+    if activation_threshold is None:
+        Logger.critical("Activation threshold must be specified.")  # pragma: no cover
+    if activation_is_signed is None:
+        Logger.critical("Parameter 'activation_is_signed' must be specified.")  # pragma: no cover
+
+    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
+                                                        activation_n_bits,
+                                                        activation_is_signed)
 
-    return lambda x: q(x, min_value, max_value, scale)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
 def uniform_quantization(activation_n_bits: int,
                          quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a uniform fake-quantization node.
@@ -109,47 +119,38 @@
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
 
     Returns:
         A fake quantization node.
     """
-    a, b = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
-
-    if a is None or b is None:
-        return None # pragma: no cover
-
-    # fixing quantization range to include 0
-    a = 0 if a > 0 else a
-    b = 0 if b < 0 else b
-    a, b = fix_range_to_include_zero(a, b, activation_n_bits)
+    min_value, max_value = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
 
-    min_value = 0
-    max_value = 2 ** activation_n_bits - 1
-    scale = (b - a) / ((2 ** activation_n_bits) - 1)
-    zero_point = -round(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
+    if min_value is None:
+        Logger.critical("Minimum value must be specified.")  # pragma: no cover
+    if max_value is None:
+        Logger.critical("Maximum value must be specified.")  # pragma: no cover
 
-    return lambda x: q(x, min_value, max_value, scale, zero_point)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
-def q(x: torch.Tensor,
-      min_value: int,
-      max_value: int,
-      scale: float,
-      zero_point: int = 0) -> torch.Tensor:
+def q(x: TFReference, min_value, max_value, activation_n_bits) -> TFReference:
     """
-    Fake-quantize the input tensor x, using a pytorch fake-quantization node.
+    Fake-quantize the input tensor x, using a tensorflow fake-quantization node.
+
     Args:
-        x: input tensor to quantize.
-        min_value: lower bound of the quantized domain.
-        max_value: upper bound of the quantized domain.
-        scale: quantization scale.
-        zero_point: quantization zero_point
+        x: Input tensor to quantize.
+        min_value: quantization range lower bound.
+        max_value: quantization range upper bound.
+        activation_n_bits: Number of bits to use for quantization.
+
     Returns:
         The fake-quantized input tensor.
     """
+    if x.dtype != tf.float32:
+        x = tf.cast(x, dtype=tf.float32)  # pragma: no cover
 
-    return torch.fake_quantize_per_tensor_affine(x,
-                                                 scale=scale,
-                                                 zero_point=zero_point,
-                                                 quant_min=min_value,
-                                                 quant_max=max_value)
+    # fake_quant_with_min_max_vars expects to get x of float32
+    return tf.quantization.fake_quant_with_min_max_vars(x,
+                                                        min=min_value,
+                                                        max=max_value,
+                                                        num_bits=activation_n_bits)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Dict, Callable
 
 import torch
 import numpy as np
 
-from model_compression_toolkit.constants import SIGNED, CLUSTER_CENTERS, THRESHOLD, MULTIPLIER_N_BITS, EPS
+from model_compression_toolkit.constants import SIGNED, LUT_VALUES, THRESHOLD, LUT_VALUES_BITWIDTH, EPS
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 
 
 def activation_lut_kmean_quantizer(activation_n_bits: int,
                                    quantization_params: Dict[str, np.ndarray]) -> Callable:
     """
     Builds a LUT quantizer for layer's activation using the provided params (threshold and clusters).
@@ -23,15 +23,15 @@
 
     lut_fake_quant = PytorchLUTFakeQuant(quantization_params=quantization_params)
     return lambda x: lut_fake_quant(x)
 
 
 class PytorchLUTFakeQuant(torch.nn.Module):
     """
-    A custom PyTorch layer for quantizing activation tensor with non-uniform quantization (using lookup table clustering).
+    A custom PyTorch layer for quantizing activation tensor with non-uniform quantization (using lookup table values).
     """
 
     def __init__(self,
                  quantization_params: Dict[str, np.ndarray]):
         """
         Construct a Pytorch module that quantizes an activation tensor.
 
@@ -39,28 +39,28 @@
             quantization_params: Dictionary of specific parameters for this quantization function.
         """
 
         super(PytorchLUTFakeQuant, self).__init__()
 
         self.quantization_params = quantization_params
         self.activation_is_signed = self.quantization_params.get(SIGNED)
-        self.cluster_centers = to_torch_tensor(self.quantization_params.get(CLUSTER_CENTERS))
+        self.lut_values = to_torch_tensor(self.quantization_params.get(LUT_VALUES))
         self.threshold = self.quantization_params.get(THRESHOLD)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         """
         Quantize the output of an activation,
 
         Args:
             x: input tensor (which is the activation output of the layer that we want to quantize)
 
         Returns:
             Quantized torch Tensor.
         """
-        if self.activation_is_signed is None or self.cluster_centers is None or self.threshold is None:
+        if self.activation_is_signed is None or self.lut_values is None or self.threshold is None:
             return None   # pragma: no cover
 
         _quant_output = self.lut_kmeans_quantizer(x)
         return _quant_output
 
     def lut_kmeans_quantizer(self, tensor_data: torch.Tensor) -> torch.Tensor:
         """
@@ -72,22 +72,22 @@
 
         Args:
             tensor_data: Input activation tensor.
 
         Returns: Quantized tensor.
         """
 
-        tensor = self.int_quantization_with_threshold(tensor_data, MULTIPLIER_N_BITS)
+        tensor = self.int_quantization_with_threshold(tensor_data, LUT_VALUES_BITWIDTH)
         tensor = tensor.unsqueeze(-1)
 
-        expanded_cluster_centers = self.cluster_centers.reshape([*[1 for _ in range(len(tensor.shape) - 1)], -1])
-        cluster_assignments = torch.argmin(torch.abs(tensor - expanded_cluster_centers), dim=-1)
-        centers = self.cluster_centers.flatten()[cluster_assignments]
+        expanded_lut_values = self.lut_values.reshape([*[1 for _ in range(len(tensor.shape) - 1)], -1])
+        lut_values_assignments = torch.argmin(torch.abs(tensor - expanded_lut_values), dim=-1)
+        centers = self.lut_values.flatten()[lut_values_assignments]
 
-        quant_tensor = (centers / (2 ** (MULTIPLIER_N_BITS - int(self.activation_is_signed)))) * self.threshold
+        quant_tensor = (centers / (2 ** (LUT_VALUES_BITWIDTH - int(self.activation_is_signed)))) * self.threshold
 
         return quant_tensor
 
     def int_quantization_with_threshold(self,
                                         data: torch.Tensor,
                                         n_bits: int,
                                         eps: float = EPS) -> torch.Tensor:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/graph_builders.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py`

 * *Files 19% similar despite different names*

```diff
@@ -11,49 +11,46 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import inspect
 from typing import Dict, List, Tuple, Callable
 import torch
-from torch.fx import GraphModule
+from torch.fx import GraphModule, Node
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.base_graph import OutTensor
 from model_compression_toolkit.core.common.graph.edge import Edge
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.pytorch.constants import OUTPUT, PLACEHOLDER, TENSOR_META, CALL_FUNCTION, TYPE, \
-    CALL_METHOD, BIAS, FUNCTIONAL_OP, OP_CALL_KWARGS, OP_CALL_ARGS, INPUTS_AS_LIST, GET_ATTR, CONSTANT, BUFFER
-from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, ConstantHolder, BufferHolder
+    CALL_METHOD, BIAS, FUNCTIONAL_OP, OP_CALL_KWARGS, OP_CALL_ARGS, INPUTS_AS_LIST, TENSOR_INPUT_INDICES, GET_ATTR
+from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder
 from model_compression_toolkit.logger import Logger
 
 
 def extract_holder_weights(constant_name, node_target, model, weights, to_numpy):
     """
-    Extract layer weights and named buffers for BufferHolder and ConstantHolder.
+    Extract layer weights and named buffers to a dictionary.
     Args:
-        constant_name: name to write the parameters under, CONSTANT for ConstantHolder and
-         BUFFER for BufferHolder.
+        constant_name: name to write the parameters under, should be the node name.
         node_target: relevant parameter name from Pytorch FX model.
         model: Pytorch FX model.
         weights: dictionary containing the weights of the node.
         to_numpy: Function to convert framework's tensor to a Numpy array.
 
     Returns:
         Updated weights dictionary.
     """
     named_parameters_weights = {constant_name: to_numpy(parameter) for name, parameter in
                                 model.named_parameters() if node_target == name}
     named_buffer_weights = {constant_name: to_numpy(parameter) for name, parameter in
                             model.named_buffers() if node_target == name}
     if len(named_parameters_weights) + len(named_buffer_weights) > 1:
-        raise Exception(
-            f'Constant parameter can only have one tensor. Here we have '
-            f'{len(named_parameters_weights)+ len(named_buffer_weights)}')
+        Logger.critical("A single constant parameter must correspond to exactly one tensor. Found {len(named_parameters_weights) + len(named_buffer_weights)} parameters.")
 
     weights.update(named_parameters_weights)
     weights.update(named_buffer_weights)
     return weights
 
 
 def nodes_builder(model: GraphModule,
@@ -72,14 +69,16 @@
     """
     # init function variables:
     inputs = []
     outputs = []
     nodes = []
     output_nodes = []
     fx_node_2_graph_node = {}
+    consts_dict = {}
+    used_consts = set()
 
     for node in model.graph.nodes:
         # extract node type and framework attributes
         framework_attr = dict(node.kwargs)
         node_has_activation = True
         if node.target in module_dict.keys():
             node_module = module_dict[node.target]
@@ -103,49 +102,46 @@
             continue
         elif node.op == CALL_METHOD:
             if hasattr(torch, node.target):
                 node_type = getattr(torch, node.target)
             elif hasattr(torch.Tensor, node.target):
                 node_type = getattr(torch.Tensor, node.target)
             else:
-                raise Exception(f'Call method of type \'{node.target}\' is currently not supported.')
+                Logger.critical(f"The call method '{node.target}' is not supported.")
         elif node.op == GET_ATTR:
-            if node.meta[TYPE] == torch.Tensor:
-                node_type = BufferHolder
-            else:
-                node_type = ConstantHolder
-            node_has_activation = False
             Logger.warning(
                 'Pytorch model has a parameter or constant Tensor value. This can cause unexpected behaviour when '
                 'converting the model.')
         else:
-            raise Exception(f'Unknown node type: {node.name}')
+            Logger.critical(f'Encountered an unsupported node type in node: {node.name}.')
 
         # extract layer weights and named buffers
         weights = {}
         if node.target in module_dict.keys():
             named_parameters_weights = {name: to_numpy(parameter) for name, parameter in
                                         module_dict[node.target].named_parameters()}
             named_buffer_weights = {name: to_numpy(parameter) for name, parameter in
                                     module_dict[node.target].named_buffers() if len(parameter.shape) > 0}
             weights.update(named_parameters_weights)
             weights.update(named_buffer_weights)
 
         if node.op == GET_ATTR:
-            if node_type == ConstantHolder:
-                weights = extract_holder_weights(CONSTANT, node.target, model, weights, to_numpy)
-                framework_attr.update(const_size=weights.get(CONSTANT).shape)
-            elif node_type == BufferHolder:
-                weights = extract_holder_weights(BUFFER, node.target, model, weights, to_numpy)
-                framework_attr.update(name=node.name)
+            new_const = extract_holder_weights(node, node.target, model, weights, to_numpy)
+            if list(new_const.keys())[0] in consts_dict:
+                Logger.critical('A constant weight appears to have been recorded multiple times.')
+            consts_dict.update(new_const)
+            continue
 
-        # extract input shapes
+        # extract input shapes and const weights
         input_shape = []
         if node.op != PLACEHOLDER:
-            for input_node in node.all_input_nodes:
+            for i, input_node in enumerate(node.all_input_nodes):
+                if input_node in consts_dict:
+                    used_consts.add(input_node)
+                    weights.update({i: consts_dict[input_node]})
                 tensor_meta = input_node.meta
                 if tensor_meta[TYPE] == torch.Tensor:
                     input_shape += [list(tensor_meta[TENSOR_META].shape)]
                 elif tensor_meta[TYPE] == tuple:
                     input_shape += [list(n.shape) for n in tensor_meta[TENSOR_META]]
                 elif tensor_meta[TYPE] == int:
                     input_shape += [[1]]
@@ -174,36 +170,37 @@
                 node_kwargs[k] = v
 
         # initiate graph nodes
         if node.op in [CALL_METHOD, CALL_FUNCTION]:
             graph_node_type = FunctionalNode
             inputs_as_list1 = len(node.args) > 0 and isinstance(node.args[0], (list, tuple)) and all(
                 [isinstance(n, torch.fx.node.Node) for n in node.args[0]])
-            inputs_as_list = inputs_as_list1 or \
-                             (len(node.args) > 0 and node.args[0].op == PLACEHOLDER and node.args[0].meta[TYPE] in (list, tuple))
+            inputs_as_list = inputs_as_list1 or (len(node.args) > 0 and isinstance(node.args[0], Node) and
+                                                 node.args[0].op == PLACEHOLDER and node.args[0].meta[TYPE] in (list, tuple))
+            tensor_input_index = []
+            op_call_args = list(node.args)
             if inputs_as_list:
-                num_inputs = 1
+                op_call_args.pop(0)
             else:
-                input_counter = 0
                 for in_node in node.all_input_nodes:
-                    for arg in node.args:
+                    for i, arg in enumerate(node.args):
                         if arg == in_node:
-                            input_counter += 1
-                num_inputs = max(len(node.all_input_nodes), input_counter)
-            op_call_args = list(node.args[num_inputs:])
+                            tensor_input_index.append(i)
 
             # remove torch.fx.node.Node from inputs to graph_node_type
-            for arg in op_call_args:
-                if isinstance(arg, torch.fx.node.Node):
-                    op_call_args.remove(arg)
+            op_call_args = [arg for arg in op_call_args if not isinstance(arg, Node)]
+            # convert torch.fx.immutable_collections.immutable_list to tuple
+            op_call_args = [tuple(arg) if isinstance(arg, torch.fx.immutable_collections.immutable_list) else arg
+                            for arg in op_call_args]
 
             kwargs = {FUNCTIONAL_OP: node_type,
                       OP_CALL_ARGS: op_call_args,
                       OP_CALL_KWARGS: node_kwargs,
-                      INPUTS_AS_LIST: inputs_as_list}
+                      INPUTS_AS_LIST: inputs_as_list,
+                      TENSOR_INPUT_INDICES: tensor_input_index}
         else:
             graph_node_type = BaseNode
             kwargs = {}
         graph_node = graph_node_type(name=node.name,
                                      framework_attr=framework_attr,
                                      input_shape=input_shape,
                                      output_shape=output_shape,
@@ -216,52 +213,57 @@
         if node.op == PLACEHOLDER:
             for ii in range(len(output_shape)):
                 inputs.append(graph_node)
 
         fx_node_2_graph_node[node] = graph_node
         nodes.append(graph_node)
 
+    # make sure all extracted constants were used in the graph
+    not_connected_consts = [c for c in consts_dict if c not in used_consts]
+    if not_connected_consts:
+        Logger.critical(f'Error reading graph: These constants are not connected in the graph: {not_connected_consts}.')
+
     # generate graph outputs list
     for node in output_nodes:
         outputs.append(OutTensor(fx_node_2_graph_node[node], output_nodes.index(node)))
 
     return nodes, inputs, outputs, fx_node_2_graph_node
 
 
 def edges_builder(model: GraphModule,
-                   fx_node_2_graph_node: Dict) -> List:
+                  fx_node_2_graph_node: Dict) -> List:
     """
 
     Args:
         model: Pytorch FX model.
         fx_node_2_graph_node: dictionary from fx node to graph node.
 
     Returns:
         List of graph edges
     """
-    src_index = 0 # in fx src_index is always zero because fx uses the getitem operator to fetch node outputs
+    src_index = 0  # in fx src_index is always zero because fx uses the getitem operator to fetch node outputs
     edges = []
     connectivity_dict = {}
     for node in model.graph.nodes:
         if node.op != OUTPUT:
             for input_node in node.all_input_nodes:
-
-                # n_edges_for_input_node is for the case that the input node appears more than
-                # once as the input of the node, for example add(x, x)
-                n_edges_for_input_node = sum([1 for a in node.args if input_node == a])
-                n_edges_for_input_node = max(n_edges_for_input_node, 1)
-
-                dst_index = node.all_input_nodes.index(input_node)
-                for i in range(n_edges_for_input_node):
-                    if connectivity_dict.get(input_node):
-                        connectivity_dict[input_node].append((node, dst_index))
-                    else:
-                        connectivity_dict[input_node] = [(node, dst_index)]
-                    dst_index += 1
+                if input_node in fx_node_2_graph_node:
+                    # n_edges_for_input_node is for the case that the input node appears more than
+                    # once as the input of the node, for example add(x, x)
+                    n_edges_for_input_node = sum([1 for a in node.args if input_node == a])
+                    n_edges_for_input_node = max(n_edges_for_input_node, 1)
+
+                    dst_index = node.all_input_nodes.index(input_node)
+                    for i in range(n_edges_for_input_node):
+                        if connectivity_dict.get(input_node):
+                            connectivity_dict[input_node].append((node, dst_index))
+                        else:
+                            connectivity_dict[input_node] = [(node, dst_index)]
+                        dst_index += 1
     for node in model.graph.nodes:
         out_nodes = connectivity_dict.get(node)
         if out_nodes:
             for (out_node, dst_index) in out_nodes:
                 edges.append(
                     Edge(fx_node_2_graph_node[node], fx_node_2_graph_node[out_node], src_index, dst_index))
 
-    return edges
+    return edges
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/reader/reader.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/reader.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from typing import Callable, Dict
 
 import numpy as np
 import torch
 from torch.fx import symbolic_trace
 from torch.fx.passes.shape_prop import ShapeProp
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.pytorch.reader.graph_builders import edges_builder, nodes_builder
 from model_compression_toolkit.core.pytorch.utils import set_model
 
 
 def generate_module_dict(model: torch.nn.Module) -> Dict:
     """
@@ -80,15 +81,20 @@
         representative_data_gen (Callable): Representative dataset used for shape inference.
         to_tensor: Function to convert a Numpy array to a framework's tensor.
 
     Returns:
         A fx.GraphModule (static model) representing the Pytorch model.
     """
     set_model(pytorch_model)
-    symbolic_traced = symbolic_trace(pytorch_model)
+
+    try:
+        symbolic_traced = symbolic_trace(pytorch_model)
+    except torch.fx.proxy.TraceError as e:
+        Logger.critical(f'Error parsing model with torch.fx\n'
+                        f'fx error: {e}')
     inputs = next(representative_data_gen())
     input_for_shape_infer = [to_tensor(i) for i in inputs]
     ShapeProp(symbolic_traced).propagate(*input_for_shape_infer)
     return symbolic_traced
 
 
 def remove_broken_nodes_from_graph(graph):
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/core/pytorch/utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,26 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 import numpy as np
 from typing import Union
-from model_compression_toolkit.core.pytorch.constants import CUDA, CPU
-
-
-def get_working_device() -> str:
-    """
-    Get the working device of the environment
-
-    Returns:
-        Device "cuda" if GPU is available, else "cpu"
-
-    """
-    return torch.device(CUDA if torch.cuda.is_available() else CPU)
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
+from model_compression_toolkit.logger import Logger
 
 
 def set_model(model: torch.nn.Module, train_mode: bool = False):
     """
     Set model to work in train/eval mode and GPU mode if GPU is available
 
     Args:
@@ -40,18 +30,16 @@
 
     """
     if train_mode:
         model.train()
     else:
         model.eval()
 
-    if torch.cuda.is_available():
-        model.cuda()
-    else:
-        model.cpu()
+    device = get_working_device()
+    model.to(device)
 
 
 def to_torch_tensor(tensor):
     """
     Convert a Numpy array to a Torch tensor.
     Args:
         tensor: Numpy array.
@@ -64,16 +52,18 @@
         return tensor.to(working_device)
     elif isinstance(tensor, list):
         return [to_torch_tensor(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return (to_torch_tensor(t) for t in tensor)
     elif isinstance(tensor, np.ndarray):
         return torch.from_numpy(tensor.astype(np.float32)).to(working_device)
+    elif isinstance(tensor, (int, float)):
+        return torch.from_numpy(np.array(tensor).astype(np.float32)).to(working_device)
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(torch.Tensor)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Torch.tensor: {type(tensor)}.')
 
 
 def torch_tensor_to_numpy(tensor: Union[torch.Tensor, list, tuple]) -> Union[np.ndarray, list, tuple]:
     """
     Convert a Pytorch tensor to a Numpy array.
     Args:
         tensor: Pytorch tensor.
@@ -86,8 +76,8 @@
     elif isinstance(tensor, list):
         return [torch_tensor_to_numpy(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return tuple([torch_tensor_to_numpy(t) for t in tensor])
     elif isinstance(tensor, torch.Tensor):
         return tensor.cpu().detach().contiguous().numpy()
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(np.ndarray)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Numpy array: {type(tensor)}.')
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
+from model_compression_toolkit.exporter.model_exporter.fw_agonstic.quantization_format import QuantizationFormat
 from model_compression_toolkit.exporter.model_exporter.keras.export_serialization_format import \
     KerasExportSerializationFormat
 from model_compression_toolkit.exporter.model_exporter.pytorch.export_serialization_format import \
     PytorchExportSerializationFormat
 from model_compression_toolkit.exporter.model_exporter.keras.keras_export_facade import keras_export_model
 from model_compression_toolkit.exporter.model_exporter.pytorch.pytorch_export_facade import pytorch_export_model
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py`

 * *Files 17% similar despite different names*

```diff
@@ -13,14 +13,19 @@
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
 from model_compression_toolkit.exporter.model_exporter.fw_agonstic.exporter import Exporter
 import keras
 
+import tensorflow as tf
+from packaging import version
+
+DEFAULT_KERAS_EXPORT_EXTENTION = '.keras'
+
 
 class BaseKerasExporter(Exporter):
     """
     Base Keras exporter class.
     """
 
     def __init__(self,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from enum import Enum
 
 
 class KerasExportSerializationFormat(Enum):
     """
     Specify which serialization format to use for exporting a quantized Keras model.
 
-    KERAS_H5 - .h5 file format
+    KERAS - .keras file format
 
     TFLITE - .tflite file format
 
     """
 
-    KERAS_H5 = 0
+    KERAS = 0
     TFLITE = 1
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,50 +14,57 @@
 # ==============================================================================
 import copy
 import importlib
 from typing import Dict, Callable
 
 import keras.models
 import tensorflow as tf
-from keras.engine.base_layer import Layer
+from packaging import version
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.base_layer import Layer
+else:
+    from keras.engine.base_layer import Layer
 
 from mct_quantizers import KerasQuantizationWrapper
-from mct_quantizers.logger import Logger
 
 layers = keras.layers
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.exporter.model_exporter.keras.base_keras_exporter import BaseKerasExporter
 
-from keras.engine import base_layer_utils
-from keras import backend
+from tensorflow.python.keras.engine import base_layer_utils
+from tensorflow.python.keras import backend
 
 
 class FakelyQuantKerasExporter(BaseKerasExporter):
     """
     Exporter for fakely-quant Keras models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
     """
 
     def __init__(self,
                  model: keras.models.Model,
                  is_layer_exportable_fn: Callable,
-                 save_model_path: str):
+                 save_model_path: str,
+                 verbose: bool = True):
         """
 
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             save_model_path: Path to save the exported model.
+            verbose: Whether to log information about the export process or not.
         """
 
         super().__init__(model,
                          is_layer_exportable_fn,
                          save_model_path)
+        self._verbose = verbose
 
     def export(self) -> Dict[str, type]:
         """
         Convert an exportable (fully-quantized) Keras model to a fakely-quant model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
         """
 
@@ -88,15 +95,15 @@
 
                     # Create a list of weights for the new created layer
                     if isinstance(layer.layer, layers.DepthwiseConv2D):
                         weights_list.append(layer.get_quantized_weights()['depthwise_kernel'])
                     elif isinstance(layer.layer, (layers.Conv2D, layers.Dense, layers.Conv2DTranspose)):
                         weights_list.append(layer.get_quantized_weights()['kernel'])
                     else:
-                        Logger.error(f'KerasQuantizationWrapper should wrap only DepthwiseConv2D, Conv2D, Dense'
+                        Logger.critical(f'KerasQuantizationWrapper should wrap only DepthwiseConv2D, Conv2D, Dense'
                                      f' and Conv2DTranspose layers but wrapped layer is {layer.layer}')
 
                     if layer.layer.bias is not None:
                         weights_list.append(layer.layer.bias)
 
                     # In order to add the weights of the layer, we need to build it. To build it
                     # we need to pass its input shape. Not every layer has input_shape since some
@@ -130,15 +137,16 @@
         filtered_weights = self.get_filtered_weights()
         new_model.set_weights(filtered_weights)
         self.exported_model = new_model
 
         if self.exported_model is None:
             Logger.critical(f'Exporter can not save model as it is not exported')  # pragma: no cover
 
-        Logger.info(f'Exporting FQ Keras model to: {self.save_model_path}')
+        if self._verbose:
+            Logger.info(f'Exporting FQ Keras model to: {self.save_model_path}')
 
         keras.models.save_model(self.exported_model, self.save_model_path)
 
         return FakelyQuantKerasExporter.get_custom_objects()
 
     def transform_model_cfg(self) -> dict:
         """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,24 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import os
+from pathlib import Path
 import tempfile
 from typing import Callable
 
-import keras.models
 import tensorflow as tf
+import keras.models
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import FakelyQuantKerasExporter
 from model_compression_toolkit.trainable_infrastructure.keras.load_model import keras_load_quantized_model
-
+from model_compression_toolkit.exporter.model_exporter.keras.base_keras_exporter import DEFAULT_KERAS_EXPORT_EXTENTION
 
 class FakelyQuantTFLiteExporter(FakelyQuantKerasExporter):
     """
     Exporter for fakely-quant TFLite models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
@@ -53,19 +54,25 @@
         """
         Convert an exportable (fully-quantized) Keras model to a fakely-quant TFLite model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
         """
         # Use Keras exporter to quantize model's weights before converting it to TFLite.
         # Since exporter saves the model, we use a tmp path for saving, and then we delete it.
-        _, tmp_h5_file = tempfile.mkstemp('.h5')
-        custom_objects = FakelyQuantKerasExporter(self.model,
-                                                  self.is_layer_exportable_fn,
-                                                  tmp_h5_file).export()
-
-        model = keras_load_quantized_model(tmp_h5_file)
-        os.remove(tmp_h5_file)
+        handle, tmp_file = tempfile.mkstemp(DEFAULT_KERAS_EXPORT_EXTENTION)
+        # Close handle right away, the file is going to be reopenned by Keras exporter
+        os.close(handle)
+        try:
+            custom_objects = FakelyQuantKerasExporter(self.model,
+                                                      self.is_layer_exportable_fn,
+                                                      tmp_file,
+                                                      verbose=False).export()
+
+            model = keras_load_quantized_model(tmp_file)
+        # Ensures artifact is removed even in case of error
+        finally:
+            Path(tmp_file).unlink(missing_ok=True)
 
         self.exported_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()
         Logger.info(f'Exporting FQ tflite model to: {self.save_model_path}')
         with open(self.save_model_path, 'wb') as f:
             f.write(self.exported_model)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py`

 * *Files 23% similar despite different names*

```diff
@@ -8,99 +8,100 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, Dict
+from typing import Callable
 
-from model_compression_toolkit.constants import FOUND_TF
-from model_compression_toolkit.exporter.model_exporter.keras.export_serialization_format import \
-    KerasExportSerializationFormat
+from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.exporter.model_exporter.fw_agonstic.quantization_format import QuantizationFormat
+from model_compression_toolkit.exporter.model_exporter.pytorch.export_serialization_format import \
+    PytorchExportSerializationFormat
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
 
-if FOUND_TF:
-    import keras
-    from model_compression_toolkit.exporter.model_wrapper.keras.validate_layer import is_keras_layer_exportable
-    from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import \
-        FakelyQuantKerasExporter
-    from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_tflite_exporter import \
-        FakelyQuantTFLiteExporter
-    from model_compression_toolkit.exporter.model_exporter.keras.int8_tflite_exporter import INT8TFLiteExporter
+
+if FOUND_TORCH:
+    import torch.nn
+    from model_compression_toolkit.exporter.model_exporter.pytorch.fakely_quant_onnx_pytorch_exporter import \
+    FakelyQuantONNXPyTorchExporter, DEFAULT_ONNX_OPSET_VERSION
+    from model_compression_toolkit.exporter.model_exporter.pytorch.fakely_quant_torchscript_pytorch_exporter import \
+        FakelyQuantTorchScriptPyTorchExporter
+    from model_compression_toolkit.exporter.model_wrapper.pytorch.validate_layer import is_pytorch_layer_exportable
 
     supported_serialization_quantization_export_dict = {
-        KerasExportSerializationFormat.KERAS_H5: [QuantizationFormat.FAKELY_QUANT],
-        KerasExportSerializationFormat.TFLITE: [QuantizationFormat.FAKELY_QUANT, QuantizationFormat.INT8]
+        PytorchExportSerializationFormat.TORCHSCRIPT: [QuantizationFormat.FAKELY_QUANT],
+        PytorchExportSerializationFormat.ONNX: [QuantizationFormat.FAKELY_QUANT, QuantizationFormat.MCTQ]
     }
 
-    def keras_export_model(model: keras.models.Model,
-                           save_model_path: str,
-                           target_platform_capabilities: TargetPlatformCapabilities,
-                           is_layer_exportable_fn: Callable = is_keras_layer_exportable,
-                           serialization_format: KerasExportSerializationFormat = KerasExportSerializationFormat.KERAS_H5) -> Dict[str, type]:
+    def pytorch_export_model(model: torch.nn.Module,
+                             save_model_path: str,
+                             repr_dataset: Callable,
+                             is_layer_exportable_fn: Callable = is_pytorch_layer_exportable,
+                             serialization_format: PytorchExportSerializationFormat = PytorchExportSerializationFormat.ONNX,
+                             quantization_format : QuantizationFormat = QuantizationFormat.MCTQ,
+                             onnx_opset_version=DEFAULT_ONNX_OPSET_VERSION) -> None:
         """
-        Export a Keras quantized model to a h5 or tflite model.
+        Export a PyTorch quantized model to a torchscript or onnx model.
         The model will be saved to the path in save_model_path.
-        keras_export_model supports the combination of QuantizationFormat.FAKELY_QUANT (where weights
-        and activations are float fakely-quantized values) and KerasExportSerializationFormat.KERAS_H5 (where the model
-        will be saved to h5 model) or the combination of KerasExportSerializationFormat.TFLITE (where the model will be
-        saved to tflite model) with QuantizationFormat.FAKELY_QUANT or QuantizationFormat.INT8 (where weights and
-        activations are represented using 8bits integers).
+        Currently, pytorch_export_model supports only QuantizationFormat.FAKELY_QUANT (where weights
+        and activations are float fakely-quantized values) and PytorchExportSerializationFormat.TORCHSCRIPT
+        (where the model will be saved to TorchScript model) or PytorchExportSerializationFormat.ONNX
+        (where the model will be saved to ONNX model).
 
         Args:
             model: Model to export.
             save_model_path: Path to save the model.
-            target_platform_capabilities: TargetPlatformCapabilities object that describes the desired inference
-            target platform (includes quantization format).
+            repr_dataset: Representative dataset for tracing the pytorch model (mandatory for exporting it).
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             serialization_format: Format to export the model according to (by default
-            KerasExportSerializationFormat.KERAS_H5).
-
-        Returns:
-            Custom objects dictionary needed to load the model.
+            PytorchExportSerializationFormat.ONNX).
+            quantization_format: Format of how quantizers are exported (fakely-quant, int8, MCTQ quantizers).
+            onnx_opset_version: ONNX opset version to use for exported ONNX model.
 
         """
 
-        if serialization_format == KerasExportSerializationFormat.KERAS_H5:
-            if target_platform_capabilities.tp_model.quantization_format == QuantizationFormat.FAKELY_QUANT:
-                exporter = FakelyQuantKerasExporter(model,
-                                                    is_layer_exportable_fn,
-                                                    save_model_path)
+        if serialization_format == PytorchExportSerializationFormat.TORCHSCRIPT:
+            if quantization_format in supported_serialization_quantization_export_dict[serialization_format]:
+                exporter = FakelyQuantTorchScriptPyTorchExporter(model,
+                                                                 is_layer_exportable_fn,
+                                                                 save_model_path,
+                                                                 repr_dataset)
             else:
                 Logger.critical(
-                    f'Unsupported quantization {target_platform_capabilities.tp_model.quantization_format} for '
-                    f'serialization {serialization_format} was used to export Keras model. Please see API for '
+                    f'Unsupported quantization {quantization_format} for '
+                    f'serialization {serialization_format} was used to export Pytorch model. Please see API for '
                     f'supported formats.')  # pragma: no cover
 
-        elif serialization_format == KerasExportSerializationFormat.TFLITE:
-            if target_platform_capabilities.tp_model.quantization_format == QuantizationFormat.FAKELY_QUANT:
-                exporter = FakelyQuantTFLiteExporter(model,
-                                                     is_layer_exportable_fn,
-                                                     save_model_path)
-
-            elif target_platform_capabilities.tp_model.quantization_format == QuantizationFormat.INT8:
-                exporter = INT8TFLiteExporter(model,
-                                              is_layer_exportable_fn,
-                                              save_model_path)
+        elif serialization_format == PytorchExportSerializationFormat.ONNX:
+            if quantization_format == QuantizationFormat.FAKELY_QUANT:
+                exporter = FakelyQuantONNXPyTorchExporter(model,
+                                                          is_layer_exportable_fn,
+                                                          save_model_path,
+                                                          repr_dataset,
+                                                          onnx_opset_version=onnx_opset_version)
+            elif quantization_format == QuantizationFormat.MCTQ:
+                exporter = FakelyQuantONNXPyTorchExporter(model,
+                                                          is_layer_exportable_fn,
+                                                          save_model_path,
+                                                          repr_dataset,
+                                                          use_onnx_custom_quantizer_ops=True,
+                                                          onnx_opset_version=onnx_opset_version)
             else:
                 Logger.critical(
-                    f'Unsupported quantization {target_platform_capabilities.tp_model.quantization_format} for '
-                    f'serialization {serialization_format} was used to export Keras model. Please see API for '
+                    f'Unsupported quantization {quantization_format} for '
+                    f'serialization {serialization_format} was used to export Pytorch model. Please see API for '
                     f'supported formats.')  # pragma: no cover
 
         else:
             Logger.critical(
-                f'Unsupported serialization {serialization_format} was used to export Keras model. Please see API '
-                f'for supported formats.')  # pragma: no cover
+                f'Unsupported serialization {serialization_format} was used to export Pytorch model.'
+                f' Please see API for supported formats.')  # pragma: no cover
 
         exporter.export()
 
-        return exporter.get_custom_objects()
 else:
-    def keras_export_model(*args, **kwargs):
-        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                     'when using keras_export_model. '
-                     'Could not find some or all of TensorFlow packages.')  # pragma: no cover
+    def pytorch_export_model(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_export_model'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py`

 * *Files 0% similar despite different names*

```diff
@@ -71,15 +71,15 @@
         self.model = copy.deepcopy(self.model)
         self.repr_dataset = repr_dataset
 
     def _substitute_fully_quantized_model(self):
         """
         Substitution for pytorch "fully-quantized" models. It first uses the weight quantizers
         in PytorchQuantizationWrapper layers to quantize the weights and set them in the layer.
-        Then, it replace all wrapped layers with the layers the wrap.
+        Then, it replaces all wrapped layers with the layers the wrap.
         """
 
         # Replace float weight with wrapped quantized weights
         for layer in self.model.modules():
             if isinstance(layer, PytorchQuantizationWrapper):
                 _set_quantized_weights_in_wrapper(layer)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,13 +17,13 @@
 
 class PytorchExportSerializationFormat(Enum):
     """
     Specify which serialization format to use for exporting a quantized Pytorch model.
 
     TORCHSCRIPT - torchscript format
 
-    ONNX - onnx fromat
+    ONNX - onnx format
 
     """
 
     TORCHSCRIPT = 0
     ONNX = 1
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,31 +8,25 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
 from typing import Callable
 
 import torch.nn
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from model_compression_toolkit.exporter.model_exporter.pytorch.base_pytorch_exporter import BasePyTorchExporter
-from packaging import version
-
-# ONNX opset version 16 is supported from PyTorch 1.12
-if version.parse(torch.__version__) < version.parse("1.12"):
-    OPSET_VERSION = 15
-else:
-    OPSET_VERSION = 16
 
 
-class FakelyQuantONNXPyTorchExporter(BasePyTorchExporter):
+class FakelyQuantTorchScriptPyTorchExporter(BasePyTorchExporter):
     """
     Exporter for fakely-quant PyTorch models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
     """
 
@@ -51,34 +45,31 @@
         """
 
         super().__init__(model,
                          is_layer_exportable_fn,
                          save_model_path,
                          repr_dataset)
 
-
     def export(self) -> None:
         """
         Convert an exportable (fully-quantized) PyTorch model to a fakely-quant model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
         Returns:
             Fake-quant PyTorch model.
         """
         for layer in self.model.children():
             self.is_layer_exportable_fn(layer)
 
         self._substitute_fully_quantized_model()
 
-        Logger.info(f"Exporting PyTorch fake quant onnx model: {self.save_model_path}")
+        torch_traced = torch.jit.trace(self.model,
+                                       to_torch_tensor(next(self.repr_dataset())),
+                                       check_trace=True)
+
+        self.exported_model = torch.jit.script(torch_traced)
+
+        Logger.info(f"Exporting PyTorch torch script Model: {self.save_model_path}")
+
+        torch.jit.save(self.exported_model, self.save_model_path)
 
-        model_input = to_torch_tensor(next(self.repr_dataset())[0])
 
-        torch.onnx.export(self.model,
-                          model_input,
-                          self.save_model_path,
-                          opset_version=OPSET_VERSION,
-                          verbose=False,
-                          input_names=['input'],
-                          output_names=['output'],
-                          dynamic_axes={'input': {0: 'batch_size'},
-                                        'output': {0: 'batch_size'}})
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_exporter/keras/mctq_keras_exporter.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,75 +1,52 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 from typing import Callable
 
-import torch.nn
+import keras.models
+layers = keras.layers
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-from model_compression_toolkit.exporter.model_exporter.pytorch.base_pytorch_exporter import BasePyTorchExporter
-
+from model_compression_toolkit.exporter.model_exporter.keras.base_keras_exporter import BaseKerasExporter
 
-class FakelyQuantTorchScriptPyTorchExporter(BasePyTorchExporter):
-    """
-    Exporter for fakely-quant PyTorch models.
-    The exporter expects to receive an exportable model (where each layer's full quantization parameters
-    can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
-    format) and fake-quant layers for the activations.
-    """
+class MCTQKerasExporter(BaseKerasExporter):
 
     def __init__(self,
-                 model: torch.nn.Module,
+                 model: keras.models.Model,
                  is_layer_exportable_fn: Callable,
-                 save_model_path: str,
-                 repr_dataset: Callable):
+                 save_model_path: str):
         """
 
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             save_model_path: Path to save the exported model.
-            repr_dataset: Representative dataset (needed for creating torch script).
         """
 
         super().__init__(model,
                          is_layer_exportable_fn,
-                         save_model_path,
-                         repr_dataset)
+                         save_model_path)
 
-    def export(self) -> None:
+    def export(self):
         """
-        Convert an exportable (fully-quantized) PyTorch model to a fakely-quant model
-        (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
-
-        Returns:
-            Fake-quant PyTorch model.
+        Export an exportable (fully-quantized) Keras model to a Keras model with
+        MCTQ quantizers. This is done by using keras saving model function.
         """
-        for layer in self.model.children():
-            self.is_layer_exportable_fn(layer)
-
-        self._substitute_fully_quantized_model()
-
-        torch_traced = torch.jit.trace(self.model,
-                                       to_torch_tensor(next(self.repr_dataset())),
-                                       check_trace=True)
-
-        self.exported_model = torch.jit.script(torch_traced)
-
-        Logger.info(f"Exporting PyTorch torch script Model: {self.save_model_path}")
+        if self.model is None:
+            Logger.critical(f'Exporter can not save None model')  # pragma: no cover
 
-        torch.jit.save(self.exported_model, self.save_model_path)
+        Logger.info(f'Exporting Keras model with MCTQ custom quantizers to: {self.save_model_path}')
+        keras.models.save_model(self.model, self.save_model_path)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,86 +9,88 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Tuple, Callable
+from typing import Union, Callable
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.constants import FOUND_TF
-from model_compression_toolkit.core.common.user_info import UserInformation
+from model_compression_toolkit.constants import FOUND_TORCH
 from model_compression_toolkit.logger import Logger
-from mct_quantizers import KerasActivationQuantizationHolder
+from model_compression_toolkit.core.common import BaseNode
+import model_compression_toolkit.core as C
 
-if FOUND_TF:
-    import tensorflow as tf
-    from tensorflow.keras.layers import Layer
-    from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
-    from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizers import get_quantization_quantizers
-    from mct_quantizers import KerasQuantizationWrapper
+if FOUND_TORCH:
+    import torch
+    from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
+    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
 
-    def _get_wrapper(node: common.BaseNode,
-                     layer: Layer) -> Layer:
+
+    def fully_quantized_wrapper(node: common.BaseNode,
+                                module: torch.nn.Module,
+                                fw_impl) -> Union[torch.nn.Module,PytorchQuantizationWrapper]:
         """
-        A function which takes a computational graph node and a keras layer and perform the quantization wrapping
+        A function which takes a computational graph node and a pytorch module and
+        perform the quantization wrapping
+
         Args:
             node: A node of mct graph.
-            layer: A keras layer
-
-        Returns: Wrapped layer with weights quantizers and activation quantizers
+            module: A Pytorch module
+        Returns: Wrapped layer
 
         """
-        weights_quantizers, _ = get_quantization_quantizers(node)
-        if len(weights_quantizers) > 0:
-            return KerasQuantizationWrapper(layer,
-                                            weights_quantizers)
-        return layer
-
+        weight_quantizers, _ = fw_impl.get_inferable_quantizers(node)
+        if len(weight_quantizers) > 0:
+            return PytorchQuantizationWrapper(module, weight_quantizers)
+        return module
 
-    def get_activation_quantizer_holder(node: common.BaseNode) -> Callable:
+    def get_activation_quantizer_holder(node: BaseNode, fw_impl) -> Callable:
         """
-        Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
-
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
+        If the layer is not supposed to be wrapped with an activation quantizer - return None.
         Args:
-            node: Node to get ActivationQuantizationHolder to attach in its output.
-
+            node: Node to attach a PytorchActivationQuantizationHolder to its output.
         Returns:
-            A ActivationQuantizationHolder layer for the node activation quantization.
+            A PytorchActivationQuantizationHolder module for the node's activation quantization.
         """
-        _, activation_quantizers = get_quantization_quantizers(node)
-
+        _, activation_quantizers = fw_impl.get_inferable_quantizers(node)
         # Holder by definition uses a single quantizer for the activation quantization
-        # thus we make sure this is the only possible case (unless it's a node with no activation
+        # thus we make sure this is the only possible case (unless it's a node we no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
-            return KerasActivationQuantizationHolder(activation_quantizers[0])
-
-        Logger.error(
-            f'ActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        Logger.critical(
+            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
             f'were found for node {node}')
 
-
-
-    def get_exportable_keras_model(graph: Graph) -> Tuple[tf.keras.models.Model, UserInformation]:
+    def get_exportable_pytorch_model(graph: Graph):
         """
-        Convert graph to an exportable Keras model (model with all quantization parameters).
-        An exportable model can then be exported using model_exporter, to retrieve the
-        final exported model.
+        Convert graph to fully quantized PyTorch model.
 
         Args:
-            graph: Graph to convert to an exportable Keras model.
+            graph: Graph to convert to a PyTorch model.
 
         Returns:
-            Exportable Keras model and user information.
+            Fully quantized PyTorch model.
         """
-        exportable_model, user_info = KerasModelBuilder(graph=graph,
-                                                        wrapper=_get_wrapper,
-                                                        get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
-        exportable_model.trainable = False
+        exportable_model, user_info = PyTorchModelBuilder(graph=graph,
+                                                          wrapper=lambda n, m:
+                                                          fully_quantized_wrapper(n, m,
+                                                                                  fw_impl=C.pytorch.pytorch_implementation.PytorchImplementation()),
+                                                          get_activation_quantizer_holder_fn=lambda n:
+                                                          get_activation_quantizer_holder(n,
+                                                                                          fw_impl=C.pytorch.pytorch_implementation.PytorchImplementation())).build_model()
+
+        Logger.info("Please run your accuracy evaluation on the exported quantized model to verify it's accuracy.\n"
+                    "Checkout the FAQ and Troubleshooting pages for resolving common issues and improving the quantized model accuracy:\n"
+                    "FAQ: https://github.com/sony/model_optimization/tree/main/FAQ.md\n"
+                    "Quantization Troubleshooting: https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md")
+
         return exportable_model, user_info
+
+
 else:
-    def get_exportable_keras_model(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                     'when using get_exportable_keras_model. '
-                     'Could not find Tensorflow package.')
+    def get_exportable_pytorch_model(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'get_exportable_pytorch_model'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,145 +1,195 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict, Any
+import numpy as np
+import torch
+import torch.nn as nn
+from torch import Tensor
 
-from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.constants import THRESHOLD, RANGE_MIN, RANGE_MAX, SIGNED, CLUSTER_CENTERS, SCALE_PER_CHANNEL
+from model_compression_toolkit.constants import RANGE_MAX, RANGE_MIN
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
 
-from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.qat import TrainingMethod
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from mct_quantizers import QuantizationTarget
-from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
-from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
-from mct_quantizers import constants as qi_keras_consts
+from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
+from model_compression_toolkit import constants as C
 
-def get_inferable_quantizer_kwargs(node: BaseNode,
-                                   quantization_target: QuantizationTarget) -> Dict[str, Any]:
-    """
-    Get the quantization parameters for an inferable quantizer.
-    Args:
-        node: The node for which the quantizer is being created.
-        quantization_target: The target of the quantization (weights or activations).
-    Returns:
-        The quantization parameters as a dictionary.
-    """
-
-    if quantization_target == QuantizationTarget.Weights:
-        # Get the weights quantization configuration for the node
-        node_w_qc = node.final_weights_quantization_cfg
-        quantization_method = node_w_qc.weights_quantization_method
-
-        # Return the appropriate quantization parameters based on the quantization method
-        if quantization_method in [QuantizationMethod.POWER_OF_TWO,
-                                   QuantizationMethod.SYMMETRIC]:
-            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
-                    qi_keras_consts.THRESHOLD: list(node_w_qc.weights_quantization_params[THRESHOLD].flatten()),
-                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
-                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[THRESHOLD].shape)}
-
-        elif quantization_method in [QuantizationMethod.UNIFORM]:
-            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
-                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                    qi_keras_consts.MIN_RANGE: list(node_w_qc.weights_quantization_params[RANGE_MIN].flatten()),
-                    qi_keras_consts.MAX_RANGE: list(node_w_qc.weights_quantization_params[RANGE_MAX].flatten()),
-                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
-                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[RANGE_MIN].shape)}
-
-        elif quantization_method in [QuantizationMethod.LUT_SYM_QUANTIZER, QuantizationMethod.LUT_POT_QUANTIZER]:
-            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
-                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                    qi_keras_consts.CLUSTER_CENTERS: node_w_qc.weights_quantization_params[CLUSTER_CENTERS],
-                    qi_keras_consts.THRESHOLD: list(node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten()),
-                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
-                    # TODO: how to pass multiplier nbits and eps for a specific node?
-                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
-
-        else:
-            Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
-
-    elif quantization_target == QuantizationTarget.Activation:
-        # Get the activation quantization configuration for the node
-        node_qc = node.final_activation_quantization_cfg
-        quantization_method = node_qc.activation_quantization_method
-
-        # Return the appropriate quantization parameters based on the quantization method
-        if quantization_method in [QuantizationMethod.POWER_OF_TWO,
-                                   QuantizationMethod.SYMMETRIC]:
-            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
-                    # In activation quantization is per-tensor only - thus we hold the threshold as a list with a len of 1
-                    qi_keras_consts.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]],
-                    qi_keras_consts.SIGNED: node_qc.activation_quantization_params[SIGNED]}
-
-        elif quantization_method in [QuantizationMethod.UNIFORM]:
-            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
-                    # In activation quantization is per-tensor only - thus we hold the min/max as a list with a len of 1
-                    qi_keras_consts.MIN_RANGE: [node_qc.activation_quantization_params[RANGE_MIN]],
-                    qi_keras_consts.MAX_RANGE: [node_qc.activation_quantization_params[RANGE_MAX]]}
-
-        elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER]:
-            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
-                    qi_keras_consts.SIGNED: node_qc.activation_quantization_params[SIGNED],
-                    qi_keras_consts.CLUSTER_CENTERS: node_qc.activation_quantization_params[CLUSTER_CENTERS],
-                    qi_keras_consts.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]]
-                    # TODO: how to pass multiplier nbits and eps for a specific node?
-                    }
-        else:
-            Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
-    else:
-        Logger.critical(f'{quantization_target} is not supported')  # pragma: no cover
-
-
-def get_weights_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
-    """
-    Get weights quantizer for a node.
-    Args:
-        node: Node to create a weight quantizer for.
-    Returns:
-        Quantizer for the node's weights.
-    """
-    if node.final_weights_quantization_cfg is None:
-        Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
-        # no cover
-    node_w_qc = node.final_weights_quantization_cfg
-    weights_quantization_method = node_w_qc.weights_quantization_method
-
-    quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
-                                                      weights_quantization_method,
-                                                      BaseKerasInferableQuantizer)
-    kwargs = get_inferable_quantizer_kwargs(node, QuantizationTarget.Weights)
-
-    return quantier_for_node(**kwargs)
-
-
-def get_activations_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
-    """
-    Get activation quantizer for a node.
-    Args:
-        node: Node to create an activation quantizer for.
-    Returns:
-        Quantizer for the node's activations.
-    """
-    if node.final_activation_quantization_cfg is None:
-        Logger.critical(f'Can not set quantizer for a node with no final activation quantization configuration')  #
-        # pragma: no cover
-    node_act_qc = node.final_activation_quantization_cfg
-    activation_quantization_method = node_act_qc.activation_quantization_method
-
-    quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
-                                                      activation_quantization_method,
-                                                      BaseKerasInferableQuantizer)
-    kwargs = get_inferable_quantizer_kwargs(node, QuantizationTarget.Activation)
-
-    return quantier_for_node(**kwargs)
+from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
+from model_compression_toolkit.qat.pytorch.quantizer.quantizer_utils import uniform_quantizer
+from mct_quantizers.pytorch.quantizers import \
+    WeightsUniformInferableQuantizer, ActivationUniformInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
+    TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.UNIFORM],
+                identifier=TrainingMethod.STE)
+class STEUniformWeightQATQuantizer(BasePytorchQATTrainableQuantizer):
+    """
+    Trainable constrained quantizer to quantize a layer inputs.
+    """
+
+    def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
+        """
+        Initialize a TrainableWeightQuantizer object with parameters to use
+        for the quantization.
+
+        Args:
+            quantization_config: trainable quantizer config class
+        """
+        super().__init__(quantization_config)
+        self.num_bits = self.quantization_config.weights_n_bits
+        self.min_int = 0
+        self.max_int = 2 ** self.num_bits - 1
+        self.max_values = quantization_config.weights_quantization_params[RANGE_MAX]
+        self.min_values = quantization_config.weights_quantization_params[RANGE_MIN]
+        self.min_max_shape = np.asarray(self.max_values).shape
+        self.max = np.reshape(self.max_values,
+                              [-1]) if self.quantization_config.weights_per_channel_threshold else float(
+            self.max_values)
+        self.min = np.reshape(self.min_values,
+                              [-1]) if self.quantization_config.weights_per_channel_threshold else float(
+            self.min_values)
+
+
+    def initialize_quantization(self,
+                                tensor_shape: torch.Size,
+                                name: str,
+                                layer: PytorchQuantizationWrapper):
+        """
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+
+        # Add min and max variables to layer.
+        layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_values), requires_grad=False))
+        layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_values), requires_grad=False))
+
+        # Save the quantizer parameters for later calculations
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
+
+
+    def __call__(self,
+                 inputs: nn.Parameter,
+                 training: bool) -> Tensor:
+        """
+        Quantize a tensor
+        Args:
+            inputs: Input tensor to quantize.
+            training: whether in training mode or not
+        Returns:
+            quantized tensor
+        """
+        return uniform_quantizer(inputs, self.get_quantizer_variable(FQ_MIN), self.get_quantizer_variable(FQ_MAX), self.num_bits)
+
+    def convert2inferable(self) -> WeightsUniformInferableQuantizer:
+        """
+        Convert quantizer to inferable quantizer.
+
+        Returns:
+            A pytorch inferable quanizer object.
+        """
+        _min = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        _max = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
+
+        return WeightsUniformInferableQuantizer(num_bits=self.num_bits,
+                                                min_range=_min.tolist(),
+                                                max_range=_max.tolist(),
+                                                per_channel=self.quantization_config.weights_per_channel_threshold,
+                                                channel_axis=self.quantization_config.weights_channels_axis)
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
+                quantization_method=[QuantizationMethod.UNIFORM],
+                identifier=TrainingMethod.STE)
+class STEUniformActivationQATQuantizer(BasePytorchQATTrainableQuantizer):
+    """
+    Trainable constrained quantizer to quantize a layer activations.
+    """
+
+    def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
+        """
+        Initialize a STEUniformActivationQATQuantizer object with parameters to use
+        for uniform quantization.
+
+        Args:
+            quantization_config: trainable quantizer config class
+        """
+        super().__init__(quantization_config)
+
+        np_min_range = quantization_config.activation_quantization_params[C.RANGE_MIN]
+        np_max_range = quantization_config.activation_quantization_params[C.RANGE_MAX]
+        self.min_range_tensor = torch.Tensor([np_min_range])
+        self.max_range_tensor = torch.Tensor([np_max_range])
+        self.num_bits = quantization_config.activation_n_bits
+
+    def initialize_quantization(self,
+                                tensor_shape: torch.Size,
+                                name: str,
+                                layer: PytorchQuantizationWrapper):
+        """
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+        layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_range_tensor), requires_grad=True))
+        layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_range_tensor), requires_grad=True))
+
+        # Save the quantizer parameters for later calculations
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
+
+    def __call__(self,
+                 inputs: torch.Tensor,
+                 training: bool = True) -> torch.Tensor:
+        """
+        Quantize a tensor.
+        Args:
+            inputs: Input tensor to quantize.
+            training: Whether the graph is in training mode.
+
+        Returns:
+            The quantized tensor.
+        """
+
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
+        q_tensor = uniform_quantizer(inputs, _min, _max, self.num_bits)
+        return q_tensor
+
+    def convert2inferable(self) -> ActivationUniformInferableQuantizer:
+        """
+        Convert quantizer to inferable quantizer.
+
+        Returns:
+            A pytorch inferable quanizer object.
+        """
+        _min = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        _max = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
+
+        return ActivationUniformInferableQuantizer(num_bits=self.num_bits,
+                                                   min_range=_min.tolist(),
+                                                   max_range=_max.tolist())
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,39 +8,42 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict, List, Tuple
+from typing import Dict, List, Tuple, Callable
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
-    get_weights_quantizer_for_node, get_activations_quantizer_for_node
 
 
-def get_quantization_quantizers(node: BaseNode) -> Tuple[Dict, List]:
+def get_inferable_quantizers(node: BaseNode,
+                             get_weights_quantizer_for_node: Callable,
+                             get_activations_quantizer_for_node: Callable,
+                             attributes_names: List[str] = []) -> Tuple[Dict, List]:
     """
     Create quantizers to wrap a layer for its corresponding node.
 
     Args:
         node: Node to create quantizers for.
+        get_weights_quantizer_for_node: A function that returns weights quantizer for the node attributes.
+        get_activations_quantizer_for_node: A function that returns activation quantizer for the node activation tensor.
+        attributes_names: A potential list of attribute names to set weights quantizers to.
 
     Returns:
         weight_quantizers: A dictionary between a weight's name to its quantizer.
         activation_quantizers: A list of activations quantization, one for each layer output.
     """
+
     weight_quantizers = {}
     activation_quantizers = []
 
-    if node.is_weights_quantization_enabled():
-        weight_attrs = DEFAULT_KERAS_INFO.get_kernel_op_attributes(node.type)
-        weight_quantizer = get_weights_quantizer_for_node(node)
-        for attr in weight_attrs:
+    for attr in attributes_names:
+        if node.is_weights_quantization_enabled(attr):
+            weight_quantizer = get_weights_quantizer_for_node(node, attr)
             weight_quantizers[attr] = weight_quantizer
 
     if node.is_activation_quantization_enabled():
         num_of_outputs = len(node.output_shape) if isinstance(node.output_shape, list) else 1
         activation_quantizers = [get_activations_quantizer_for_node(node)] * num_of_outputs
 
     return weight_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,65 +10,60 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any
 
-from mct_quantizers import BaseInferableQuantizer, KerasActivationQuantizationHolder
-from model_compression_toolkit.constants import FOUND_TF
 from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TORCH
 
-if FOUND_TF:
-    from keras.engine.base_layer import Layer
-    from keras.engine.input_layer import InputLayer
-    from mct_quantizers import KerasQuantizationWrapper
 
-    def is_keras_layer_exportable(layer: Any) -> bool:
+if FOUND_TORCH:
+    import torch.nn as nn
+    from mct_quantizers import PytorchQuantizationWrapper
+    from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
+    from mct_quantizers.pytorch.activation_quantization_holder import PytorchActivationQuantizationHolder
+
+    def is_pytorch_layer_exportable(layer: Any) -> bool:
         """
-        Check whether a Keras layer is a valid exportable layer or not.
+        Check whether a torch Module is a valid exportable module or not.
 
         Args:
-            layer: Keras layer to check if considered to be valid for exporting.
+            layer: PyTorch module to check if considered to be valid for exporting.
 
         Returns:
-            Check whether a Keras layer is a valid exportable layer or not.
+            Check whether a PyTorch layer is a valid exportable layer or not.
         """
-        # Keras Input layers are not wrapped
-        if isinstance(layer, InputLayer):
-            return True
-
-        valid_layer = isinstance(layer, Layer)
-        if not valid_layer:
-            Logger.error(
-                f'Exportable layer must be a Keras layer, but layer {layer.name} is of type '
-                f'{type(layer)}') # pragma: no cover
+        if not isinstance(layer, nn.Module):
+            Logger.critical(f'Exportable layer must be a nn.Module layer, but layer {layer.name} is of type {type(layer)}.') # pragma: no cover
 
-        if isinstance(layer, KerasQuantizationWrapper):
+        if isinstance(layer, PytorchQuantizationWrapper):
             valid_weights_quantizers = isinstance(layer.weights_quantizers, dict)
             if not valid_weights_quantizers:
-                Logger.error(
-                    f'KerasQuantizationWrapper must have a weights_quantizers but has a '
-                    f'{type(layer.weights_quantizers)} object') # pragma: no cover
+                Logger.critical(
+                    f'PytorchQuantizationWrapper must have a weights_quantizers but has a '
+                    f'{type(layer.weights_quantizers)} object.') # pragma: no cover
 
             if len(layer.weights_quantizers) == 0:
-                Logger.error(f'KerasQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers. If layer is not quantized it should be a Keras layer.')
+                Logger.critical(f'PytorchQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers.'
+                             f'If layer is not quantized it should be a Keras layer.')
 
             for _, weights_quantizer in layer.weights_quantizers.items():
-                if not isinstance(weights_quantizer, BaseInferableQuantizer):
-                    Logger.error(
-                        f'weights_quantizer must be a BaseInferableQuantizer object but has a '
-                        f'{type(weights_quantizer)} object')  # pragma: no cover
-
-        if isinstance(layer, KerasActivationQuantizationHolder):
-            if not isinstance(layer.activation_holder_quantizer, BaseInferableQuantizer):
-                Logger.error(
-                    f'activation quantizer in KerasActivationQuantizationHolder'
-                    f' must be a BaseInferableQuantizer object but has a '
-                    f'{type(layer.activation_holder_quantizer)} object')  # pragma: no cover
+                if not isinstance(weights_quantizer, BasePyTorchInferableQuantizer):
+                    Logger.critical(
+                        f'weights_quantizer must be a BasePyTorchInferableQuantizer object but has a '
+                        f'{type(weights_quantizer)} object.')  # pragma: no cover
+
+        elif isinstance(layer, PytorchActivationQuantizationHolder):
+            if not isinstance(layer.activation_holder_quantizer, BasePyTorchInferableQuantizer):
+                Logger.critical(
+                    f'activation quantizer in PytorchActivationQuantizationHolder'
+                    f' must be a BasePyTorchInferableQuantizer object but has a '
+                    f'{type(layer.activation_holder_quantizer)} object.')  # pragma: no cover
 
         return True
+
 else:
-    def is_keras_layer_exportable(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                     'when using is_keras_layer_exportable. '
-                     'Could not find Tensorflow package.')
+    def is_pytorch_layer_exportable(*args, **kwargs):  # pragma: no cover
+        Logger.critical("PyTorch must be installed to use 'is_pytorch_layer_exportable'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,85 +1,102 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Union, Callable
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.constants import FOUND_TORCH
+from typing import Dict, List, Any
+
+from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import ConfigurableQuantizerIdentifier
+from model_compression_toolkit.core.common.mixed_precision.configurable_quantizer_utils import \
+    verify_candidates_descending_order, init_activation_quantizers
+from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
+    CandidateNodeQuantizationConfig
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from mct_quantizers import mark_quantizer
+
+import torch
+import torch.nn as nn
+from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
+                quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC,
+                                     QuantizationMethod.UNIFORM, QuantizationMethod.LUT_POT_QUANTIZER],
+                identifier=ConfigurableQuantizerIdentifier.CONFIGURABLE_ID)
+class ConfigurableActivationQuantizer(BasePyTorchInferableQuantizer):
+    """
+    Configurable activation quantizer for mixed precision search.
+    It holds a set of activation quantizers for each of the given bit-width candidates, provided by the
+    node's quantization config. This allows to use different quantized activations on-the-fly, according to the
+    "active" quantization configuration index.
+    """
+
+    def __init__(self,
+                 node_q_cfg: List[CandidateNodeQuantizationConfig],
+                 max_candidate_idx: int = 0,
+                 kernel_attr: str = None):
+        """
+        Initializes a configurable quantizer.
 
-if FOUND_TORCH:
-    import torch
-    from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
-    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizers import \
-        get_quantization_quantizers
+        Args:
+            node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
+                use this quantizer.
+            max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
+            kernel_attr: A kernel attribute name if the node have a kernel attribute (used only for candidates order validation).
+        """
 
+        super(ConfigurableActivationQuantizer, self).__init__()
 
-    def fully_quantized_wrapper(node: common.BaseNode,
-                                module: torch.nn.Module) -> Union[torch.nn.Module,PytorchQuantizationWrapper]:
-        """
-        A function which takes a computational graph node and a pytorch module and
-        perform the quantization wrapping
+        self.node_q_cfg = node_q_cfg
 
-        Args:
-            node: A node of mct graph.
-            module: A Pytorch module
-        Returns: Wrapped layer
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
 
-        """
-        weight_quantizers, _ = get_quantization_quantizers(node)
-        if len(weight_quantizers) > 0:
-            return PytorchQuantizationWrapper(module, weight_quantizers)
-        return module
+        for qc in self.node_q_cfg:
+            if qc.activation_quantization_cfg.enable_activation_quantization != \
+                   self.node_q_cfg[0].activation_quantization_cfg.enable_activation_quantization:
+                Logger.critical("Unsupported configuration: Mixing candidates with differing activation quantization states (enabled/disabled).")  # pragma: no cover
+
+        # Setting layer's activation
+        self.activation_quantizers = init_activation_quantizers(self.node_q_cfg)
+        self.active_quantization_config_index = max_candidate_idx  # initialize with first config as default
 
-    def get_activation_quantizer_holder(node: BaseNode) -> Callable:
+    def set_active_activation_quantizer(self,
+                                        index: int):
         """
-        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
-        If the layer is not supposed to be wrapped with an activation quantizer - return None.
+        Set an activation quantizer to use by the layer wrapped by the module.
+
         Args:
-            node: Node to attach a PytorchActivationQuantizationHolder to its output.
-        Returns:
-            A PytorchActivationQuantizationHolder module for the node's activation quantization.
+            index: Index of a candidate quantization configuration to use its quantizer
+                for quantizing the activation.
         """
-        _, activation_quantizers = get_quantization_quantizers(node)
-        # Holder by definition uses a single quantizer for the activation quantization
-        # thus we make sure this is the only possible case (unless it's a node we no activation
-        # quantization, which in this case has an empty list).
-        if len(activation_quantizers) == 1:
-            return PytorchActivationQuantizationHolder(activation_quantizers[0])
-        Logger.error(
-            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
-            f'were found for node {node}')
 
-    def get_exportable_pytorch_model(graph: Graph):
+        assert index < len(self.node_q_cfg), f'Quantizer has {len(self.node_q_cfg)} ' \
+                                             f'possible nbits. Can not set index {index}'
+        self.active_quantization_config_index = index
+
+    def __call__(self,
+                 inputs: nn.Parameter) -> torch.Tensor:
         """
-        Convert graph to fully quantized PyTorch model.
+        Method to return the quantized activation tensor. This method is called when the framework needs to
+        quantize a float activation tensor, and is expected to return the quantized tensor, according to the active
+        activation quantizer.
 
         Args:
-            graph: Graph to convert to a PyTorch model.
+            inputs: Input tensor to quantize.
 
         Returns:
-            Fully quantized PyTorch model.
+            Quantized activation tensor.
         """
-        return PyTorchModelBuilder(graph=graph,
-                                   wrapper=fully_quantized_wrapper,
-                                   get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
-
-else:
-    def get_exportable_pytorch_model(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing torch is mandatory '
-                     'when using get_exportable_pytorch_model. '
-                     'Could not find PyTorch package.')
+
+        return self.activation_quantizers[self.active_quantization_config_index](inputs)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,107 +13,143 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import Dict, Any
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX, \
-    SCALE_PER_CHANNEL, CLUSTER_CENTERS
+    SCALE_PER_CHANNEL, LUT_VALUES
+from model_compression_toolkit.core.common.quantization.node_quantization_config import BaseNodeQuantizationConfig, \
+    NodeWeightsQuantizationConfig, NodeActivationQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers import \
     constants as qi_inferable_quantizers_constants
 from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
 import numpy as np
 
 
-def get_weights_inferable_quantizer_kwargs(node: BaseNode) -> Dict[str, Any]:
-    # Get the weights quantization configuration for the node
-    node_w_qc = node.final_weights_quantization_cfg
-    quantization_method = node_w_qc.weights_quantization_method
+def get_weights_inferable_quantizer_kwargs(node_qc: NodeWeightsQuantizationConfig, attr_name: str) -> Dict[str, Any]:
+    """
+    Get the quantization parameters for a weights inferable quantizer.
+    Args:
+        node_qc: The node quantization configuration of the node for which the quantizer is being created.
+            Needs to match the specific quantization target.
+        attr_name: The weights attribute to get its quantizer kwargs (if target is weights quantization).
+
+
+    Returns:
+        The quantization parameters as a dictionary.
+    """
+
+    if not isinstance(node_qc, NodeWeightsQuantizationConfig):
+        Logger.critical(
+            f"Non-compatible node quantization config was given for quantization target Weights.")  # pragma: no cover
+
+    if attr_name is None:
+        Logger.error(f"Attribute name was not specified for retrieving weights quantizer kwargs.")
+
+    attr_node_qc = node_qc.get_attr_config(attr_name=attr_name)
+
+    quantization_method = attr_node_qc.weights_quantization_method
 
     # Return the appropriate quantization parameters based on the quantization method
     if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                QuantizationMethod.SYMMETRIC]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_w_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.THRESHOLD: node_w_qc.weights_quantization_params[THRESHOLD].flatten(),
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_w_qc.weights_channels_axis}
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.THRESHOLD: attr_node_qc.weights_quantization_params[THRESHOLD].flatten().tolist(),
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                }
 
     elif quantization_method in [QuantizationMethod.UNIFORM]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_w_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.MIN_RANGE: node_w_qc.weights_quantization_params[RANGE_MIN].flatten(),
-                qi_inferable_quantizers_constants.MAX_RANGE: node_w_qc.weights_quantization_params[RANGE_MAX].flatten(),
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_w_qc.weights_channels_axis}
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.MIN_RANGE: attr_node_qc.weights_quantization_params[RANGE_MIN].flatten().tolist(),
+                qi_inferable_quantizers_constants.MAX_RANGE: attr_node_qc.weights_quantization_params[RANGE_MAX].flatten().tolist(),
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                }
 
     elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER, QuantizationMethod.LUT_SYM_QUANTIZER]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_w_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.CLUSTER_CENTERS: node_w_qc.weights_quantization_params[CLUSTER_CENTERS].flatten(),
-                qi_inferable_quantizers_constants.THRESHOLD: node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten(),
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_w_qc.weights_channels_axis}
-                # TODO: Add MULTIPLIER_N_BITS & EPS to node quantization config
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.LUT_VALUES: attr_node_qc.weights_quantization_params[LUT_VALUES].flatten().tolist(),
+                qi_inferable_quantizers_constants.THRESHOLD: attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten().tolist(),
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                qi_inferable_quantizers_constants.INPUT_RANK: len(attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
+                # TODO: Add LUT_VALUES_BITWIDTH & EPS to node quantization config
 
     else:
         Logger.critical(f'Not supported quantization method for weights inferable quantizers.')  # pragma: no cover
 
 
-def get_activation_inferable_quantizer_kwargs(node: BaseNode) -> Dict[str, Any]:
-    # Get the activation quantization configuration for the node
-    node_qc = node.final_activation_quantization_cfg
+def get_activation_inferable_quantizer_kwargs(node_qc: NodeActivationQuantizationConfig) -> Dict[str, Any]:
+    """
+    Get the quantization parameters for an activation inferable quantizer.
+
+    Args:
+        node_qc: The node quantization configuration of the node for which the quantizer is being created.
+            Needs to match the specific quantization target.
+
+    Returns:
+        The quantization parameters as a dictionary.
+    """
+
+    if not isinstance(node_qc, NodeActivationQuantizationConfig):
+        Logger.critical(
+            f"Non-compatible node quantization config was given for quantization target Activation.")  # pragma: no cover
+
     quantization_method = node_qc.activation_quantization_method
 
     # Return the appropriate quantization parameters based on the quantization method
     if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                QuantizationMethod.SYMMETRIC]:
         return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.activation_n_bits,
-                qi_inferable_quantizers_constants.THRESHOLD: np.asarray([node_qc.activation_quantization_params[THRESHOLD]]),
+                qi_inferable_quantizers_constants.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]],
                 qi_inferable_quantizers_constants.SIGNED: node_qc.activation_quantization_params.get(SIGNED)}
 
     elif quantization_method in [QuantizationMethod.UNIFORM]:
         return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.activation_n_bits,
-                qi_inferable_quantizers_constants.MIN_RANGE: np.asarray([node_qc.activation_quantization_params[RANGE_MIN]]),
-                qi_inferable_quantizers_constants.MAX_RANGE: np.asarray([node_qc.activation_quantization_params[RANGE_MAX]])}
+                qi_inferable_quantizers_constants.MIN_RANGE: [node_qc.activation_quantization_params[RANGE_MIN]],
+                qi_inferable_quantizers_constants.MAX_RANGE: [node_qc.activation_quantization_params[RANGE_MAX]]}
 
     elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER]:
         return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.activation_n_bits,
-                qi_inferable_quantizers_constants.CLUSTER_CENTERS: np.asarray(
-                    [node_qc.activation_quantization_params[CLUSTER_CENTERS]]),
-                qi_inferable_quantizers_constants.THRESHOLD: np.asarray(
-                    [node_qc.activation_quantization_params[THRESHOLD]]),
+                qi_inferable_quantizers_constants.LUT_VALUES: node_qc.activation_quantization_params[LUT_VALUES].flatten().tolist(),
+                qi_inferable_quantizers_constants.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]],
                 qi_inferable_quantizers_constants.SIGNED: node_qc.activation_quantization_params.get(SIGNED)}
-        # TODO: Add MULTIPLIER_N_BITS & EPS to node quantization config
+        # TODO: Add LUT_VALUES_BITWIDTH & EPS to node quantization config
     else:
         Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
 
 
-def get_weights_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
+def get_weights_quantizer_for_node(node: BaseNode, attr_name: str) -> BasePyTorchInferableQuantizer:
     """
     Get weights quantizer for a node.
 
     Args:
         node: Node to create a weight quantizer for.
+        attr_name: Attribute name to get its quantizer.
 
     Returns:
         Quantizer for the node's weights.
 
     """
     if node.final_weights_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
         # no cover
     node_w_qc = node.final_weights_quantization_cfg
-    weights_quantization_method = node_w_qc.weights_quantization_method
+    weights_quantization_method = node_w_qc.get_attr_config(attr_name).weights_quantization_method
 
     quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
                                                       weights_quantization_method,
                                                       BasePyTorchInferableQuantizer)
-    kwargs = get_weights_inferable_quantizer_kwargs(node)
+    kwargs = get_weights_inferable_quantizer_kwargs(node_w_qc, attr_name)
 
     return quantier_for_node(**kwargs)
 
 
 def get_activations_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
     """
     Get activation quantizer for a node.
@@ -130,11 +166,11 @@
         # pragma: no cover
     node_act_qc = node.final_activation_quantization_cfg
     activation_quantization_method = node_act_qc.activation_quantization_method
 
     quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
                                                        activation_quantization_method,
                                                        BasePyTorchInferableQuantizer)
-    kwargs = get_activation_inferable_quantizer_kwargs(node)
+    kwargs = get_activation_inferable_quantizer_kwargs(node_act_qc)
 
     return quantizer_for_node(**kwargs)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,12 +9,12 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType, GradientPTQConfigV2, GPTQHessianWeightsConfig
-from model_compression_toolkit.gptq.keras.quantization_facade import keras_gradient_post_training_quantization_experimental
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType, GPTQHessianScoresConfig
+from model_compression_toolkit.gptq.keras.quantization_facade import keras_gradient_post_training_quantization
 from model_compression_toolkit.gptq.keras.quantization_facade import get_keras_gptq_config
-from model_compression_toolkit.gptq.pytorch.quantization_facade import pytorch_gradient_post_training_quantization_experimental
+from model_compression_toolkit.gptq.pytorch.quantization_facade import pytorch_gradient_post_training_quantization
 from model_compression_toolkit.gptq.pytorch.quantization_facade import get_pytorch_gptq_config
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_config.py`

 * *Files 27% similar despite different names*

```diff
@@ -10,99 +10,98 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from enum import Enum
 from typing import Callable, Any, Dict
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core import common
-from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR, MAX_LSB_STR, REG_DEFAULT
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 
 
 class RoundingType(Enum):
     """
-    An enum for choosing the GPTQ rounding methods
-    0. STRAIGHT-THROUGH ESTIMATOR
-    1. SoftQuantizer
+    An enum for choosing the GPTQ rounding methods:
+
+    STE - STRAIGHT-THROUGH ESTIMATOR
+
+    SoftQuantizer - SoftQuantizer
+
     """
     STE = 0
     SoftQuantizer = 1
 
 
-class GPTQHessianWeightsConfig:
+class GPTQHessianScoresConfig:
     """
-    Configuration to use for computing the Hessian-based weights for GPTQ loss metric.
+    Configuration to use for computing the Hessian-based scores for GPTQ loss metric.
     """
 
     def __init__(self,
                  hessians_num_samples: int = 16,
-                 norm_weights: bool = True,
+                 norm_scores: bool = True,
                  log_norm: bool = True,
-                 scale_log_norm: bool = False,
-                 hessians_n_iter: int = 50):
+                 scale_log_norm: bool = False):
 
         """
         Initialize a GPTQHessianWeightsConfig.
 
         Args:
-            hessians_num_samples (int): Number of samples to use for computing the Hessian-based weights.
-            norm_weights (bool): Whether to normalize the returned weights (to get values between 0 and 1).
-            log_norm (bool): Whether to use log normalization to the GPTQ Hessian-based weights.
-            scale_log_norm (bool): Whether to scale the final vector of the Hessian weights.
-            hessians_n_iter (int): Number of random iterations to run Hessian approximation for GPTQ weights.
+            hessians_num_samples (int): Number of samples to use for computing the Hessian-based scores.
+            norm_scores (bool): Whether to normalize the returned scores of the weighted loss function (to get values between 0 and 1).
+            log_norm (bool): Whether to use log normalization for the GPTQ Hessian-based scores.
+            scale_log_norm (bool): Whether to scale the final vector of the Hessian-based scores.
         """
 
         self.hessians_num_samples = hessians_num_samples
-        self.norm_weights = norm_weights
+        self.norm_scores = norm_scores
         self.log_norm = log_norm
         self.scale_log_norm = scale_log_norm
-        self.hessians_n_iter = hessians_n_iter
 
 
 class GradientPTQConfig:
     """
-    Configuration to use for quantization with GradientPTQ (experimental).
+    Configuration to use for quantization with GradientPTQ.
     """
-
-    def __init__(self, n_iter: int,
+    def __init__(self,
+                 n_epochs: int,
                  optimizer: Any,
                  optimizer_rest: Any = None,
                  loss: Callable = None,
                  log_function: Callable = None,
                  train_bias: bool = True,
                  rounding_type: RoundingType = RoundingType.SoftQuantizer,
                  use_hessian_based_weights: bool = True,
                  optimizer_quantization_parameter: Any = None,
                  optimizer_bias: Any = None,
                  regularization_factor: float = REG_DEFAULT,
-                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
+                 hessian_weights_config: GPTQHessianScoresConfig = GPTQHessianScoresConfig(),
                  gptq_quantizer_params_override: Dict[str, Any] = None):
         """
         Initialize a GradientPTQConfig.
 
         Args:
-            n_iter (int): Number of iterations to train.
+            n_epochs (int): Number of representative dataset epochs to train.
             optimizer (Any): Optimizer to use.
             optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
             loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
              the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
              accordingly. see example in multiple_tensors_mse_loss
             log_function (Callable): Function to log information about the GPTQ process.
             train_bias (bool): Whether to update the bias during the training or not.
             rounding_type (RoundingType): An enum that defines the rounding type.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
             optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
             optimizer_bias (Any): Optimizer to override the rest optimizer for bias.
             regularization_factor (float): A floating point number that defines the regularization factor.
-            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
+            hessian_weights_config (GPTQHessianScoresConfig): A configuration that include all necessary arguments to run a computation of Hessian scores for the GPTQ loss.
             gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
 
         """
-        self.n_iter = n_iter
+
+        self.n_epochs = n_epochs
         self.optimizer = optimizer
         self.optimizer_rest = optimizer_rest
         self.loss = loss
         self.log_function = log_function
         self.train_bias = train_bias
 
         self.rounding_type = rounding_type
@@ -112,75 +111,7 @@
         self.regularization_factor = regularization_factor
         self.hessian_weights_config = hessian_weights_config
 
         self.gptq_quantizer_params_override = {} if gptq_quantizer_params_override is None \
             else gptq_quantizer_params_override
 
 
-class GradientPTQConfigV2(GradientPTQConfig):
-    """
-    Configuration to use for quantization with GradientPTQV2 (experimental).
-    """
-    def __init__(self, n_epochs: int,
-                 optimizer: Any,
-                 optimizer_rest: Any = None,
-                 loss: Callable = None,
-                 log_function: Callable = None,
-                 train_bias: bool = True,
-                 rounding_type: RoundingType = RoundingType.SoftQuantizer,
-                 use_hessian_based_weights: bool = True,
-                 optimizer_quantization_parameter: Any = None,
-                 optimizer_bias: Any = None,
-                 regularization_factor: float = REG_DEFAULT,
-                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
-                 gptq_quantizer_params_override: Dict[str, Any] = None):
-        """
-        Initialize a GradientPTQConfigV2.
-
-        Args:
-            n_epochs (int): Number of representative dataset epochs to train.
-            optimizer (Any): Optimizer to use.
-            optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
-            loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
-             the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
-             accordingly. see example in multiple_tensors_mse_loss
-            log_function (Callable): Function to log information about the GPTQ process.
-            train_bias (bool): Whether to update the bias during the training or not.
-            rounding_type (RoundingType): An enum that defines the rounding type.
-            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
-            optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
-            optimizer_bias (Any): Optimizer to override the rest optimizerfor bias.
-            regularization_factor (float): A floating point number that defines the regularization factor.
-            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
-            gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
-
-        """
-
-        super().__init__(n_iter=None,
-                         optimizer=optimizer,
-                         optimizer_rest=optimizer_rest,
-                         loss=loss,
-                         log_function=log_function,
-                         train_bias=train_bias,
-                         rounding_type=rounding_type,
-                         use_hessian_based_weights=use_hessian_based_weights,
-                         optimizer_quantization_parameter=optimizer_quantization_parameter,
-                         optimizer_bias=optimizer_bias,
-                         regularization_factor=regularization_factor,
-                         hessian_weights_config=hessian_weights_config,
-                         gptq_quantizer_params_override=gptq_quantizer_params_override)
-        self.n_epochs = n_epochs
-
-    @classmethod
-    def from_v1(cls, n_ptq_iter: int, config_v1: GradientPTQConfig):
-        """
-        Initialize a GradientPTQConfigV2 from GradientPTQConfig instance.
-
-        Args:
-            n_ptq_iter (int): Number of PTQ calibration iters (length of representative dataset).
-            config_v1 (GradientPTQConfig): A GPTQ config to convert to V2.
-
-        """
-        n_epochs = int(round(config_v1.n_iter) / n_ptq_iter)
-        v1_params = config_v1.__dict__
-        v1_params = {k: v for k, v in v1_params.items() if k != 'n_iter'}
-        return cls(n_epochs, **v1_params)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_constants.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_constants.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_framework_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_framework_implementation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_graph.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/common/gptq_graph.py`

 * *Files 7% similar despite different names*

```diff
@@ -35,15 +35,17 @@
         A list of nodes std collected from BatchNorms in the graph
     """
     compare_points = []
     compare_points_mean = []
     compare_points_std = []
     compare_points_name = []
     for n in input_graph.get_topo_sorted_nodes():
-        if len(n.weights) > 0 and n.is_weights_quantization_enabled() and not n.reuse:
+        # only nodes with kernel attribute are currently trained with GPTQ and are used as compare points
+        kernel_attr = input_graph.fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr) and not n.reuse:
             compare_points.append(n)
             compare_points_name.append(n.name)
             compare_points_std.append(n.prior_info.std_output)
             compare_points_mean.append(n.prior_info.mean_output)
     return compare_points, compare_points_name, compare_points_mean, compare_points_std
 
 
@@ -56,11 +58,11 @@
         fw_info: A FrameworkInfo object.
 
     Returns: The name of the kernel attribute.
 
     """
     kernel_attribute = fw_info.get_kernel_op_attributes(layer_type)
     if len(kernel_attribute) != 1:
-        Logger.error(  # pragma: no cover
-            f"In GPTQ training only the kernel weights attribute should be trained, but number of kernel "
-            f"attributes is {len(kernel_attribute)}.")
+        Logger.critical(  # pragma: no cover
+            f"In GPTQ training, only the kernel weights attribute should be trained. "
+            f"However, the number of kernel attributes is {len(kernel_attribute)}.")
     return kernel_attribute[0]
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/common/gptq_training.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_training.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,288 +8,306 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
-from abc import ABC, abstractmethod
+from typing import Callable, List, Tuple, Union
+
 import numpy as np
-from typing import Callable, List, Any
+from torch.nn import Module
+from tqdm import tqdm
+import copy
+import torch
+
+from model_compression_toolkit.core.common.hessian import HessianInfoService
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
+from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
 from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR
-from model_compression_toolkit.gptq.common.gptq_framework_implementation import GPTQFrameworkImplemantation
-from model_compression_toolkit.gptq.common.gptq_graph import get_compare_points
-from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
-from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.pytorch.constants import BIAS
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, set_model, torch_tensor_to_numpy
+from model_compression_toolkit.gptq.pytorch.graph_info import get_gptq_trainable_parameters, \
+    get_weights_for_loss
+from model_compression_toolkit.gptq.pytorch.quantizer.quantization_builder import quantization_builder
+from model_compression_toolkit.gptq.pytorch.quantizer.regularization_factory import get_regularization
+from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
 
 
-class GPTQTrainer(ABC):
+class PytorchGPTQTrainer(GPTQTrainer):
     """
-    Abstract GPTQ training class for fine-tuning a quantized model
+    Pytorch GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
                  gptq_config: GradientPTQConfig,
-                 fw_impl: GPTQFrameworkImplemantation,
-                 fw_info: FrameworkInfo):
+                 fw_impl: FrameworkImplementation,
+                 fw_info: FrameworkInfo,
+                 representative_data_gen: Callable,
+                 hessian_info_service: HessianInfoService = None):
         """
         Build two models from a graph: A teacher network (float model) and a student network (quantized model).
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
-            gptq_config: GradientPTQConfig with parameters about the tuning process.
-            fw_impl: Framework implementation
+            gptq_config: GradientPTQConfigV2 with parameters about the tuning process.
+            fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             fw_info: Framework information
+            representative_data_gen: Dataset to use for inputs of the models.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
         """
-        self.graph_float = copy.deepcopy(graph_float)
-        self.graph_quant = copy.deepcopy(graph_quant)
-        self.gptq_config = gptq_config
-        self.fw_impl = fw_impl
-        self.fw_info = fw_info
-
-        # ----------------------------------------------
-        # Build two models and create compare nodes
-        # ----------------------------------------------
-        self.compare_points, _, self.compare_points_mean, self.compare_points_std = get_compare_points(self.graph_float)
-
-        self.float_model, self.float_user_info = fw_impl.model_builder(self.graph_float,
-                                                                       mode=ModelBuilderMode.FLOAT,
-                                                                       append2output=self.compare_points,
-                                                                       fw_info=self.fw_info)
-
-        self.fxp_model, self.gptq_user_info = self.build_gptq_model()
-
-    def get_optimizer_with_param(self,
-                                 flattened_trainable_weights: List[Any],
-                                 flattened_bias_weights: List[Any],
-                                 trainable_quantization_parameters: List[Any]) -> List[Any]:
-        """
-        Create Optimizers with their trainable parameters
-        Args:
-            flattened_trainable_weights: list of trainable weights parameters (flattened)
-            flattened_bias_weights: list of trainable bias parameters (flattened)
-            trainable_quantization_parameters: list of trainable quantization parameters
-        Returns:
-            List of Optimizer objects with parameters
-        """
+        super().__init__(graph_float,
+                         graph_quant,
+                         gptq_config,
+                         fw_impl,
+                         fw_info,
+                         hessian_info_service=hessian_info_service)
+
+        self.loss_list = []
+        self.input_scale = 1
+        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
+            Logger.critical("Input scale mismatch between float and GPTQ networks. "
+                            "Ensure both networks have matching input scales.")  # pragma: no cover
+        else:
+            self.input_scale = self.gptq_user_info.input_scale
 
-        w2train = [*flattened_trainable_weights]
+        trainable_weights, trainable_bias, trainable_threshold = get_gptq_trainable_parameters(
+            self.fxp_model,
+            add_bias=self.gptq_config.train_bias)
 
-        quant_params_learning = self.gptq_config.gptq_quantizer_params_override.get(QUANT_PARAM_LEARNING_STR, False)
+        self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
+        if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
+                self.fxp_weights_list)):
+            Logger.critical("GPTQ: Number of comparison points, layers with trainable weights, "
+                            "and float vs. quantized weights for loss calculation do not match. "
+                            "Verify consistency across these parameters for successful GPTQ training.")
 
-        optimizer_with_param = [(self.gptq_config.optimizer, w2train)]
-        if self.gptq_config.train_bias or quant_params_learning:
-            w2train_res = []
-            if self.gptq_config.train_bias:
-                if self.gptq_config.optimizer_bias is not None:
-                    optimizer_with_param.append((self.gptq_config.optimizer_bias, flattened_bias_weights))
-                else:
-                    w2train_res.extend(flattened_bias_weights)
-                    if self.gptq_config.optimizer_rest is None:
-                        Logger.error(  # pragma: no cover
-                            "To enable bias micro training an additional optimizer is required, please define the optimizer_rest")
-            if quant_params_learning:
-                if self.gptq_config.optimizer_quantization_parameter is not None:  # Ability to override optimizer
-                    optimizer_with_param.append((self.gptq_config.optimizer_quantization_parameter,
-                                                 trainable_quantization_parameters))
-                else:
-                    w2train_res.extend(trainable_quantization_parameters)
-                if self.gptq_config.optimizer_rest is None:
-                    Logger.error(  # pragma: no cover
-                        "To enable quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
-            if len(w2train_res) > 0:
-                # Either bias or quantization parameters are trainable but did not provide a specific optimizer,
-                # so we should use optimizer_rest to train them
-                if self.gptq_config.optimizer_rest is None:
-                    Logger.error(  # pragma: no cover
-                        "To enable bias or quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
-                optimizer_with_param.append((self.gptq_config.optimizer_rest, w2train_res))
+        self.optimizer_with_param = self.get_optimizer_with_param(trainable_weights,
+                                                                  trainable_bias,
+                                                                  trainable_threshold)
 
-        return optimizer_with_param
+        self.weights_for_average_loss = to_torch_tensor(self.compute_hessian_based_weights())
 
+        self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
-    def compute_hessian_based_weights(self,
-                                      representative_data_gen: Callable) -> np.ndarray:
+    def _is_gptq_weights_trainable(self,
+                                   node: BaseNode) -> bool:
         """
-        Computes the Hessian-based weights using the framework's model_grad method per batch of images.
+        A function for deciding if a layer should be fine-tuned during GPTQ.
 
         Args:
-            representative_data_gen: Dataset used for inference to compute the Hessian-based weights.
+            node (BaseNode): Node for quantization decision
 
-        Returns: A vector of weights, one for each compare point,
-        to be used for the loss metric weighted average computation when running GPTQ training.
+        Returns:
+            A boolean whether the layer is to be wrapped with a Quantization Wrapper.
         """
-        if self.gptq_config.use_hessian_based_weights:
-            images = self._generate_images_batch(representative_data_gen,
-                                                 self.gptq_config.hessian_weights_config.hessians_num_samples)
-
-            model_output_replacement = self._get_model_output_replacement()
-
-            points_apprx_jacobians_weights = []
-            for i in range(1, images.shape[0] + 1):
-                Logger.info(f"Computing Jacobian-based weights approximation for image sample {i} out of {images.shape[0]}...")
-                # Note that in GPTQ loss weights computation we assume that there aren't replacement output nodes,
-                # therefore, output_list is just the graph outputs, and we don't need the tuning factor for
-                # defining the output weights (since the output layer is not a compare point).
-                image_ip_gradients = self.fw_impl.model_grad(self.graph_float,
-                                                             {inode: self.fw_impl.to_tensor(images[i - 1:i]) for inode
-                                                              in
-                                                              self.graph_float.get_inputs()},
-                                                             self.compare_points,
-                                                             output_list=model_output_replacement,
-                                                             all_outputs_indices=[],
-                                                             alpha=0,
-                                                             norm_weights=self.gptq_config.hessian_weights_config.norm_weights,
-                                                             n_iter=self.gptq_config.hessian_weights_config.hessians_n_iter)
-                points_apprx_jacobians_weights.append(image_ip_gradients)
-            if self.gptq_config.hessian_weights_config.log_norm:
-                mean_jacobian_weights = np.mean(points_apprx_jacobians_weights, axis=0)
-                mean_jacobian_weights = np.where(mean_jacobian_weights != 0, mean_jacobian_weights,
-                                                 np.partition(mean_jacobian_weights, 1)[1])
-                log_weights = np.log10(mean_jacobian_weights)
-
-                if self.gptq_config.hessian_weights_config.scale_log_norm:
-                    return (log_weights - np.min(log_weights)) / (np.max(log_weights) - np.min(log_weights))
-
-                return log_weights - np.min(log_weights)
-            else:
-                return np.mean(points_apprx_jacobians_weights, axis=0)
-        else:
-            num_nodes = len(self.compare_points)
-            return np.asarray([1 / num_nodes for _ in range(num_nodes)])
 
-    @staticmethod
-    def _generate_images_batch(representative_data_gen: Callable, num_samples_for_loss: int) -> np.ndarray:
+        kernel_attr = self.fw_info.get_kernel_op_attributes(node.type)[0]
+        return kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)
+
+    def gptq_wrapper(self,
+                     n: BaseNode,
+                     layer: Module) -> Union[PytorchQuantizationWrapper, Module]:
         """
-        Construct batches of image samples for inference.
+        A function which takes a computational graph node and a pytorch layer and perform the quantization wrapping.
 
         Args:
-            representative_data_gen: A callable method to retrieve images from Dataset.
-            num_samples_for_loss: Num of total images for evaluation.
+            n: A node of mct graph.
+            layer: A pytorch layer
 
-        Returns: A tensor of images batches
+        Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
         """
-        # First, select images to use for all measurements.
-        samples_count = 0  # Number of images we used so far to compute the distance matrix.
-        images = []
-        for inference_batch_input in representative_data_gen():
-            if samples_count >= num_samples_for_loss:
-                break
-            num_images = inference_batch_input[0].shape[0]
-
-            # If we sampled more images than we should,
-            # we take only a subset of these images and use only them.
-            if num_images > num_samples_for_loss - samples_count:
-                inference_batch_input = [x[:num_samples_for_loss - samples_count] for x in inference_batch_input]
-                assert num_samples_for_loss - samples_count == inference_batch_input[0].shape[0]
-                num_images = num_samples_for_loss - samples_count
 
-            images.append(inference_batch_input[0])
-            samples_count += num_images
-        else:
-            if samples_count < num_samples_for_loss:
-                Logger.warning(f'Not enough images in representative dataset to generate {num_samples_for_loss} data points, '
-                               f'only {samples_count} were generated')
+        if self._is_gptq_weights_trainable(n):
+            # If we are here, then the node has a kernel attribute to quantize and training during GPTQ
+            weights_quantizers, _ = quantization_builder(n,
+                                                         self.gptq_config,
+                                                         self.fw_info.get_kernel_op_attributes(n.type)[0])
 
-        return np.concatenate(images, axis=0)
+            if len(weights_quantizers) > 0:
+                return PytorchQuantizationWrapper(layer,
+                                                  weights_quantizers=weights_quantizers)
 
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return layer
+
+    def get_activation_quantizer_holder(self, n: BaseNode) -> Callable:
+        """
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
+        If the layer is not supposed to be wrapped with an activation quantizer - return None.
+        Args:
+            n: Node to attach a PytorchActivationQuantizationHolder to its output.
+        Returns:
+            A PytorchActivationQuantizationHolder module for the node's activation quantization.
+        """
+        _, activation_quantizers = quantization_builder(n, self.gptq_config)
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node we no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        Logger.critical(f"'PytorchActivationQuantizationHolder' requires exactly one quantizer, "
+                        f"but {len(activation_quantizers)} were found for node {n.name}. "
+                        f"Ensure the node is configured with a single activation quantizer.")
 
-    @abstractmethod
     def build_gptq_model(self):
         """
         Build the GPTQ model with QuantizationWrappers
         Returns:
             Quantized graph for GPTQ fine-tuning, GPTQ graph user info
         """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s GPTQ model builder method.')  # pragma: no cover
+        gptq_model, gptq_user_info = PyTorchModelBuilder(graph=self.graph_quant,
+                                                         append2output=self.compare_points,
+                                                         fw_info=self.fw_info,
+                                                         wrapper=self.gptq_wrapper,
+                                                         return_float_outputs=True,
+                                                         get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder).build_model()
+
+        return gptq_model, gptq_user_info
 
-    @abstractmethod
     def train(self, representative_data_gen: Callable):
         """
-        Train the quantized model using GPTQ training process
-        Args:
-            representative_data_gen: Dataset to use for inputs of the models.
-        """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s train method.')  # pragma: no cover
+          GPTQ Training using pytorch framework
+          Args:
+              representative_data_gen: Dataset generator to get images.
+          Returns:
+              Graph after GPTQ training
+          """
+        # Set Optimizers
+        for (optimizer, params) in self.optimizer_with_param:
+            optimizer.param_groups.clear()
+            optimizer.add_param_group({'params': params})
+
+        # Set models mode
+        set_model(self.float_model, False)
+        set_model(self.fxp_model, True)
+        self._set_requires_grad()
 
-    @abstractmethod
-    def update_graph(self) -> Graph:
+        # ----------------------------------------------
+        # Training loop
+        # ----------------------------------------------
+        self.micro_training_loop(representative_data_gen, self.gptq_config.n_epochs)
+
+    def compute_gradients(self,
+                          y_float: List[torch.Tensor],
+                          input_tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, List[np.ndarray]]:
         """
-        Update a graph using GPTQ after minimizing the loss between the float model's output
-        and the quantized model's outputs.
+        Get outputs from both teacher and student networks. Compute the observed error,
+        and use it to compute the gradients and applying them to the student weights.
+        Args:
+            y_float: A list of reference tensor from the floating point network.
+            input_tensors: A list of Input tensors to pass through the networks.
         Returns:
-            Updated graph after GPTQ.
+            Loss and gradients.
         """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s update_graph method.')  # pragma: no cover
 
-    def _get_model_output_replacement(self) -> List[BaseNode]:
-        """
-        If a model's output node is not compatible for the task of gradients computation we need to find a predecessor
-        node in the model's graph representation which is compatible and use it for the gradients' computation.
-        This method searches for this predecessor node for each output of the model.
+        # Forward-pass
+        y_fxp = self.fxp_model(input_tensors)
 
-        Returns: A list of output replacement nodes.
+        # Loss
+        loss_value = self.gptq_config.loss(y_fxp,
+                                           y_float,
+                                           self.fxp_weights_list,
+                                           self.flp_weights_list,
+                                           self.compare_points_mean,
+                                           self.compare_points_std,
+                                           self.weights_for_average_loss)
 
-        """
+        reg_value = self.reg_func(self.fxp_model, self.gptq_config.regularization_factor)
 
-        replacement_outputs = []
-        for n in self.graph_float.get_outputs():
-            prev_node = n.node
-            while not self.fw_impl.is_node_compatible_for_metric_outputs(prev_node):
-                prev_node = self.graph_float.get_prev_nodes(n.node)
-                assert len(prev_node) == 1, "A none compatible output node has multiple inputs, " \
-                                            "which is incompatible for metric computation."
-                prev_node = prev_node[0]
-            replacement_outputs.append(prev_node)
-        return replacement_outputs
+        loss_value += reg_value
 
+        # Back-pass
+        loss_value.backward()
 
-def gptq_training(graph_float: Graph,
-                  graph_quant: Graph,
-                  gptq_config: GradientPTQConfig,
-                  representative_data_gen: Callable,
-                  fw_impl: GPTQFrameworkImplemantation,
-                  fw_info: FrameworkInfo) -> Graph:
-    """
-    GPTQ training process using knowledge distillation with a teacher network (float model) and a student network (quantized model).
-    Args:
-        graph_float: Graph to build a float networks from.
-        graph_quant: Graph to build a quantized networks from.
-        gptq_config: GradientPTQConfig with parameters about the tuning process.
-        representative_data_gen: Dataset to use for inputs of the models.
-        fw_impl: Framework implementation
-        fw_info: Framework information
+        # Get gradients
+        grads = []
+        for param in self.fxp_model.parameters():
+            if param.requires_grad and param.grad is not None:
+                grads.append(torch_tensor_to_numpy(param.grad))
 
-    Returns:
-        Quantized graph for export
+        return loss_value, grads
 
-    """
-    # Get GPTQ object and initialize it
-    gptq_trainer_obj = fw_impl.get_gptq_trainer_obj()
-
-    gptq_trainer = gptq_trainer_obj(graph_float,
-                                    graph_quant,
-                                    gptq_config,
-                                    fw_impl,
-                                    fw_info,
-                                    representative_data_gen)
-
-    # Training process
-    gptq_trainer.train(representative_data_gen)
+    def micro_training_loop(self,
+                            data_function: Callable,
+                            n_epochs: int):
+        """
+        This function run a micro training loop on given set of parameters.
+        Args:
+            data_function: A callable function that give a batch of samples.
+            n_epochs: Number of update iterations of representative dataset.
+        """
+        for _ in tqdm(range(n_epochs)):
+            for data in tqdm(data_function()):
+                input_data = [d * self.input_scale for d in data]
+                input_tensor = to_torch_tensor(input_data)
+                y_float = self.float_model(input_tensor)  # running float model
+                loss_value, grads = self.compute_gradients(y_float, input_tensor)
+                # Run one step of gradient descent by updating the value of the variables to minimize the loss.
+                for (optimizer, _) in self.optimizer_with_param:
+                    optimizer.step()
+                    optimizer.zero_grad()
+                if self.gptq_config.log_function is not None:
+                    self.gptq_config.log_function(loss_value.item(),
+                                                  torch_tensor_to_numpy(grads),
+                                                  torch_tensor_to_numpy(self.optimizer_with_param[0][-1]))
+                self.loss_list.append(loss_value.item())
+                Logger.debug(f'last loss value: {self.loss_list[-1]}')
 
-    # Update graph
-    graph_quant = gptq_trainer.update_graph()
+    def update_graph(self) -> Graph:
+        """
+        Update a graph using GPTQ after minimizing the loss between the float model's output
+        and the quantized model's outputs.
+        Returns:
+            Updated graph after GPTQ.
+        """
+        graph_quant = copy.copy(self.graph_quant)
 
-    return graph_quant
+        # Update graph after training
+        for name, layer in self.fxp_model.named_modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                node = self.graph_quant.find_node_by_name(name)
+                if len(node) != 1:
+                    Logger.critical(f"Cannot update GPTQ graph: Layer with name '{name}' is missing or not unique. "
+                                    f"Ensure each layer has a unique name and exists within the graph for updates.")
+                node = node[0]
+                kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
+                                                                      fw_info=self.fw_info)
+                weights, weight_quant_config, activation_quant_config = \
+                    layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
+                for weight_attr, weight in weights.items():
+                    node.set_weights_by_keys(weight_attr, self.fw_impl.to_numpy(weight))
+                for config_attr, config_value in weight_quant_config.items():
+                    node.final_weights_quantization_cfg.set_quant_config_attr(config_attr, config_value)
+                for config_attr, config_value in activation_quant_config.items():
+                    node.final_activation_quantization_cfg.set_quant_config_attr(config_attr, config_value)
+                if self.gptq_config.train_bias and hasattr(layer.layer, BIAS):
+                    node.set_weights_by_keys(BIAS, self.fw_impl.to_numpy(getattr(layer.layer, BIAS)))
+
+        return graph_quant
+
+    def _set_requires_grad(self):
+        """
+        Set require_grad flag for trainable parameters for GPTQ training
+        """
+        # Float model: freeze all the parameters in the network
+        for param in self.float_model.parameters():
+            param.requires_grad = False
+
+        # Fxp model: unfreeze bias trainable parameters
+        for layer in self.fxp_model.modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                if hasattr(layer.layer, BIAS):
+                    bias = getattr(layer.layer, BIAS)
+                    bias.requires_grad = self.gptq_config.train_bias
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_loss.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/gptq_training.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/gptq_training.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,30 +16,32 @@
 
 import tensorflow as tf
 from keras import Model
 from packaging import version
 from tensorflow.keras.layers import Layer
 from tqdm import tqdm
 
+from model_compression_toolkit.core.common.hessian import HessianInfoService
 # As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.gptq.keras.quantizer.quantization_builder import quantization_builder
 from model_compression_toolkit.logger import Logger
-from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder
+from mct_quantizers import KerasActivationQuantizationHolder
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.engine.base_layer import TensorFlowOpLayer
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.base_layer import TensorFlowOpLayer
 else:
     from keras.engine.base_layer import TensorFlowOpLayer
 
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.core import common
 from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.gptq.keras.graph_info import get_weights_for_loss, get_gptq_trainable_parameters
 from model_compression_toolkit.gptq.keras.quantizer.regularization_factory import get_regularization
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 import numpy as np
 import copy
@@ -50,108 +52,115 @@
     """
     Keras GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
-                 gptq_config: GradientPTQConfigV2,
+                 gptq_config: GradientPTQConfig,
                  fw_impl: FrameworkImplementation,
                  fw_info: FrameworkInfo,
-                 representative_data_gen: Callable):
+                 representative_data_gen: Callable,
+                 hessian_info_service: HessianInfoService = None):
         """
         Build two models from a graph: A teacher network (float model) and a student network (quantized model).
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
             gptq_config: GradientPTQConfig with parameters about the tuning process.
             fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             fw_info: Framework information.
             representative_data_gen: Dataset to use for inputs of the models.
+            hessian_info_service: HessianInfoService for fetching and computing Hessian's trace approximation.
+
         """
         super().__init__(graph_float,
                          graph_quant,
                          gptq_config,
                          fw_impl,
-                         fw_info)
+                         fw_info,
+                         hessian_info_service=hessian_info_service)
 
         self.loss_list = []
         self.input_scale = 1
 
         trainable_weights, bias_weights, trainable_threshold = get_gptq_trainable_parameters(
             self.fxp_model,
             fw_info,
             add_bias=gptq_config.train_bias)
 
         self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
 
         if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
                 self.fxp_weights_list)):
-            raise Exception(
-                "GPTQ: Mismatch between number of compare points, number of layers with trainable weights " +
-                "and number of float and quantized weights for loss")
+            Logger.critical("Mismatch in the number of comparison points, layers with trainable weights, "
+                            "and the number of float and quantized weights for loss calculation. "
+                            "Ensure all these elements align to proceed with GPTQ training.")
 
         flattened_trainable_weights = [w for layer_weights in trainable_weights for w in layer_weights]
         flattened_bias_weights = [w for layer_weights in bias_weights for w in layer_weights]
         trainable_quantization_parameters = trainable_threshold
         self.optimizer_with_param = self.get_optimizer_with_param(flattened_trainable_weights,
                                                                   flattened_bias_weights,
                                                                   trainable_quantization_parameters)
         self.has_params_to_train = np.sum(
             [len(optimizer_params_tuple[1]) for optimizer_params_tuple in self.optimizer_with_param]) > 0
 
         if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
-            Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
+            Logger.critical("Input scale mismatch detected between the float model and the GPTQ model. "
+                            "Confirm that the input scales for both models are correctly configured and aligned.")  # pragma: no cover
         else:
             self.input_scale = self.gptq_user_info.input_scale
 
-        self.weights_for_average_loss = self.compute_hessian_based_weights(representative_data_gen)
+        self.weights_for_average_loss = self.compute_hessian_based_weights()
 
         self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
     def _is_gptq_weights_trainable(self,
                                    node: common.BaseNode) -> bool:
         """
         A function for deciding if a layer should be fine-tuned during GPTQ.
 
         Args:
             node (BaseNode): Node for quantization decision
 
         Returns:
             A boolean whether the layer is to be wrapped with a QuantizeWrapper
         """
-
-        if node.is_weights_quantization_enabled() and not self.fw_info.is_kernel_op(node.type):
-            Logger.error(f"GPTQ Error: Quantizing node {node.name} of type {node.type} "
-                                f"without a kernel isn't supported")
-        return node.is_weights_quantization_enabled()
+        kernel_attr = self.fw_info.get_kernel_op_attributes(node.type)[0]
+        return kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)
 
     def gptq_wrapper(self,
                      n: common.BaseNode,
-                     layer: Layer) -> Union[KerasQuantizationWrapper, Layer]:
+                     layer: Layer) -> Union[KerasTrainableQuantizationWrapper, Layer]:
         """
         A function which takes a computational graph node and a keras layer and perform the quantization wrapping.
 
         Args:
             n: A node of mct graph.
             layer: A keras layer
 
         Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
 
         """
         if self._is_gptq_weights_trainable(n):
+            # If we are here, then the node has a kernel attribute to quantize and training during GPTQ
             weights_quantizers, _ = quantization_builder(n,
-                                                         self.gptq_config) # TODO: split quantizers building into two functions: for weights and activations
+                                                         self.gptq_config,  # TODO: split quantizers building into two functions: for weights and activations
+                                                         self.fw_info.get_kernel_op_attributes(n.type)[0])
             if len(weights_quantizers) > 0:
-                return KerasQuantizationWrapper(layer,
-                                                   weights_quantizers=weights_quantizers)
+                return KerasTrainableQuantizationWrapper(layer,
+                                                         weights_quantizers=weights_quantizers)
+
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
         return layer
 
     def get_activation_quantizer_holder(self, n: common.BaseNode) -> Callable:
         """
         Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
         If the layer is not supposed to be wrapped with activation quantizers - return None.
 
@@ -165,17 +174,17 @@
 
         # Holder by definition uses a single quantizer for the activation quantization
         # thus we make sure this is the only possible case (unless it's a node with no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
             return KerasActivationQuantizationHolder(activation_quantizers[0])
 
-        Logger.error(
-            f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
-            f'were found for node {n}')
+        Logger.critical(f"'KerasActivationQuantizationHolder' is designed to support a single quantizer, "
+                        f"but {len(activation_quantizers)} quantizers were found for node '{n}'. "
+                        f"Ensure only one quantizer is configured for each node's activation.")
 
 
     def build_gptq_model(self) -> Tuple[Model, UserInformation]:
         """
         Build the GPTQ model with QuantizationWrappers
 
         Returns:
@@ -314,20 +323,21 @@
         and the quantized model's outputs.
         Returns:
             Updated graph after GPTQ.
         """
         graph = copy.copy(self.graph_quant)
 
         for layer in self.fxp_model.layers:
-            if isinstance(layer, KerasQuantizationWrapper):
+            if isinstance(layer, KerasTrainableQuantizationWrapper):
                 node = graph.find_node_by_name(layer.layer.name)
                 if len(node) == 0 and isinstance(layer.layer, TensorFlowOpLayer):
                     node = graph.find_node_by_name('_'.join(layer.layer.name.split('_')[3:]))
                 if len(node) != 1:
-                    Logger.error(f"Can't update GPTQ graph due to missing layer named: {layer.layer.name}")
+                    Logger.critical(f"Unable to update the GPTQ graph because the layer named '{layer.layer.name}' could not be found. "
+                                    f"Verify that the layer names in the GPTQ model match those in the graph.")
                 node = node[0]
                 kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
                                                                       fw_info=self.fw_info)
                 weights, weight_quant_config, activation_quant_config = \
                     layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
                 for weight_attr, weight in weights.items():
                     node.set_weights_by_keys(weight_attr, weight.numpy())
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/graph_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/graph_info.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from typing import Tuple, List
 from model_compression_toolkit.core.keras.constants import USE_BIAS
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from tensorflow.keras.models import Model
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.logger import Logger
-from mct_quantizers import KerasQuantizationWrapper
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
 def get_gptq_trainable_parameters(fxp_model: Model,
                                   fw_info: FrameworkInfo,
                                   add_bias: bool = False) -> (
         List[tf.Variable], List[tf.Variable], List[tf.Variable]):
@@ -42,21 +42,21 @@
     """
 
     trainable_weights: List[tf.Tensor] = []
     trainable_threshold: List[tf.Tensor] = []
     bias_weights: List[List[tf.Tensor]] = []
 
     for layer in fxp_model.layers:
-        if isinstance(layer, KerasQuantizationWrapper):
+        if isinstance(layer, KerasTrainableQuantizationWrapper):
             kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
                                                                   fw_info=DEFAULT_KERAS_INFO)
 
             # collect trainable weights per quantizer
             if kernel_attribute not in layer.weights_quantizers:
-                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+                Logger.critical(f"'{kernel_attribute}' was not found in the weight quantizers of layer '{layer.layer}'.")
 
             quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
             quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
             trainable_weights.append(quantizer_trainable_weights)
             trainable_threshold.extend(quantizer_trainable_threshold)
 
             if add_bias:
@@ -80,15 +80,15 @@
         A list of float kernels, each item is the float kernel of the layer
         A list of quantized kernels, each item is the quantized kernel of the layer
     """
 
     flp_weights_list = []
     fxp_weights_list = []
     for layer in fxp_model.layers:
-        if isinstance(layer, KerasQuantizationWrapper):
+        if isinstance(layer, KerasTrainableQuantizationWrapper):
 
             # collect pairs of float and quantized weights per layer
             _layer_flp_weights, _layer_fxp_weights = [], []
             for weight, quantizer_vars, quantizer in layer.get_weights_vars():
                 _layer_flp_weights.append(quantizer_vars)
                 _layer_fxp_weights.append(quantizer(training=False, inputs=quantizer_vars))
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantization_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantization_facade.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,23 +12,25 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable, Tuple
 from packaging import version
 
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
+from model_compression_toolkit.core.runner import core_runner
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 
 LR_DEFAULT = 0.15
 LR_REST_DEFAULT = 1e-4
@@ -46,36 +48,40 @@
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
     from model_compression_toolkit import get_target_platform_capabilities
 
     # As from TF2.9 optimizers package is changed
     if version.parse(tf.__version__) < version.parse("2.9"):
         from keras.optimizer_v2.optimizer_v2 import OptimizerV2
-    else:
+    elif version.parse(tf.__version__) < version.parse("2.12"):
         from keras.optimizers.optimizer_v2.optimizer_v2 import OptimizerV2
+    else:
+        from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2
 
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
     def get_keras_gptq_config(n_epochs: int,
                               optimizer: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_DEFAULT),
                               optimizer_rest: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_REST_DEFAULT),
                               loss: Callable = GPTQMultipleTensorsLoss(),
                               log_function: Callable = None,
-                              use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
+                              use_hessian_based_weights: bool = True,
+                              regularization_factor: float = REG_DEFAULT) -> GradientPTQConfig:
         """
         Create a GradientPTQConfigV2 instance for Keras models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (OptimizerV2): Keras optimizer to use for fine-tuning for auxiliry variable with a default learning rate set to 0.2.
             optimizer_rest (OptimizerV2): Keras optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
+            regularization_factor (float): A floating point number that defines the regularization factor.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and TensorFlow:
@@ -92,59 +98,55 @@
             >>> gptq_conf = mct.gptq.get_keras_gptq_config(n_epochs=3, optimizer=tf.keras.optimizers.Nadam())
 
             The configuration can be passed to :func:`~model_compression_toolkit.keras_post_training_quantization` in order to quantize a keras model using gptq.
 
         """
         bias_optimizer = tf.keras.optimizers.SGD(learning_rate=LR_BIAS_DEFAULT,
                                                  momentum=GPTQ_MOMENTUM)
-        return GradientPTQConfigV2(n_epochs,
-                                   optimizer,
-                                   optimizer_rest=optimizer_rest,
-                                   loss=loss,
-                                   log_function=log_function,
-                                   train_bias=True,
-                                   optimizer_bias=bias_optimizer,
-                                   use_hessian_based_weights=use_hessian_based_weights)
-
-
-    def keras_gradient_post_training_quantization_experimental(in_model: Model,
-                                                               representative_data_gen: Callable,
-                                                               gptq_config: GradientPTQConfigV2,
-                                                               gptq_representative_data_gen: Callable = None,
-                                                               target_kpi: KPI = None,
-                                                               core_config: CoreConfig = CoreConfig(),
-                                                               fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                                               target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                               new_experimental_exporter: bool = True) -> Tuple[Model, UserInformation]:
+        return GradientPTQConfig(n_epochs,
+                                 optimizer,
+                                 optimizer_rest=optimizer_rest,
+                                 loss=loss,
+                                 log_function=log_function,
+                                 train_bias=True,
+                                 optimizer_bias=bias_optimizer,
+                                 use_hessian_based_weights=use_hessian_based_weights,
+                                 regularization_factor=regularization_factor)
+
+
+    def keras_gradient_post_training_quantization(in_model: Model, representative_data_gen: Callable,
+                                                  gptq_config: GradientPTQConfig,
+                                                  gptq_representative_data_gen: Callable = None,
+                                                  target_resource_utilization: ResourceUtilization = None,
+                                                  core_config: CoreConfig = CoreConfig(),
+                                                  target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> Tuple[Model, UserInformation]:
         """
         Quantize a trained Keras model using post-training quantization. The model is quantized using a
         symmetric constraint quantization thresholds (power of two).
         The model is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
         For each possible bit width (per layer) a threshold is then being calculated using the collected
         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
         (both coefficients and activations by default).
-        In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
+        In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
         is set (in bytes).
         Then, the quantized weights are optimized using gradient based post
         training quantization by comparing points between the float and quantized models, and minimizing the observed
         loss.
 
         Args:
             in_model (Model): Keras model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
+            gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
-            target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
 
             A quantized model and information the user may need to handle the quantized model.
 
         Examples:
 
@@ -166,89 +168,75 @@
 
             >>> config = mct.core.CoreConfig()
 
             If mixed precision is desired, create an MCT core config with a mixed-precision configuration, to quantize a model
             with different bitwidths for different layers.
             The candidates bitwidth for quantization should be defined in the target platform model:
 
-            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=1))
 
-            For mixed-precision set a target KPI object:
-            Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
+            For mixed-precision set a target resource utilization object:
+            Create a resource utilization object to limit our returned model's size. Note that this value affects only coefficients
             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
             while the bias will not):
 
-            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+            >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
             Create GPTQ config:
 
             >>> gptq_config = mct.gptq.get_keras_gptq_config(n_epochs=1)
 
             Pass the model with the representative dataset generator to get a quantized model:
 
-            >>> quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization_experimental(model, repr_datagen, gptq_config, target_kpi=kpi, core_config=config)
+            >>> quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization(model, repr_datagen, gptq_config, target_resource_utilization=ru, core_config=config)
 
         """
         KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
+                             fw_info=DEFAULT_KERAS_INFO).validate()
 
         if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config for mixed-precision is not of type 'MixedPrecisionQuantizationConfig'. "
+                                "Ensure usage of the correct API for keras_post_training_quantization "
+                                "or provide a valid mixed-precision configuration.")  # pragma: no cover
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
+        tb_w = init_tensorboard_writer(DEFAULT_KERAS_INFO)
 
         fw_impl = GPTQKerasImplemantation()
 
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
+        tg, bit_widths_config, hessian_info_service = core_runner(in_model=in_model,
+                                                                  representative_data_gen=representative_data_gen,
+                                                                  core_config=core_config,
+                                                                  fw_info=DEFAULT_KERAS_INFO,
+                                                                  fw_impl=fw_impl,
+                                                                  tpc=target_platform_capabilities,
+                                                                  target_resource_utilization=target_resource_utilization,
+                                                                  tb_w=tb_w)
 
         tg_gptq = gptq_runner(tg,
                               core_config,
                               gptq_config,
                               representative_data_gen,
                               gptq_representative_data_gen if gptq_representative_data_gen else representative_data_gen,
-                              fw_info,
+                              DEFAULT_KERAS_INFO,
                               fw_impl,
-                              tb_w)
+                              tb_w,
+                              hessian_info_service=hessian_info_service)
+
+        del hessian_info_service
 
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, tg_gptq, fw_impl, fw_info)
 
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling keras_gradient_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_keras_model(tg_gptq)
-
-        return export_model(tg_gptq,
-                            fw_info,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        return get_exportable_keras_model(tg_gptq)
 
 else:
-    # If tensorflow or tensorflow_model_optimization are not installed,
+    # If tensorflow is not installed,
     # we raise an exception when trying to use these functions.
     def get_keras_gptq_config(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_post_training_quantization_mixed_precision. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        Logger.critical("Tensorflow must be installed to use get_keras_gptq_config. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
 
 
-    def keras_gradient_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_gradient_post_training_quantization_experimental. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_gradient_post_training_quantization(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_gradient_post_training_quantization. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer
 
 if FOUND_TF:
     import tensorflow as tf
 
     from model_compression_toolkit.trainable_infrastructure import BaseKerasTrainableQuantizer
-    from mct_quantizers import KerasQuantizationWrapper
+    from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 
     class BaseKerasGPTQTrainableQuantizer(BaseKerasTrainableQuantizer):
         """
         A base class for trainable Keras quantizer for GPTQ.
         """
 
         def __init__(self,
@@ -40,16 +40,15 @@
 
             Args:
                 quantization_config: quantizer config class contains all the information about a quantizer configuration.
             """
 
             super().__init__(quantization_config)
 
-
-        def update_layer_quantization_params(self, layer: KerasQuantizationWrapper
+        def update_layer_quantization_params(self, layer: KerasTrainableQuantizationWrapper
                                              ) -> (Dict[str, tf.Tensor], Dict[str, Dict], Dict):
             """
             A Function to calculate the needed change in attributes in NodeQuantizationConfig after retraining.
 
             Args:
                 layer: A wrapped Keras layer.
 
@@ -58,16 +57,16 @@
                 that changed during GPTQ retraining.
                 Keys must match NodeQuantizationConfig attributes
 
             """
             weights = {}
             for weight, quantizer_vars, quantizer in layer.get_weights_vars():
                 if not isinstance(quantizer, BaseTrainableQuantizer):
-                    Logger.error(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
-                                 f"but got {type(quantizer)} which is not callable.")
+                    Logger.critical(f"Expecting a GPTQ trainable quantizer for layer '{layer.name}', but received {type(quantizer)}. "
+                                    f"Ensure a trainable quantizer is used.") # pragma: no cover
                 weights.update({weight: quantizer(training=False, inputs=quantizer_vars)})
 
             quant_config = {WEIGHTS_QUANTIZATION_PARAMS: self.get_quant_config()}
 
             return weights, quant_config, {}
 
         def get_aux_variable(self) -> List[tf.Tensor]:
@@ -102,10 +101,9 @@
             """
             raise NotImplemented(f'{self.__class__.__name__} have to implement the '  # pragma: no cover
                                  f'quantizer\'s get_quant_config.')
 
 else:
     class BaseKerasGPTQTrainableQuantizer:  # pragma: no cover
         def __init__(self, *args, **kwargs):
-            Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                            'when using BaseKerasGPTQTrainableQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasGPTQTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,67 +10,72 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, List, Tuple
 
-from model_compression_toolkit.gptq import GradientPTQConfigV2
+from model_compression_toolkit.gptq import GradientPTQConfig
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
     get_inferable_quantizer_kwargs
-from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
     get_trainable_quantizer_weights_config
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         gptq_config: GradientPTQConfigV2
-                         ) -> Tuple[Dict[str, BaseKerasGPTQTrainableQuantizer], List[BaseKerasInferableQuantizer]]:
+                         gptq_config: GradientPTQConfig,
+                         kernel_attr: str = None) -> Tuple[Dict[str, BaseKerasGPTQTrainableQuantizer], List[BaseKerasInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
+        gptq_config (GradientPTQConfig): GradientPTQConfig configuration.
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
         Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
         to be compatible with the quantization infrastructure template.
     """
 
     weights_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during GPTQ
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
 
         quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
-                                                        quantizer_type=gptq_config.rounding_type,
+                                                        quantizer_id=gptq_config.rounding_type,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BaseKerasGPTQTrainableQuantizer)
-        kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=n.type,
-                                                              fw_info=DEFAULT_KERAS_INFO)
 
-        weights_quantizers.update({kernel_attribute: quantizer_class(get_trainable_quantizer_weights_config(n),
-                                                                     **gptq_config.gptq_quantizer_params_override)})
+        weights_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                       kernel_attr),
+                                                                **gptq_config.gptq_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
+        if n.final_activation_quantization_cfg is None:
+            Logger.critical(f"Cannot set quantizer for a node without a final activation quantization configuration.")  # pragma: no cover
+
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
 
         quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BaseKerasInferableQuantizer)
 
-        kwargs = get_inferable_quantizer_kwargs(n, QuantizationTarget.Activation)
+        kwargs = get_inferable_quantizer_kwargs(n.final_activation_quantization_cfg, QuantizationTarget.Activation)
 
         activation_quantizers.append(quantizer_class(**kwargs))
 
     return weights_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,16 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
-from model_compression_toolkit.gptq import RoundingType, GradientPTQConfigV2, GradientPTQConfig
-from model_compression_toolkit.gptq.keras.quantizer.soft_rounding.soft_quantizer_reg import \
+from model_compression_toolkit.gptq import RoundingType, GradientPTQConfig, GradientPTQConfig
+from model_compression_toolkit.gptq.pytorch.quantizer.soft_rounding.soft_quantizer_reg import \
     SoftQuantizerRegularization
 
 
 def get_regularization(gptq_config: GradientPTQConfig, representative_data_gen: Callable) -> Callable:
     """
     Returns a function that computes the regularization term for GPTQ training based on the given
     rounding type in the GPTQ configuration.
@@ -34,12 +34,10 @@
     """
     if gptq_config.rounding_type == RoundingType.SoftQuantizer:
         # dry run on the representative dataset to count number of batches
         num_batches = 0
         for _ in representative_data_gen():
             num_batches += 1
 
-        n_epochs = GradientPTQConfigV2.from_v1(n_ptq_iter=num_batches, config_v1=gptq_config).n_epochs if \
-            not type(gptq_config) == GradientPTQConfigV2 else gptq_config.n_epochs
-        return SoftQuantizerRegularization(total_gradient_steps=num_batches * n_epochs)
+        return SoftQuantizerRegularization(total_gradient_steps=num_batches * gptq_config.n_epochs)
     else:
         return lambda m, e_reg: 0
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/common/pruning/mask/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from typing import List
 
 import tensorflow as tf
 from keras import Model
 
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
-from mct_quantizers import KerasQuantizationWrapper
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 
 
 class LinearTempDecay:
     """
     Annealing process for the soft quantizer regularization temperature term.
     """
 
@@ -89,15 +89,15 @@
             entropy_reg: Entropy value to scale the quantizer regularization.
 
         Returns: Regularization value.
         """
         soft_reg_aux: List[tf.Tensor] = []
         b = self.linear_decay(self.count_iter.value())
         for layer in model.layers:
-            if isinstance(layer, KerasQuantizationWrapper):
+            if isinstance(layer, KerasTrainableQuantizationWrapper):
                 kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
                                                                       fw_info=DEFAULT_KERAS_INFO)
 
                 st = layer.weights_quantizers[kernel_attribute].get_soft_targets()
                 soft_reg_aux.append(tf.reduce_sum(1 - tf.pow(tf.math.abs(st - .5) * 2, b)))
 
         reg = 0
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files 0% similar despite different names*

```diff
@@ -64,15 +64,15 @@
     min_int = -int(signed) * (2 ** (num_bits - int(signed)))
     max_int = (2 ** (num_bits - int(signed))) - 1
     return delta * clip(tensor_q, max_val=max_int, min_val=min_int)
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=RoundingType.SoftQuantizer)
+                identifier=RoundingType.SoftQuantizer)
 class SymmetricSoftRoundingGPTQ(BaseKerasGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py`

 * *Files 0% similar despite different names*

```diff
@@ -59,15 +59,15 @@
     return delta * qutils.ste_clip(tensor_q,
                                    min_val=0,
                                    max_val=2 ** num_bits - 1) + min_range
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.UNIFORM],
-                quantizer_type=RoundingType.SoftQuantizer)
+                identifier=RoundingType.SoftQuantizer)
 class UniformSoftRoundingGPTQ(BaseKerasGPTQTrainableQuantizer):
     """
     Trainable uniform quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,49 +1,50 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-from typing import Dict, Any
-
+import torch
+import torch.nn as nn
+from typing import Dict
 import numpy as np
-import tensorflow as tf
+from model_compression_toolkit.defaultdict import DefaultDict
 
-from model_compression_toolkit.gptq import RoundingType
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from mct_quantizers import QuantizationTarget
-from model_compression_toolkit.gptq.common.gptq_constants import AUXVAR, PTQ_THRESHOLD
-from model_compression_toolkit.gptq.keras.quantizer import quant_utils as qutils
+from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
+from model_compression_toolkit.gptq.common.gptq_config import RoundingType
+from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
+    BasePytorchGPTQTrainableQuantizer
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy
+from model_compression_toolkit.gptq.pytorch.quantizer import quant_utils as qutils
+from model_compression_toolkit.gptq.common.gptq_constants import AUXVAR, PTQ_THRESHOLD, MAX_LSB_CHANGE
 from model_compression_toolkit.constants import THRESHOLD
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
 from mct_quantizers import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
     get_threshold_reshape_shape
-from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-def pertubation_symmetric_quantizer(input_tensor: tf.Tensor,
-                                    auxvar_tensor: tf.Variable,
-                                    max_tensor: tf.Tensor,
+def pertubation_symmetric_quantizer(input_tensor: torch.Tensor,
+                                    auxvar_tensor: nn.Parameter,
+                                    max_tensor: torch.Tensor,
                                     num_bits: int,
                                     signed: bool,
                                     power_of_two: bool,
-                                    max_lsbs_change: int = 1) -> tf.Tensor:
+                                    max_lsbs_change: int = MAX_LSB_CHANGE) -> nn.Parameter:
     """
     Quantize a tensor symmetrically with maximum LSBs shift.
 
     Args:
         input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
         auxvar_tensor: Tensor that manifests the bit shift the weight due to gptq
         max_tensor: Tensor with max values to compute the threshold.
@@ -55,106 +56,118 @@
     Returns:
         A quantized tensor.
     """
 
     if power_of_two:
         max_tensor = qutils.power_of_two_max(max_tensor)
     delta = qutils.calculate_delta(max_tensor, num_bits, signed)
-    input_tensor_int = tf.stop_gradient(tf.round(input_tensor / delta))
-    tensor_q = qutils.ste_round(
-        input_tensor_int + qutils.ste_clip(auxvar_tensor, max_val=max_lsbs_change * delta) / delta)
+    delta = to_torch_tensor(delta)
+    max_tensor_change = delta * max_lsbs_change
+
     min_int = -int(signed) * (2 ** (num_bits - int(signed)))
     max_int = (2 ** (num_bits - int(signed))) - 1
+
+    tensor_clipped = qutils.ste_clip(auxvar_tensor, min_val=-max_tensor_change, max_val=max_tensor_change) / delta
+    input_tensor_int = torch.round(input_tensor / delta).detach()
+
+    tensor_q = qutils.ste_round(qutils.ste_round(input_tensor_int + tensor_clipped))
+
     return delta * qutils.ste_clip(tensor_q, max_val=max_int, min_val=min_int)
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=RoundingType.STE)
-class STEWeightGPTQQuantizer(BaseKerasGPTQTrainableQuantizer):
+                identifier=RoundingType.STE)
+class STEWeightGPTQQuantizer(BasePytorchGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to quantize a layer weights.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
-                 max_lsbs_change_map: dict = DefaultDict({}, lambda: 1)):
+                 max_lsbs_change_map: dict = DefaultDict(default_value=1)):
         """
-        Initialize a STEWeightGPTQQuantizer object with parameters to use for the quantization.
+        Construct a Pytorch model that utilize a fake weight quantizer of STE (Straight Through Estimator) for symmetric quantizer.
 
         Args:
             quantization_config: Trainable weights quantizer config.
-            max_lsbs_change_map: a mapping between number of bits to max lsb change.
         """
         super().__init__(quantization_config)
         self.num_bits = quantization_config.weights_n_bits
         self.per_channel = quantization_config.weights_per_channel_threshold
 
         threshold_values = quantization_config.weights_quantization_params[THRESHOLD]
         self.threshold_shape = np.asarray(threshold_values).shape
         self.threshold_values = np.reshape(np.asarray(threshold_values), [-1]) if self.per_channel else float(
             threshold_values)
 
         self.quantization_axis = quantization_config.weights_channels_axis
         self.power_of_two = quantization_config.weights_quantization_method == QuantizationMethod.POWER_OF_TWO
         self.max_lsbs_change = max_lsbs_change_map.get(self.num_bits)
 
+
     def initialize_quantization(self,
-                                tensor_shape: Any,
+                                tensor_shape: torch.Size,
                                 name: str,
-                                layer: Any):
+                                layer: PytorchQuantizationWrapper):
         """
         Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
             tensor_shape: tensor shape of the quantized tensor.
             name: Tensor name.
             layer: Layer to quantize.
         """
 
-        ptq_threshold_tensor = layer.add_weight(
-            f"{name}_{PTQ_THRESHOLD}",
-            shape=len(self.threshold_values) if self.per_channel else (),
-            initializer=tf.keras.initializers.Constant(1.0),
-            trainable=False)
-        ptq_threshold_tensor.assign(self.threshold_values)
-
-        w = getattr(layer.layer, name)
-        auxvar_tensor = layer.add_weight(
-            f"{name}_{AUXVAR}",
-            shape=list(w.shape),
-            initializer=tf.keras.initializers.Constant(0.0),
-            trainable=True)
+        layer.register_parameter(f"{name}_{PTQ_THRESHOLD}",
+                                 nn.Parameter(torch.tensor(self.threshold_values, requires_grad=False)
+                                              if not self.per_channel
+                                              else to_torch_tensor(self.threshold_values),requires_grad=False))
+        layer.register_parameter(f"{name}_{AUXVAR}", nn.Parameter(to_torch_tensor(torch.zeros(self.threshold_shape)),
+                                                                  requires_grad=True))
 
         # save the quantizer added parameters for later calculations
-        self.add_quantizer_variable(PTQ_THRESHOLD, ptq_threshold_tensor, VariableGroup.QPARAMS)
-        self.add_quantizer_variable(AUXVAR, auxvar_tensor, VariableGroup.WEIGHTS)
+        self.add_quantizer_variable(PTQ_THRESHOLD, layer.get_parameter(f"{name}_{PTQ_THRESHOLD}"), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(AUXVAR, layer.get_parameter(f"{name}_{AUXVAR}"), VariableGroup.WEIGHTS)
+
+
+    def get_quant_config(self) -> Dict[str, np.ndarray]:
+        """
+        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
+
+        Returns:
+            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
+            Keys must match NodeQuantizationConfig attributes
+
+        """
+        old_threshold = self.get_quantizer_variable(PTQ_THRESHOLD)
+        return {THRESHOLD: torch_tensor_to_numpy(old_threshold).reshape(self.threshold_shape)}
 
     def __call__(self,
-                 inputs: tf.Tensor,
-                 training: bool):
+                 inputs: nn.Parameter,
+                 training: bool) -> nn.Parameter:
         """
         Quantize a tensor.
 
         Args:
             inputs: Input tensor to quantize.
-            training: Whether the graph is in training mode.
+            training: whether in training mode or not
 
         Returns:
-            The quantized tensor.
+            quantized tensor
         """
-
         auxvar = self.get_quantizer_variable(AUXVAR)
         ptq_threshold_tensor = self.get_quantizer_variable(PTQ_THRESHOLD)
 
         if self.per_channel:
             reshape_shape = get_threshold_reshape_shape(inputs.shape,
                                                         quant_axis=self.quantization_axis,
                                                         quant_axis_dim=-1)
-            ptq_threshold_tensor = tf.reshape(ptq_threshold_tensor, reshape_shape)
+            ptq_threshold_tensor = torch.reshape(ptq_threshold_tensor, reshape_shape)
+
             q_tensor = pertubation_symmetric_quantizer(inputs,
                                                        auxvar,
                                                        ptq_threshold_tensor,
                                                        self.num_bits,
                                                        signed=True,
                                                        power_of_two=self.power_of_two,
                                                        max_lsbs_change=self.max_lsbs_change)
@@ -162,20 +175,7 @@
         else:
             return pertubation_symmetric_quantizer(inputs,
                                                    auxvar,
                                                    ptq_threshold_tensor,
                                                    self.num_bits,
                                                    signed=True,
                                                    power_of_two=self.power_of_two)
-
-
-    def get_quant_config(self) -> Dict[str, np.ndarray]:
-        """
-        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
-
-        Returns:
-            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
-            Keys must match NodeQuantizationConfig attributes
-
-        """
-        old_threshold = self.get_quantizer_variable(PTQ_THRESHOLD)
-        return {THRESHOLD: old_threshold.numpy().reshape(self.threshold_shape)}
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_loss.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/gptq_training.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantization_facade.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,287 +8,241 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, List, Tuple, Union
-
-import numpy as np
-from torch.nn import Module
-from tqdm import tqdm
 import copy
-import torch
+from typing import Callable
+from functools import partial
+
+from model_compression_toolkit.constants import FOUND_TORCH, PYTORCH
+
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
-from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
-from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.pytorch.constants import BIAS
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, set_model, torch_tensor_to_numpy
-from model_compression_toolkit.gptq.pytorch.graph_info import get_gptq_trainable_parameters, \
-    get_weights_for_loss
-from model_compression_toolkit.gptq.pytorch.quantizer.quantization_builder import quantization_builder
-from model_compression_toolkit.gptq.pytorch.quantizer.regularization_factory import get_regularization
-from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
-
-
-class PytorchGPTQTrainer(GPTQTrainer):
-    """
-    Pytorch GPTQ training class for fine-tuning a quantized model
-    """
-
-    def __init__(self,
-                 graph_float: Graph,
-                 graph_quant: Graph,
-                 gptq_config: GradientPTQConfigV2,
-                 fw_impl: FrameworkImplementation,
-                 fw_info: FrameworkInfo,
-                 representative_data_gen: Callable):
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+
+if FOUND_TORCH:
+    import torch.nn as nn
+    from torch.nn import Module
+    from mct_quantizers import PytorchActivationQuantizationHolder
+    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
+    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+    from mct_quantizers import PytorchQuantizationWrapper
+    from model_compression_toolkit import get_target_platform_capabilities
+    from model_compression_toolkit.qat.common.qat_config import QATConfig
+    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import get_activation_quantizer_holder
+    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import quantization_builder
+
+    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
+
+
+    def qat_wrapper(n: common.BaseNode,
+                    module: nn.Module,
+                    qat_config: QATConfig):
         """
-        Build two models from a graph: A teacher network (float model) and a student network (quantized model).
-        Use the dataset generator to pass images through the teacher and student networks to get intermediate
-        layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
-        in the student network, to minimize it in the next similar steps.
-        All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
+        A function which takes a computational graph node and a pytorch module and perform the quantization wrapping
         Args:
-            graph_float: Graph to build a float networks from.
-            graph_quant: Graph to build a quantized networks from.
-            gptq_config: GradientPTQConfigV2 with parameters about the tuning process.
-            fw_impl: FrameworkImplementation object with a specific framework methods implementation.
-            fw_info: Framework information
-            representative_data_gen: Dataset to use for inputs of the models.
+            n: A node of mct graph.
+            module: A Pytorch module
+            qat_config (QATConfig): QAT configuration
+        Returns: Wrapped layer
+
         """
-        super().__init__(graph_float, graph_quant, gptq_config, fw_impl, fw_info)
-        self.loss_list = []
-        self.input_scale = 1
-        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
-            Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
-        else:
-            self.input_scale = self.gptq_user_info.input_scale
-
-        trainable_weights, trainable_bias, trainable_threshold = get_gptq_trainable_parameters(
-            self.fxp_model,
-            add_bias=self.gptq_config.train_bias)
-
-        self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
-        if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
-                self.fxp_weights_list)):
-            Logger.error(
-                "GPTQ: Mismatch between number of compare points, number of layers with trainable weights " +
-                "and number of float and quantized weights for loss")
-
-        self.optimizer_with_param = self.get_optimizer_with_param(trainable_weights,
-                                                                  trainable_bias,
-                                                                  trainable_threshold)
+        if is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
+            # If we are here, then the node has a kernel attribute to quantize and training during QAT
+            weights_quantizers, _ = quantization_builder(n, qat_config,
+                                                         DEFAULT_PYTORCH_INFO.get_kernel_op_attributes(n.type)[0])
+            if len(weights_quantizers) > 0:
+                return PytorchQuantizationWrapper(module, weights_quantizers)
 
-        self.weights_for_average_loss = to_torch_tensor(self.compute_hessian_based_weights(representative_data_gen))
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return module
 
-        self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
-    def _is_gptq_weights_trainable(self,
-                                   node: BaseNode) -> bool:
-        """
-        A function for deciding if a layer should be fine-tuned during GPTQ.
-        Args:
-            node (BaseNode): Node for quantization decision
-        Returns:
-            A boolean whether the layer is to be wrapped with a Quantization Wrapper.
+    def pytorch_quantization_aware_training_init_experimental(in_model: Module,
+                                                              representative_data_gen: Callable,
+                                                              target_resource_utilization: ResourceUtilization = None,
+                                                              core_config: CoreConfig = CoreConfig(),
+                                                              qat_config: QATConfig = QATConfig(),
+                                                              target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
+         Prepare a trained Pytorch model for quantization aware training. First the model quantization is optimized
+         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
+         quantized using a symmetric quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
+         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
+         quantization wrapper's weight quantizer.
+         In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
+         is set (in bytes).
 
-        if node.is_weights_quantization_enabled() and not self.fw_info.is_kernel_op(node.type):
-            Logger.error(f"GPTQ Error: Quantizing node {node.name} of type {node.type} "
-                         f"without a kernel isn't supported.")
-        return node.is_weights_quantization_enabled()
-
-    def gptq_wrapper(self,
-                     n: BaseNode,
-                     layer: Module) -> Union[PytorchQuantizationWrapper, Module]:
-        """
-        A function which takes a computational graph node and a pytorch layer and perform the quantization wrapping.
+         Args:
+             in_model (Model): Pytorch model to quantize.
+             representative_data_gen (Callable): Dataset used for initial calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             qat_config (QATConfig): QAT configuration
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Pytorch model according to.
 
-        Args:
-            n: A node of mct graph.
-            layer: A pytorch layer
+         Returns:
 
-        Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
-        """
+             A quantized model.
+             User information that may be needed to handle the quantized model.
 
-        if self._is_gptq_weights_trainable(n):
-            weights_quantizers, activation_quantizers = quantization_builder(n, self.gptq_config)
-            return PytorchQuantizationWrapper(layer,
-                                              weights_quantizers=weights_quantizers)
-        else:
-            return layer
+         Examples:
 
-    def get_activation_quantizer_holder(self, n: BaseNode) -> Callable:
-        """
-        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
-        If the layer is not supposed to be wrapped with an activation quantizer - return None.
-        Args:
-            n: Node to attach a PytorchActivationQuantizationHolder to its output.
-        Returns:
-            A PytorchActivationQuantizationHolder module for the node's activation quantization.
-        """
-        _, activation_quantizers = quantization_builder(n, self.gptq_config)
-        # Holder by definition uses a single quantizer for the activation quantization
-        # thus we make sure this is the only possible case (unless it's a node we no activation
-        # quantization, which in this case has an empty list).
-        if len(activation_quantizers) == 1:
-            return PytorchActivationQuantizationHolder(activation_quantizers[0])
-        Logger.error(
-            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
-            f'were found for node {n}')
+             Import MCT:
 
-    def build_gptq_model(self):
-        """
-        Build the GPTQ model with QuantizationWrappers
-        Returns:
-            Quantized graph for GPTQ fine-tuning, GPTQ graph user info
-        """
-        gptq_model, gptq_user_info = PyTorchModelBuilder(graph=self.graph_quant,
-                                                         append2output=self.compare_points,
-                                                         fw_info=self.fw_info,
-                                                         wrapper=self.gptq_wrapper,
-                                                         return_float_outputs=True,
-                                                         get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder).build_model()
+             >>> import model_compression_toolkit as mct
 
-        return gptq_model, gptq_user_info
+             Import a Pytorch model:
 
-    def train(self, representative_data_gen: Callable):
-        """
-          GPTQ Training using pytorch framework
-          Args:
-              representative_data_gen: Dataset generator to get images.
-          Returns:
-              Graph after GPTQ training
-          """
-        # Set Optimizers
-        for (optimizer, params) in self.optimizer_with_param:
-            optimizer.param_groups.clear()
-            optimizer.add_param_group({'params': params})
-
-        # Set models mode
-        set_model(self.float_model, False)
-        set_model(self.fxp_model, True)
-        self._set_requires_grad()
-
-        # ----------------------------------------------
-        # Training loop
-        # ----------------------------------------------
-        self.micro_training_loop(representative_data_gen, self.gptq_config.n_epochs)
-
-    def compute_gradients(self,
-                          y_float: List[torch.Tensor],
-                          input_tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, List[np.ndarray]]:
-        """
-        Get outputs from both teacher and student networks. Compute the observed error,
-        and use it to compute the gradients and applying them to the student weights.
-        Args:
-            y_float: A list of reference tensor from the floating point network.
-            input_tensors: A list of Input tensors to pass through the networks.
-        Returns:
-            Loss and gradients.
-        """
+             >>> from torchvision.models import mobilenet_v2
+             >>> model = mobilenet_v2(pretrained=True)
 
-        # Forward-pass
-        y_fxp = self.fxp_model(input_tensors)
+            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
+            In this example a random dataset of 10 batches each containing 4 images is used.
 
-        # Loss
-        loss_value = self.gptq_config.loss(y_fxp,
-                                           y_float,
-                                           self.fxp_weights_list,
-                                           self.flp_weights_list,
-                                           self.compare_points_mean,
-                                           self.compare_points_std,
-                                           self.weights_for_average_loss)
-
-        reg_value = self.reg_func(self.fxp_model, self.gptq_config.regularization_factor)
-
-        loss_value += reg_value
-
-        # Back-pass
-        loss_value.backward()
-
-        # Get gradients
-        grads = []
-        for param in self.fxp_model.parameters():
-            if param.requires_grad and param.grad is not None:
-                grads.append(torch_tensor_to_numpy(param.grad))
-
-        return loss_value, grads
-
-    def micro_training_loop(self,
-                            data_function: Callable,
-                            n_epochs: int):
-        """
-        This function run a micro training loop on given set of parameters.
-        Args:
-            data_function: A callable function that give a batch of samples.
-            n_epochs: Number of update iterations of representative dataset.
-        """
-        for _ in tqdm(range(n_epochs)):
-            for data in tqdm(data_function()):
-                input_data = [d * self.input_scale for d in data]
-                input_tensor = to_torch_tensor(input_data)
-                y_float = self.float_model(input_tensor)  # running float model
-                loss_value, grads = self.compute_gradients(y_float, input_tensor)
-                # Run one step of gradient descent by updating the value of the variables to minimize the loss.
-                for (optimizer, _) in self.optimizer_with_param:
-                    optimizer.step()
-                    optimizer.zero_grad()
-                if self.gptq_config.log_function is not None:
-                    self.gptq_config.log_function(loss_value.item(),
-                                                  torch_tensor_to_numpy(grads),
-                                                  torch_tensor_to_numpy(self.optimizer_with_param[0][-1]))
-                self.loss_list.append(loss_value.item())
-                Logger.debug(f'last loss value: {self.loss_list[-1]}')
+            >>> import numpy as np
+            >>> num_calibration_batches = 10
+            >>> def repr_datagen():
+            >>>     for _ in range(num_calibration_batches):
+            >>>         yield [np.random.random((4, 3, 224, 224))]
 
-    def update_graph(self) -> Graph:
-        """
-        Update a graph using GPTQ after minimizing the loss between the float model's output
-        and the quantized model's outputs.
-        Returns:
-            Updated graph after GPTQ.
-        """
-        graph_quant = copy.copy(self.graph_quant)
+             Create a MCT core config, containing the quantization configuration:
 
-        # Update graph after training
-        for name, layer in self.fxp_model.named_modules():
-            if isinstance(layer, PytorchQuantizationWrapper):
-                node = self.graph_quant.find_node_by_name(name)
-                if len(node) != 1:
-                    Logger.error(f"Can't update GPTQ graph due to missing layer named: {name}")
-                node = node[0]
-                kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
-                                                                      fw_info=self.fw_info)
-                weights, weight_quant_config, activation_quant_config = \
-                    layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
-                for weight_attr, weight in weights.items():
-                    node.set_weights_by_keys(weight_attr, self.fw_impl.to_numpy(weight))
-                for config_attr, config_value in weight_quant_config.items():
-                    node.final_weights_quantization_cfg.set_quant_config_attr(config_attr, config_value)
-                for config_attr, config_value in activation_quant_config.items():
-                    node.final_activation_quantization_cfg.set_quant_config_attr(config_attr, config_value)
-                if self.gptq_config.train_bias and hasattr(layer.layer, BIAS):
-                    node.set_weights_by_keys(BIAS, self.fw_impl.to_numpy(getattr(layer.layer, BIAS)))
+             >>> config = mct.core.CoreConfig()
 
-        return graph_quant
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+             quantized model. Now the model contains quantizer wrappers for fine tunning the weights:
 
-    def _set_requires_grad(self):
-        """
-        Set require_grad flag for trainable parameters for GPTQ training
+             >>> quantized_model, quantization_info = mct.qat.pytorch_quantization_aware_training_init_experimental(model, repr_datagen, core_config=config)
+
+             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+
+         """
+        Logger.warning(
+            f"pytorch_quantization_aware_training_init_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use pytorch_post_training_quantization API,"
+                             "or pass a valid mixed precision configuration.")
+
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+        fw_impl = PytorchImplementation()
+
+        # Ignore trace hessian service as we do not use it here
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_PYTORCH_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+
+        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
+
+        qat_model, user_info = PyTorchModelBuilder(graph=tg,
+                                                   fw_info=DEFAULT_PYTORCH_INFO,
+                                                   wrapper=_qat_wrapper,
+                                                   get_activation_quantizer_holder_fn=partial(
+                                                       get_activation_quantizer_holder,
+                                                       qat_config=qat_config)).build_model()
+
+        user_info.mixed_precision_cfg = bit_widths_config
+
+        # Remove fw_info from graph to enable saving the pytorch model (fw_info can not be pickled)
+        delattr(qat_model.graph, 'fw_info')
+
+        return qat_model, user_info
+
+
+    def pytorch_quantization_aware_training_finalize_experimental(in_model: Module):
         """
-        # Float model: freeze all the parameters in the network
-        for param in self.float_model.parameters():
-            param.requires_grad = False
-
-        # Fxp model: unfreeze bias trainable parameters
-        for layer in self.fxp_model.modules():
-            if isinstance(layer, PytorchQuantizationWrapper):
-                if hasattr(layer.layer, BIAS):
-                    bias = getattr(layer.layer, BIAS)
-                    bias.requires_grad = self.gptq_config.train_bias
+         Convert a model fine-tuned by the user to a network with QuantizeWrappers containing
+         InferableQuantizers, that quantizes both the layers weights and outputs
+
+         Args:
+             in_model (Model): Pytorch model to remove QuantizeWrappers.
+
+         Returns:
+             A quantized model with QuantizeWrappers and InferableQuantizers.
+
+         Examples:
+
+             Import MCT:
+
+             >>> import model_compression_toolkit as mct
+
+             Import a Pytorch model:
+
+             >>> from torchvision.models import mobilenet_v2
+             >>> model = mobilenet_v2(pretrained=True)
+
+             Create a random dataset generator:
+
+             >>> import numpy as np
+             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
+
+             Create a MCT core config, containing the quantization configuration:
+
+             >>> config = mct.core.CoreConfig()
+
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+             quantized model:
+
+             >>> quantized_model, quantization_info = mct.qat.pytorch_quantization_aware_training_init_experimental(model, repr_datagen, core_config=config)
+
+             Use the quantized model for fine-tuning. Finally, remove the quantizer wrappers and keep a quantize model ready for inference.
+
+             >>> quantized_model = mct.qat.pytorch_quantization_aware_training_finalize_experimental(quantized_model)
+
+         """
+        Logger.warning(
+            f"pytorch_quantization_aware_training_finalize_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        for _, layer in in_model.named_children():
+            if isinstance(layer, (PytorchQuantizationWrapper, PytorchActivationQuantizationHolder)):
+                layer.convert_to_inferable_quantizers()
+
+        return in_model
+
+
+else:
+    # If torch is not installed,
+    # we raise an exception when trying to use these functions.
+    def pytorch_quantization_aware_training_init_experimental(*args, **kwargs):
+        Logger.critical('PyTorch must be installed to use pytorch_quantization_aware_training_init_experimental. '
+                        "The 'torch' package is missing.")  # pragma: no cover
+
+
+    def pytorch_quantization_aware_training_finalize_experimental(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_quantization_aware_training_finalize_experimental'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/graph_info.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/graph_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     for layer in fxp_model.modules():
         if isinstance(layer, PytorchQuantizationWrapper):
             kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
                                                                   fw_info=DEFAULT_PYTORCH_INFO)
 
             # collect trainable weights per quantizer
             if kernel_attribute not in layer.weights_quantizers:
-                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+                Logger.critical(f"'{kernel_attribute}' was not found in the weight quantizers of layer '{layer.layer}'.")
             quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
             quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
             trainable_aux_weights.extend(quantizer_trainable_weights)
             trainable_threshold.extend(quantizer_trainable_threshold)
 
             if add_bias and hasattr(layer.layer, BIAS):
                 bias = getattr(layer.layer, BIAS)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantization_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,27 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 from model_compression_toolkit.core import common
 from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import PYTORCH
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.runner import core_runner
 from model_compression_toolkit.gptq.keras.quantization_facade import GPTQ_MOMENTUM
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
+    MixedPrecisionQuantizationConfig
 
 LR_DEFAULT = 1e-4
 LR_REST_DEFAULT = 1e-4
 LR_BIAS_DEFAULT = 1e-4
 LR_QUANTIZATION_PARAM_DEFAULT = 1e-4
 
 if FOUND_TORCH:
@@ -40,35 +42,34 @@
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.gptq.pytorch.gptq_loss import multiple_tensors_mse_loss
     from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     import torch
     from torch.nn import Module
     from torch.optim import Adam, Optimizer
     from model_compression_toolkit import get_target_platform_capabilities
-
-
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
-
     def get_pytorch_gptq_config(n_epochs: int,
                                 optimizer: Optimizer = Adam([torch.Tensor([])], lr=LR_DEFAULT),
                                 optimizer_rest: Optimizer = Adam([torch.Tensor([])], lr=LR_REST_DEFAULT),
                                 loss: Callable = multiple_tensors_mse_loss,
                                 log_function: Callable = None,
-                                use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
+                                use_hessian_based_weights: bool = True,
+                                regularization_factor: float = REG_DEFAULT) -> GradientPTQConfig:
         """
         Create a GradientPTQConfigV2 instance for Pytorch models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (Optimizer): Pytorch optimizer to use for fine-tuning for auxiliry variable.
             optimizer_rest (Optimizer): Pytorch optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
+            regularization_factor (float): A floating point number that defines the regularization factor.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and Create a GradientPTQConfigV2 to run for 5 epochs:
@@ -81,26 +82,27 @@
             >>> import torch
             >>> gptq_conf = mct.gptq.get_pytorch_gptq_config(n_epochs=3, optimizer=torch.optim.Adam([torch.Tensor(1)]))
 
             The configuration can be passed to :func:`~model_compression_toolkit.pytorch_post_training_quantization` in order to quantize a pytorch model using gptq.
 
         """
         bias_optimizer = torch.optim.SGD([torch.Tensor([])], lr=LR_BIAS_DEFAULT, momentum=GPTQ_MOMENTUM)
-        return GradientPTQConfigV2(n_epochs, optimizer, optimizer_rest=optimizer_rest, loss=loss,
-                                   log_function=log_function, train_bias=True, optimizer_bias=bias_optimizer, use_hessian_based_weights=use_hessian_based_weights)
+        return GradientPTQConfig(n_epochs, optimizer, optimizer_rest=optimizer_rest, loss=loss,
+                                 log_function=log_function, train_bias=True, optimizer_bias=bias_optimizer,
+                                 use_hessian_based_weights=use_hessian_based_weights,
+                                 regularization_factor=regularization_factor)
 
 
-    def pytorch_gradient_post_training_quantization_experimental(model: Module,
-                                                                 representative_data_gen: Callable,
-                                                                 target_kpi: KPI = None,
-                                                                 core_config: CoreConfig = CoreConfig(),
-                                                                 gptq_config: GradientPTQConfigV2 = None,
-                                                                 gptq_representative_data_gen: Callable = None,
-                                                                 target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC,
-                                                                 new_experimental_exporter: bool = True):
+    def pytorch_gradient_post_training_quantization(model: Module,
+                                                    representative_data_gen: Callable,
+                                                    target_resource_utilization: ResourceUtilization = None,
+                                                    core_config: CoreConfig = CoreConfig(),
+                                                    gptq_config: GradientPTQConfig = None,
+                                                    gptq_representative_data_gen: Callable = None,
+                                                    target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
         Quantize a trained Pytorch module using post-training quantization.
         By default, the module is quantized using a symmetric constraint quantization thresholds
         (power of two) as defined in the default TargetPlatformCapabilities.
         The module is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
@@ -112,26 +114,29 @@
         Then, the quantized weights are optimized using gradient based post
         training quantization by comparing points between the float and quantized models, and minimizing the observed
         loss.
 
         Args:
             model (Module): Pytorch model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-            gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
+            gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
-            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
             A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
+            Import Model Compression Toolkit:
+
+            >>> import model_compression_toolkit as mct
+
             Import a Pytorch module:
 
             >>> from torchvision import models
             >>> module = models.mobilenet_v2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
@@ -144,76 +149,63 @@
 
             Create MCT core configurations with number of calibration iterations set to 1:
 
             >>> config = mct.core.CoreConfig()
 
             Pass the module, the representative dataset generator and the configuration (optional) to get a quantized module
 
-            >>> quantized_module, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization_experimental(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
+            >>> quantized_module, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
 
         """
 
         if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config for mixed-precision is not of type 'MixedPrecisionQuantizationConfig'. "
+                                "Ensure usage of the correct API for 'keras_post_training_quantization' "
+                                "or provide a valid mixed-precision configuration.")  # pragma: no cover
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
 
         fw_impl = GPTQPytorchImplemantation()
 
         # ---------------------- #
         # Core Runner
         # ---------------------- #
-        graph, bit_widths_config = core_runner(in_model=model,
-                                               representative_data_gen=representative_data_gen,
-                                               core_config=core_config,
-                                               fw_info=DEFAULT_PYTORCH_INFO,
-                                               fw_impl=fw_impl,
-                                               tpc=target_platform_capabilities,
-                                               target_kpi=target_kpi,
-                                               tb_w=tb_w)
+        graph, bit_widths_config, hessian_info_service = core_runner(in_model=model,
+                                                                     representative_data_gen=representative_data_gen,
+                                                                     core_config=core_config,
+                                                                     fw_info=DEFAULT_PYTORCH_INFO,
+                                                                     fw_impl=fw_impl,
+                                                                     tpc=target_platform_capabilities,
+                                                                     target_resource_utilization=target_resource_utilization,
+                                                                     tb_w=tb_w)
 
         # ---------------------- #
         # GPTQ Runner
         # ---------------------- #
-        graph_gptq = gptq_runner(graph, core_config, gptq_config,
+        graph_gptq = gptq_runner(graph,
+                                 core_config,
+                                 gptq_config,
                                  representative_data_gen,
                                  gptq_representative_data_gen if gptq_representative_data_gen else representative_data_gen,
-                                 DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+                                 DEFAULT_PYTORCH_INFO,
+                                 fw_impl,
+                                 tb_w,
+                                 hessian_info_service=hessian_info_service)
+
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, graph_gptq, fw_impl, DEFAULT_PYTORCH_INFO)
 
-        # ---------------------- #
-        # Export
-        # ---------------------- #
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling pytorch_gradient_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_pytorch_model(graph_gptq)
-
-        return export_model(graph_gptq,
-                            DEFAULT_PYTORCH_INFO,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        return get_exportable_pytorch_model(graph_gptq)
+
 
 else:
     # If torch is not installed,
     # we raise an exception when trying to use these functions.
     def get_pytorch_gptq_config(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_gradient_post_training_quantization_experimental. '
-                        'Could not find torch package.')  # pragma: no cover
+        Logger.critical("PyTorch must be installed to use 'get_pytorch_gptq_config'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
 
 
-    def pytorch_gradient_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_gradient_post_training_quantization_experimental. '
-                        'Could not find the torch package.')  # pragma: no cover
+    def pytorch_gradient_post_training_quantization(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_gradient_post_training_quantization'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -59,15 +59,15 @@
                 that changed during GPTQ retraining.
                 Keys must match NodeQuantizationConfig attributes
 
             """
             weights = {}
             for weight, quantizer_vars, quantizer in layer.get_weights_vars():
                 if not isinstance(quantizer, BaseTrainableQuantizer):
-                    Logger.error(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
+                    Logger.critical(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
                                  f"but got {type(quantizer)} which is not callable.")
                 weights.update({weight: quantizer(training=False, inputs=quantizer_vars)})
 
             quant_config = {WEIGHTS_QUANTIZATION_PARAMS: self.get_quant_config()}
 
             return weights, quant_config, {}
 
@@ -83,10 +83,9 @@
             """
             raise NotImplemented(f'{self.__class__.__name__} have to implement the '  # pragma: no cover
                                  f'quantizer\'s get_quant_config.')
 
 else:
     class BasePytorchGPTQTrainableQuantizer:  # pragma: no cover
         def __init__(self, *args, **kwargs):
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchGPTQTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("PyTorch must be installed to use 'BasePytorchGPTQTrainableQuantizer'. "
+                            "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,64 +10,71 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Dict, Tuple
 
-from model_compression_toolkit.gptq import GradientPTQConfigV2
+from model_compression_toolkit.gptq import GradientPTQConfig
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.pytorch.constants import KERNEL
 from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
     get_activation_inferable_quantizer_kwargs
 from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
     BasePytorchGPTQTrainableQuantizer
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
+
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
     get_trainable_quantizer_weights_config
 from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         gptq_config: GradientPTQConfigV2,
-                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
-                                    List[BasePyTorchInferableQuantizer]]:
+                         gptq_config: GradientPTQConfig,
+                         kernel_attr: str = None
+                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer], List[BasePyTorchInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
+        gptq_config (GradientPTQConfig): GradientPTQConfig configuration.
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
         Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
         to be compatible with the quantization infrastructure template.
     """
 
     weights_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during GPTQ
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
         quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
-                                                        quantizer_type=gptq_config.rounding_type,
+                                                        quantizer_id=gptq_config.rounding_type,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BasePytorchGPTQTrainableQuantizer)
-        weights_quantizers.update({KERNEL: quantizer_class(get_trainable_quantizer_weights_config(n),
-                                                           **gptq_config.gptq_quantizer_params_override)})
+        weights_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                       kernel_attr),
+                                                                **gptq_config.gptq_quantizer_params_override)})
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
+        if n.final_activation_quantization_cfg is None:
+            Logger.critical(f"Cannot set quantizer for a node without a final activation quantization configuration.")  # pragma: no cover
+
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
 
         quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BasePyTorchInferableQuantizer)
 
-        kwargs = get_activation_inferable_quantizer_kwargs(n)
+        kwargs = get_activation_inferable_quantizer_kwargs(n.final_activation_quantization_cfg)
 
         activation_quantizers.append(quantizer_class(**kwargs))
 
     return weights_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py`

 * *Files 17% similar despite different names*

```diff
@@ -10,16 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
-from model_compression_toolkit.gptq import RoundingType, GradientPTQConfigV2, GradientPTQConfig
-from model_compression_toolkit.gptq.pytorch.quantizer.soft_rounding.soft_quantizer_reg import \
+from model_compression_toolkit.gptq import RoundingType, GradientPTQConfig, GradientPTQConfig
+from model_compression_toolkit.gptq.keras.quantizer.soft_rounding.soft_quantizer_reg import \
     SoftQuantizerRegularization
 
 
 def get_regularization(gptq_config: GradientPTQConfig, representative_data_gen: Callable) -> Callable:
     """
     Returns a function that computes the regularization term for GPTQ training based on the given
     rounding type in the GPTQ configuration.
@@ -34,12 +34,10 @@
     """
     if gptq_config.rounding_type == RoundingType.SoftQuantizer:
         # dry run on the representative dataset to count number of batches
         num_batches = 0
         for _ in representative_data_gen():
             num_batches += 1
 
-        n_epochs = GradientPTQConfigV2.from_v1(n_ptq_iter=num_batches, config_v1=gptq_config).n_epochs if \
-            not type(gptq_config) == GradientPTQConfigV2 else gptq_config.n_epochs
-        return SoftQuantizerRegularization(total_gradient_steps=num_batches * n_epochs)
+        return SoftQuantizerRegularization(total_gradient_steps=num_batches * gptq_config.n_epochs)
     else:
         return lambda m, e_reg: 0
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/hessian/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -66,15 +66,15 @@
     return delta * qutils.ste_clip(tensor_q,
                                    min_val=-int(signed) * int_threshold,
                                    max_val=int_threshold - 1)
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=RoundingType.SoftQuantizer)
+                identifier=RoundingType.SoftQuantizer)
 class SymmetricSoftRoundingGPTQ(BasePytorchGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,15 @@
     return delta * qutils.ste_clip(tensor_q,
                                    min_val=0,
                                    max_val=2 ** num_bits - 1) + min_range
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.UNIFORM],
-                quantizer_type=RoundingType.SoftQuantizer)
+                identifier=RoundingType.SoftQuantizer)
 class UniformSoftRoundingGPTQ(BasePytorchGPTQTrainableQuantizer):
     """
     Trainable uniform quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/gptq/runner.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/runner.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,89 +13,92 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.hessian import HessianInfoService
 from model_compression_toolkit.core.common.statistics_correction.statistics_correction import \
     apply_statistics_correction
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.gptq.common.gptq_training import gptq_training
 
 from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
 from model_compression_toolkit.core.common.statistics_correction.apply_bias_correction_to_graph import \
     apply_bias_correction_to_graph
 from model_compression_toolkit.logger import Logger
 
 
-def _apply_gptq(gptq_config: GradientPTQConfigV2,
+def _apply_gptq(gptq_config: GradientPTQConfig,
                 representative_data_gen: Callable,
                 tb_w: TensorboardWriter,
                 tg: Graph,
                 tg_bias: Graph,
                 fw_info: FrameworkInfo,
-                fw_impl: FrameworkImplementation) -> Graph:
+                fw_impl: FrameworkImplementation,
+                hessian_info_service: HessianInfoService = None) -> Graph:
     """
     Apply GPTQ to improve accuracy of quantized model.
     Build two models from a graph: A teacher network (float model) and a student network (quantized model).
     and use the dataset generator to pass images through the teacher and student networks to get intermediate
     layers outputs and maximize their similarity.
 
     Args:
         gptq_config: Configuration for using GPTQ (e.g. optimizer).
         representative_data_gen: Dataset used for calibration.
         tb_w: TensorBoardWriter object to log events.
         tg: Float Reference Graph.
         tg_bias: Graph of quantized model.
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
         fw_impl: Framework implementation per framework
+        hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
     Returns:
 
     """
     if gptq_config is not None and gptq_config.n_epochs > 0:
-        Logger.info("Using experimental Gradient Based PTQ: If you encounter an issue "
-                           "please file a bug. To disable it, do not pass a gptq configuration.")
-
         tg_bias = gptq_training(tg,
                                 tg_bias,
                                 gptq_config,
                                 representative_data_gen,
                                 fw_impl,
-                                fw_info)
+                                fw_info,
+                                hessian_info_service=hessian_info_service)
 
         if tb_w is not None:
             tb_w.add_graph(tg_bias, 'after_gptq')
     return tg_bias
 
 
 def gptq_runner(tg: Graph,
                 core_config: CoreConfig,
-                gptq_config: GradientPTQConfigV2,
+                gptq_config: GradientPTQConfig,
                 representative_data_gen: Callable,
                 gptq_representative_data_gen: Callable,
                 fw_info: FrameworkInfo,
                 fw_impl: FrameworkImplementation,
-                tb_w: TensorboardWriter) -> Graph:
+                tb_w: TensorboardWriter,
+                hessian_info_service: HessianInfoService = None) -> Graph:
     """
     Quantize a graph that has final weights candidates quantization configurations.
     Before we quantize the graph weights, we apply GPTQ to get an improved graph.
 
     Args:
         tg: Graph to apply GPTQ and to quantize.
         core_config: CoreConfig containing parameters of how the model should be quantized.
         gptq_config: GradientPTQConfig with parameters about the tuning process.
         representative_data_gen: Dataset used for calibration.
         gptq_representative_data_gen: Dataset used for GPTQ training
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.)
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
         tb_w: A TensorBoardWriter object initialized with the logger dir path if it was set, or None otherwise.
+        hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
     Returns:
         A graph after model weights GPTQ fine-tuning.
 
     """
 
     #############################################
@@ -110,10 +113,11 @@
     #############################################
     tg_gptq = _apply_gptq(gptq_config,
                           gptq_representative_data_gen,
                           tb_w,
                           tg,
                           tg_bias,
                           fw_info,
-                          fw_impl)
+                          fw_impl,
+                          hessian_info_service=hessian_info_service)
 
     return tg_gptq
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/hessian/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/legacy/keras_quantization_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantization_facade.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,200 +1,228 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Callable, List, Tuple
+from typing import Callable
+from functools import partial
 
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.constants import TENSORFLOW
-from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.gptq import GradientPTQConfig, GradientPTQConfigV2
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.network_editors.actions import EditRule
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
-from model_compression_toolkit.core.common.quantization.quantization_config import DEFAULTCONFIG
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
-from model_compression_toolkit.gptq.runner import gptq_runner
-from model_compression_toolkit.ptq.runner import ptq_runner
-from model_compression_toolkit.core.exporter import export_model
-from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-
+    MixedPrecisionQuantizationConfig
+from mct_quantizers import KerasActivationQuantizationHolder
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
 
 if FOUND_TF:
+    import tensorflow as tf
+    from tensorflow.keras.layers import Layer
+    from tensorflow.keras.models import Model
+
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
     from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
     from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
-    from tensorflow.keras.models import Model
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
 
+    from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
+
     from model_compression_toolkit import get_target_platform_capabilities
 
+    from model_compression_toolkit import get_target_platform_capabilities
+    from model_compression_toolkit.core import common
+    from model_compression_toolkit.core.common import BaseNode
+    from model_compression_toolkit.constants import TENSORFLOW
+    from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+    from model_compression_toolkit.qat.keras.quantizer.quantization_builder import quantization_builder, \
+    get_activation_quantizer_holder
+    from model_compression_toolkit.qat.common.qat_config import QATConfig
+
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
-    def keras_post_training_quantization(in_model: Model,
-                                         representative_data_gen: Callable,
-                                         n_iter: int = 500,
-                                         quant_config: QuantizationConfig = DEFAULTCONFIG,
-                                         fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                         network_editor: List[EditRule] = [],
-                                         gptq_config: GradientPTQConfig = None,
-                                         analyze_similarity: bool = False,
-                                         target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> \
-    Tuple[Model, UserInformation]:
+    def qat_wrapper(n: common.BaseNode,
+                    layer: Layer,
+                    qat_config: QATConfig):
         """
-        Quantize a pretrained Keras model using post-training quantization. By default, the model is quantized
-        using a symmetric constraint quantization thresholds (power of two) as defined in the default TargetPlatformCapabilities.
-        The model is first optimized using several transformations (e.g. BatchNormalization folding to
-        preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-        being collected for each layer's output (and input, depends on the quantization configuration).
-        Thresholds are then being calculated using the collected statistics and the model is quantized
-        (both coefficients and activations by default).
-        If a gptq_config is passed, the quantized weights are optimized using gradient based post
-        training quantization by comparing points between the float and quantized models, and minimizing the observed
-        loss.
-
+        A function which takes a computational graph node and a keras layer and perform the quantization wrapping
         Args:
-            in_model (Model): Keras model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            n_iter (int): Number of calibration iterations to run.
-            quant_config (QuantizationConfig): QuantizationConfig containing parameters of how the model should be quantized. `Default configuration. <https://github.com/sony/model_optimization/blob/21e21c95ca25a31874a5be7af9dd2dd5da8f3a10/model_compression_toolkit/core/common/quantization/quantization_config.py#L154>`_
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
-            gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
-            analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+            qat_config: Configuration of QAT (such as training methods for example).
+            n: A node of mct graph.
+            layer: A keras layer.
+
+        Returns: Wrapped layer
+
+        """
+        if is_qat_applicable(n, DEFAULT_KERAS_INFO):
+            # If we are here, then the node has a kernel attribute to quantize and training during QAT
+            weights_quantizers, _ = quantization_builder(n,
+                                                         qat_config,
+                                                         DEFAULT_KERAS_INFO.get_kernel_op_attributes(n.type)[0])
+            if len(weights_quantizers) > 0:
+                layer.trainable = True
+                return KerasTrainableQuantizationWrapper(layer, weights_quantizers)
+
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return layer
+
+
+    def keras_quantization_aware_training_init_experimental(in_model: Model,
+                                                            representative_data_gen: Callable,
+                                                            target_resource_utilization: ResourceUtilization = None,
+                                                            core_config: CoreConfig = CoreConfig(),
+                                                            qat_config: QATConfig = QATConfig(),
+                                                            target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC):
+        """
+         Prepare a trained Keras model for quantization aware training. First the model quantization is optimized
+         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
+         quantized using a symmetric quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
+         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
+         quantization wrapper's weight quantizer.
+         In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
+         is set (in bytes).
+
+         Args:
+             in_model (Model): Keras model to quantize.
+             representative_data_gen (Callable): Dataset used for initial calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             qat_config (QATConfig): QAT configuration
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+
+         Returns:
 
-        Returns:
-            A quantized model and information the user may need to handle the quantized model.
+             A quantized model.
+             User information that may be needed to handle the quantized model.
+             Custom-Objects dictionary for loading the saved kers model.
 
-        Examples:
+         Examples:
 
-            Import a Keras model:
+             Import MCT:
 
-            >>> from tensorflow.keras.applications.mobilenet import MobileNet
-            >>> model = MobileNet()
+             >>> import model_compression_toolkit as mct
 
-            Create a random dataset generator:
+             Import a Keras model:
 
-            >>> import numpy as np
-            >>> def repr_datagen(): return [np.random.random((1,224,224,3))]
+             >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
+             >>> model = MobileNetV2()
+
+             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
+             In this example a random dataset of 10 batches each containing 4 images is used.
 
-            Import mct and pass the model with the representative dataset generator to get a quantized model:
+             >>> import numpy as np
+             >>> num_calibration_batches = 10
+             >>> def repr_datagen():
+             >>>     for _ in range(num_calibration_batches):
+             >>>         yield [np.random.random((4, 224, 224, 3))]
 
-            >>> import model_compression_toolkit as mct
-            >>> quantized_model, quantization_info = mct.keras_post_training_quantization(model, repr_datagen, n_iter=1)
+             Create a MCT core config, containing the quantization configuration:
 
-        """
-        Logger.warning('keras_post_training_quantization is deprecated and will be removed '
-                       'in the future. Please use mct.ptq.keras_post_training_quantization_experimental instead.')
+             >>> config = mct.core.CoreConfig()
+
+             If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+             The candidates bitwidth for quantization should be defined in the target platform model:
+
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfig())
+
+             For mixed-precision set a target ResourceUtilization object:
+             Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
+             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
+             while the bias will not):
+
+             >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+
+             Pass the model, the representative dataset generator, the configuration and the target Resource Utilization to get a
+             quantized model:
+
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(model, repr_datagen, ru, core_config=config)
+
+             Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
+
+             >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
+
+             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+
+         """
+
+        Logger.warning(f"keras_quantization_aware_training_init_experimental is experimental and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
 
         KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
+                             fw_info=DEFAULT_KERAS_INFO).validate()
 
-        core_config = CoreConfig(quantization_config=quant_config,
-                                 debug_config=DebugConfig(analyze_similarity=analyze_similarity,
-                                                          network_editor=network_editor)
-                                 )
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization API,"
+                             "or pass a valid mixed precision configuration.")
 
-        tb_w = _init_tensorboard_writer(fw_info)
+        tb_w = init_tensorboard_writer(DEFAULT_KERAS_INFO)
 
         fw_impl = KerasImplementation()
 
-        # convert old representative dataset generation to a generator
-        def _representative_data_gen():
-            for _ in range(n_iter):
-                yield representative_data_gen()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=_representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            tb_w=tb_w)
-
-        if gptq_config is None:
-            tg = ptq_runner(tg, _representative_data_gen, core_config, fw_info, fw_impl, tb_w)
-        else:
-            gptq_config_v2 = GradientPTQConfigV2.from_v1(n_iter, gptq_config)
-            tg = gptq_runner(tg, core_config, gptq_config_v2, _representative_data_gen, _representative_data_gen,
-                             fw_info, fw_impl, tb_w)
-
-        if core_config.debug_config.analyze_similarity:
-            analyzer_model_quantization(_representative_data_gen, tb_w, tg, fw_impl, fw_info)
-
-        quantized_model, user_info = export_model(tg, fw_info, fw_impl, tb_w, bit_widths_config)
-
-        return quantized_model, user_info
-
-
-    def keras_post_training_quantization_mixed_precision(in_model: Model,
-                                                         representative_data_gen: Callable,
-                                                         target_kpi: KPI,
-                                                         n_iter: int = 500,
-                                                         quant_config: MixedPrecisionQuantizationConfig = DEFAULT_MIXEDPRECISION_CONFIG,
-                                                         fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                                         network_editor: List[EditRule] = [],
-                                                         gptq_config: GradientPTQConfig = None,
-                                                         analyze_similarity: bool = False,
-                                                         target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> \
-    Tuple[Model, UserInformation]:
+        # Ignore hessian service since is not used in QAT at the moment
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_KERAS_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_KERAS_INFO, fw_impl, tb_w)
+
+        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
+        qat_model, user_info = KerasModelBuilder(graph=tg,
+                                                 fw_info=DEFAULT_KERAS_INFO,
+                                                 wrapper=_qat_wrapper,
+                                                 get_activation_quantizer_holder_fn=partial(get_activation_quantizer_holder,
+                                                                                            qat_config=qat_config)).build_model()
+
+        user_info.mixed_precision_cfg = bit_widths_config
+        #TODO: remove the last output after updating documentation.
+        return qat_model, user_info, {}
+
+
+    def keras_quantization_aware_training_finalize_experimental(in_model: Model) -> Model:
         """
-         Quantize a pretrained Keras model using post-training quantization. By default, the model is quantized
-         using a symmetric constraint quantization thresholds (power of two) as defined in the default
-         TargetPlatformCapabilities.
-         The model is first optimized using several transformations (e.g. BatchNormalization folding to
-         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-         being collected for each layer's output (and input, depends on the quantization configuration).
-         For each possible bit width (per operator, as defined in the TargetPlatformCapabilities) a
-         threshold is then being calculated using the collected statistics.
-         Then, using an ILP solver we find a mixed-precision configuration, and set a bit width
-         for each quantizer (for both activations and weights quantizers, by default).
-         In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
-         or activation_memory (or both) is set (in bytes).
-         The model is then quantized (both coefficients and activations by default).
-         If gptq_config is passed, the quantized weights are optimized using gradient based post
-         training quantization by comparing points between the float and quantized models, and minimizing the
-         observed loss.
-         Notice that this feature is experimental.
+         Convert a model fine-tuned by the user (Trainable quantizers) to a model with Inferable quantizers.
 
          Args:
-             in_model (Model): Keras model to quantize.
-             representative_data_gen (Callable): Dataset used for calibration.
-             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
-             n_iter (int): Number of calibration iterations to run.
-             quant_config (MixedPrecisionQuantizationConfig): QuantizationConfig containing parameters of how the model should be quantized.
-             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-             network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
-             gptq_config (GradientPTQConfig): Configuration for using GPTQ (e.g. optimizer).
-             analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-
+             in_model (Model): Keras model to replace TrainableQuantizer with InferableQuantizer
 
          Returns:
-             A quantized model and information the user may need to handle the quantized model.
+             A quantized model with Inferable quantizers
 
          Examples:
 
              Import MCT:
 
              >>> import model_compression_toolkit as mct
 
@@ -202,94 +230,71 @@
 
              >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
              >>> model = MobileNetV2()
 
              Create a random dataset generator:
 
              >>> import numpy as np
-             >>> def repr_datagen(): return [np.random.random((1,224,224,3))]
+             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
+
+             Create a MCT core config, containing the quantization configuration:
 
-             Create a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+             >>> config = mct.core.CoreConfig()
+
+             If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
              The candidates bitwidth for quantization should be defined in the target platform model:
 
-             >>> config = mct.core.MixedPrecisionQuantizationConfig()
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfig())
 
-             Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
+             For mixed-precision set a target ResourceUtilization object:
+             Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
              that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
              while the bias will not):
 
-             >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+             >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
-             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
              quantized model:
 
-             >>> quantized_model, quantization_info = mct.keras_post_training_quantization_mixed_precision(model,repr_datagen, target_kpi=kpi, n_iter=10, quant_config=config)
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(model, repr_datagen, ru, core_config=config)
 
-             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/experimental_api_docs/modules/mixed_precision_quantization_config.html#model_compression_toolkit.MixedPrecisionQuantizationConfigV2>`_.
+             Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
 
-         """
-        Logger.warning('keras_post_training_quantization_mixed_precision is deprecated and will be removed '
-                       'in the future. Please use mct.ptq.keras_post_training_quantization_experimental instead.')
+             >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
+             >>> quantized_model = mct.qat.keras_quantization_aware_training_finalize_experimental(quantized_model)
 
-        KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
-
-        if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization API,"
-                                "or pass a valid mixed precision configuration.")
-
-        Logger.info("Using experimental mixed-precision quantization. "
-                           "If you encounter an issue please file a bug.")
-
-        quantization_config, mp_config = quant_config.separate_configs()
-        core_config = CoreConfig(quantization_config=quantization_config,
-                                 mixed_precision_config=mp_config,
-                                 debug_config=DebugConfig(analyze_similarity=analyze_similarity,
-                                                          network_editor=network_editor)
-                                 )
-
-        tb_w = _init_tensorboard_writer(fw_info)
-
-        fw_impl = KerasImplementation()
-
-        # convert old representative dataset generation to a generator
-        def _representative_data_gen():
-            for _ in range(n_iter):
-                yield representative_data_gen()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=_representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
-
-        if gptq_config is None:
-            tg = ptq_runner(tg, _representative_data_gen, core_config, fw_info, fw_impl, tb_w)
-        else:
-            gptq_config_v2 = GradientPTQConfigV2.from_v1(n_iter, gptq_config)
-            tg = gptq_runner(tg, core_config, gptq_config_v2, _representative_data_gen, _representative_data_gen,
-                             fw_info, fw_impl, tb_w)
-
-        if core_config.debug_config.analyze_similarity:
-            analyzer_model_quantization(_representative_data_gen, tb_w, tg, fw_impl, fw_info)
+         """
+        Logger.warning(
+            f"keras_quantization_aware_training_finalize_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        def _export(layer):
+            if isinstance(layer, KerasTrainableQuantizationWrapper):
+                layer = layer.convert_to_inferable_quantizers()
+            # In the KerasActivationQuantizationHolder case - converting the quantizers only
+            # is not enough. We need to create a new layer with inferable quantizers. The reason for that
+            # is that if we only convert the quantizers, the layer will have some weights (such as min, max,
+            # threshold) that do not match the configuration, thus loading such a model will fail.
+            # To overcome this, the convert_to_inferable_quantizers of KerasActivationQuantizationHolder
+            # creates a new layer from its new configuration after converting the trainable quantizer
+            # to an inferable quantizer.
+            elif isinstance(layer, KerasActivationQuantizationHolder):
+                layer = layer.convert_to_inferable_quantizers()
+            return layer
 
-        quantized_model, user_info = export_model(tg, fw_info, fw_impl, tb_w, bit_widths_config)
+        # clone each layer in the model and apply _export to layers with TrainableQuantizeWrappers
+        exported_model = tf.keras.models.clone_model(in_model, input_tensors=None, clone_function=_export)
 
-        return quantized_model, user_info
+        return exported_model
 
 else:
-    # If tensorflow or tensorflow_model_optimization are not installed,
+    # If tensorflow is not installed,
     # we raise an exception when trying to use these functions.
-    def keras_post_training_quantization(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_post_training_quantization. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_quantization_aware_training_init_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_quantization_aware_training_init_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
 
 
-    def keras_post_training_quantization_mixed_precision(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_post_training_quantization_mixed_precision. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_quantization_aware_training_finalize_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_quantization_aware_training_finalize_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/logger.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/logger.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 
 import logging
 import os
 from datetime import datetime
 from pathlib import Path
 
-LOGGER_NAME = 'Constrained Model Optimization'
+LOGGER_NAME = 'Model Compression Toolkit'
 
 
 class Logger:
     # Logger has levels of verbosity.
     log_level_translate = {
         'debug': logging.DEBUG,
         'info': logging.INFO,
@@ -113,25 +113,14 @@
             msg: Message to log.
 
         """
         Logger.get_logger().critical(msg)
         raise Exception(msg)
 
     @staticmethod
-    def exception(msg: str):
-        """
-        Log a message at 'exception' severity and raise an exception.
-        Args:
-            msg: Message to log.
-
-        """
-        Logger.get_logger().exception(msg)
-        raise Exception(msg)
-
-    @staticmethod
     def debug(msg: str):
         """
         Log a message at 'debug' severity.
 
         Args:
             msg: Message to log.
 
@@ -168,15 +157,14 @@
         Log a message at 'error' severity and raise an exception.
 
         Args:
             msg: Message to log.
 
         """
         Logger.get_logger().error(msg)
-        raise Exception(msg)
 
 
 def set_log_folder(folder: str, level: int = logging.INFO):
     """
     Set a directory path for saving a log file.
 
     Args:
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,9 +9,9 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization_experimental
-from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization_experimental
+from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization
+from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/keras/quantization_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/keras/pruning_facade.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,176 +1,152 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Callable
+from typing import Callable, Tuple
 
-from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit.logger import Logger
+from model_compression_toolkit import get_target_platform_capabilities
 from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.pruning.pruner import Pruner
+from model_compression_toolkit.core.common.pruning.pruning_config import PruningConfig
+from model_compression_toolkit.core.common.pruning.pruning_info import PruningInfo
+from model_compression_toolkit.core.common.quantization.set_node_quantization_config import set_quantization_configuration_to_graph
+from model_compression_toolkit.core.graph_prep_runner import read_model_to_graph
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.exporter import export_model
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
-from model_compression_toolkit.ptq.runner import ptq_runner
+from model_compression_toolkit.core.common.quantization.quantization_config import DEFAULTCONFIG
+from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
 
 if FOUND_TF:
+    from model_compression_toolkit.core.keras.back2framework.float_model_builder import FloatKerasModelBuilder
+    from model_compression_toolkit.core.keras.pruning.pruning_keras_implementation import PruningKerasImplementation
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
-    from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
     from tensorflow.keras.models import Model
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
 
-    from model_compression_toolkit import get_target_platform_capabilities
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
-
-    def keras_post_training_quantization_experimental(in_model: Model,
-                                                      representative_data_gen: Callable,
-                                                      target_kpi: KPI = None,
-                                                      core_config: CoreConfig = CoreConfig(),
-                                                      target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                      new_experimental_exporter: bool = True):
+    def keras_pruning_experimental(model: Model,
+                                   target_resource_utilization: ResourceUtilization,
+                                   representative_data_gen: Callable,
+                                   pruning_config: PruningConfig = PruningConfig(),
+                                   target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> Tuple[Model, PruningInfo]:
         """
-         Quantize a trained Keras model using post-training quantization. The model is quantized using a
-         symmetric constraint quantization thresholds (power of two).
-         The model is first optimized using several transformations (e.g. BatchNormalization folding to
-         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-         being collected for each layer's output (and input, depends on the quantization configuration).
-         For each possible bit width (per layer) a threshold is then being calculated using the collected
-         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
-         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
-         (both coefficients and activations by default).
-         In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
-         is set (in bytes).
-
-         Args:
-             in_model (Model): Keras model to quantize.
-             representative_data_gen (Callable): Dataset used for calibration.
-             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
-             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-             new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
+        Perform structured pruning on a Keras model to meet a specified target resource utilization.
+        This function prunes the provided model according to the target resource utilization by grouping and pruning
+        channels based on each layer's SIMD configuration in the Target Platform Capabilities (TPC).
+        By default, the importance of each channel group is determined using the Label-Free Hessian
+        (LFH) method, assessing each channel's sensitivity to the Hessian of the loss function.
+        This pruning strategy considers groups of channels together for a more hardware-friendly
+        architecture. The process involves analyzing the model with a representative dataset to
+        identify groups of channels that can be removed with minimal impact on performance.
+
+        Notice that the pruned model must be retrained to recover the compressed model's performance.
+
+        Args:
+            model (Model): The original Keras model to be pruned.
+            target_resource_utilization (ResourceUtilization): The target Key Performance Indicators to be achieved through pruning.
+            representative_data_gen (Callable): A function to generate representative data for pruning analysis.
+            pruning_config (PruningConfig): Configuration settings for the pruning process. Defaults to standard config.
+            target_platform_capabilities (TargetPlatformCapabilities): Platform-specific constraints and capabilities. Defaults to DEFAULT_KERAS_TPC.
 
-         Returns:
+        Returns:
+            Tuple[Model, PruningInfo]: A tuple containing the pruned Keras model and associated pruning information.
 
-             A quantized model and information the user may need to handle the quantized model.
+        Note:
+            The pruned model should be fine-tuned or retrained to recover or improve its performance post-pruning.
 
-         Examples:
+        Examples:
 
             Import MCT:
 
             >>> import model_compression_toolkit as mct
 
             Import a Keras model:
 
-            >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
-            >>> model = MobileNetV2()
+            >>> from tensorflow.keras.applications.resnet50 import ResNet50
+            >>> model = ResNet50()
 
-            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
-            In this example a random dataset of 10 batches each containing 4 images is used.
+            Create a random dataset generator:
 
             >>> import numpy as np
-            >>> num_calibration_batches = 10
-            >>> def repr_datagen():
-            >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 224, 224, 3))]
-
-            Create a MCT core config, containing the quantization configuration:
-
-            >>> config = mct.core.CoreConfig()
-
-            If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
-            The candidates bitwidth for quantization should be defined in the target platform model.
-            In this example we use 1 image to search mixed-precision configuration:
-
-            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
-
-            For mixed-precision set a target KPI object:
-            Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
-            that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
-            while the bias will not):
-
-            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
-
-            Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-            quantized model:
+            >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
 
-            >>> quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(model, repr_datagen, kpi, core_config=config)
+            Define a target resource utilization for pruning.
+            Here, we aim to reduce the memory footprint of weights by 50%, assuming the model weights
+            are represented in float32 data type (thus, each parameter is represented using 4 bytes):
 
-            For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+            >>> dense_nparams = sum([l.count_params() for l in model.layers])
+            >>> target_resource_utilization = mct.core.ResourceUtilization(weights_memory=dense_nparams * 4 * 0.5)
 
-         """
+            Optionally, define a pruning configuration. num_score_approximations can be passed
+            to configure the number of importance scores that will be calculated for each channel.
+            A higher value for this parameter yields more precise score approximations but also
+            extends the duration of the pruning process:
 
-        fw_info = DEFAULT_KERAS_INFO
+            >>> pruning_config = mct.pruning.PruningConfig(num_score_approximations=1)
 
-        KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
+            Perform pruning:
 
-        if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            >>> pruned_model, pruning_info = mct.pruning.keras_pruning_experimental(model=model, target_resource_utilization=target_resource_utilization, representative_data_gen=repr_datagen, pruning_config=pruning_config)
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
-
-        fw_impl = KerasImplementation()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
-
-        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
-
-        if core_config.debug_config.analyze_similarity:
-            analyzer_model_quantization(representative_data_gen,
-                                        tb_w, tg,
-                                        fw_impl,
-                                        fw_info)
-
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling keras_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_keras_model(tg)
-
-        return export_model(tg,
-                            fw_info,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        """
 
+        Logger.warning(f"keras_pruning_experimental is experimental and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
+
+        # Instantiate the Keras framework implementation.
+        fw_impl = PruningKerasImplementation()
+
+        # Convert the original Keras model to an internal graph representation.
+        float_graph = read_model_to_graph(model,
+                                          representative_data_gen,
+                                          target_platform_capabilities,
+                                          DEFAULT_KERAS_INFO,
+                                          fw_impl)
+
+        # Apply quantization configuration to the graph. This step is necessary even when not quantizing,
+        # as it prepares the graph for the pruning process.
+        float_graph_with_compression_config = set_quantization_configuration_to_graph(float_graph,
+                                                                                      quant_config=DEFAULTCONFIG,
+                                                                                      mixed_precision_enable=False)
+
+        # Create a Pruner object with the graph and configuration.
+        pruner = Pruner(float_graph_with_compression_config,
+                        DEFAULT_KERAS_INFO,
+                        fw_impl,
+                        target_resource_utilization,
+                        representative_data_gen,
+                        pruning_config,
+                        target_platform_capabilities)
+
+        # Apply the pruning process.
+        pruned_graph = pruner.prune_graph()
+
+        # Retrieve pruning information which includes the pruning masks and scores.
+        pruning_info = pruner.get_pruning_info()
+
+        # Rebuild the pruned graph back into a trainable Keras model.
+        pruned_model, _ = FloatKerasModelBuilder(graph=pruned_graph).build_model()
+        pruned_model.trainable = True
 
+        # Return the pruned model along with its pruning information.
+        return pruned_model, pruning_info
 
 else:
-    # If tensorflow or tensorflow_model_optimization are not installed,
+    # If tensorflow is not installed,
     # we raise an exception when trying to use these functions.
-    def keras_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_post_training_quantization_experimental. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_pruning_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_pruning_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/ptq/runner.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/ptq/runner.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,9 +10,9 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from model_compression_toolkit.qat.common.qat_config import QATConfig, TrainingMethod
 
-from model_compression_toolkit.qat.keras.quantization_facade import keras_quantization_aware_training_init, keras_quantization_aware_training_finalize
-from model_compression_toolkit.qat.pytorch.quantization_facade import pytorch_quantization_aware_training_init, pytorch_quantization_aware_training_finalize
+from model_compression_toolkit.qat.keras.quantization_facade import keras_quantization_aware_training_init_experimental, keras_quantization_aware_training_finalize_experimental
+from model_compression_toolkit.qat.pytorch.quantization_facade import pytorch_quantization_aware_training_init_experimental, pytorch_quantization_aware_training_finalize_experimental
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/common/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/common/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/common/qat_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/common/qat_config.py`

 * *Files 9% similar despite different names*

```diff
@@ -20,38 +20,43 @@
 from model_compression_toolkit.logger import Logger
 
 
 def is_qat_applicable(node: common.BaseNode,
                       fw_info: FrameworkInfo) -> bool:
     """
     A function for deciding if a layer should be fine-tuned during QAT
+
     Args:
         node (BaseNode): Node for quantization decision
         fw_info (FrameworkInfo): Pytorch quantization information
 
     Returns:
         A boolean whether the layer is to be wrapped with a QuantizeWrapper
     """
 
-    if node.is_weights_quantization_enabled() and not fw_info.is_kernel_op(node.type):
-        Logger.error("QAT Error: Quantizing a node without a kernel isn't supported")
-    return node.is_weights_quantization_enabled() or node.is_activation_quantization_enabled()
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)[0]
+    return (kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)) \
+            or node.is_activation_quantization_enabled()
+
 
 
 class TrainingMethod(Enum):
     """
     An enum for selecting a QAT training method
 
     STE - Standard straight-through estimator. Includes PowerOfTwo, symmetric & uniform quantizers
 
     DQA -  DNN Quantization with Attention. Includes a smooth quantization introduces by DQA method
 
+    LSQ - Learned Step size Quantization. Includes PowerOfTwo, symmetric & uniform quantizers: https://arxiv.org/pdf/1902.08153.pdf
+
     """
     STE = "STE",
-    DQA = "DQA"
+    DQA = "DQA",
+    LSQ = "LSQ"
 
 
 class QATConfig:
     """
     QAT configuration class.
     """
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,7 +11,9 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import model_compression_toolkit.qat.keras.quantizer.ste_rounding.symmetric_ste
 import model_compression_toolkit.qat.keras.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.keras.quantizer.lsq.symmetric_lsq
+import model_compression_toolkit.qat.keras.quantizer.lsq.uniform_lsq
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -40,10 +40,9 @@
 
 else:
     class BaseKerasQATTrainableQuantizer(BaseKerasTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
 
             super().__init__(quantization_config)
-            Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                            'when using BaseKerasQATTrainableQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasQATTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/quant_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,14 +13,31 @@
 # limitations under the License.
 # ==============================================================================
 
 import tensorflow as tf
 from typing import Tuple
 
 
+def ste_round(x: tf.Tensor) -> tf.Tensor:
+    """
+    Return the rounded values of a tensor.
+    """
+    error = tf.stop_gradient(tf.math.round(x) - x)
+    return error + x
+
+
+def grad_scale(x: tf.Tensor, scale=1.0) -> tf.Tensor:
+    """
+    Return x in forward and x*scale in backward (for scaling the gradients).
+    """
+    x_scaled = scale * x
+    error = tf.stop_gradient(x - x_scaled)
+    return error + x_scaled
+
+
 def adjust_range_to_include_zero(range_min: tf.Tensor,
                                  range_max: tf.Tensor,
                                  n_bits: int) -> Tuple[tf.Tensor, tf.Tensor]:
     """
     Adjusting the quantization range to include representation of 0.0 in the quantization grid.
     For per_channel quantization range_min\range_max should be tensors in the specific shape that allows
     quantization along the channel_axis.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,97 +8,96 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple, Dict, List, Callable
+from typing import List, Dict, Tuple, Callable
+
+from mct_quantizers import PytorchActivationQuantizationHolder, QuantizationTarget
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
-from mct_quantizers import QuantizationTarget, KerasActivationQuantizationHolder
+from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
-    get_trainable_quantizer_weights_config, get_trainable_quantizer_activation_config, \
-    get_trainable_quantizer_quantization_candidates
+    get_trainable_quantizer_quantization_candidates, get_trainable_quantizer_weights_config, \
+    get_trainable_quantizer_activation_config
+from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
-
 def get_activation_quantizer_holder(n: common.BaseNode,
                                     qat_config: QATConfig) -> Callable:
     """
-    Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
+    Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
     If the layer is not supposed to be wrapped with activation quantizers - return None.
 
     Args:
-        n: Node to get KerasActivationQuantizationHolder to attach in its output.
-        qat_config: Configuration of QAT (such as training methods for example).
+        n: Node for which to retrieve anActivationQuantizationHolder to attach to its output.
+        qat_config: QAT configuration (for example, training methods).
 
     Returns:
-        A KerasActivationQuantizationHolder layer for the node activation quantization.
+        A ActivationQuantizationHolder layer for the node's activation quantization.
     """
     _, activation_quantizers = quantization_builder(n,
-                                                    qat_config,
-                                                    DEFAULT_KERAS_INFO)
+                                                    qat_config)
 
     # Holder by definition uses a single quantizer for the activation quantization
     # thus we make sure this is the only possible case (unless it's a node with no activation
     # quantization, which in this case has an empty list).
     if len(activation_quantizers) == 1:
-        return KerasActivationQuantizationHolder(activation_quantizers[0])
-    Logger.error(f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}')
+        return PytorchActivationQuantizationHolder(activation_quantizers[0])
+    Logger.critical(f'ActivationQuantizationHolder supports only a single quantizer, but ({len(activation_quantizers)}) quantizers were found for node {n}.')
 
 
 def quantization_builder(n: common.BaseNode,
                          qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
-                         ) -> Tuple[Dict[str, BaseKerasQATTrainableQuantizer], List[BaseKerasQATTrainableQuantizer]]:
+                         kernel_attr: str = None,
+                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
+                                    List[BasePytorchQATTrainableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration.
 
     Args:
         n: Node to build its QuantizeConfig.
         qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.
+        activation_quantizers: A list of activations quantization, one for each layer output.).
     """
+
     if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
+        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n, kernel_attr)
     else:
         wq_cand, aq_cand = None, None
 
     weight_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
-
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during QAT
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Weights,
                                                         qat_config.weight_training_method,
                                                         quant_method,
-                                                        BaseKerasQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                            **qat_config.weight_quantizer_params_override)})
+                                                        BasePytorchQATTrainableQuantizer)
+
+        weight_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                               attr_name=kernel_attr,
+                                                                                               weights_quantization_candidates=wq_cand),
+                                                        **qat_config.weight_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
-        # single output -> normalize to list of output_shapes
-        output_shapes = n.output_shape if isinstance(n.output_shape[0], (list, tuple)) else [n.output_shape]
-
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Activation,
                                                         qat_config.activation_training_method,
                                                         quant_method,
-                                                        BaseKerasQATTrainableQuantizer)
+                                                        BasePytorchQATTrainableQuantizer)
 
         activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)] * len(output_shapes)
+                                                 **qat_config.activation_quantizer_params_override)]
 
     return weight_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 1% similar despite different names*

```diff
@@ -20,29 +20,30 @@
 from tensorflow.python.framework.tensor_shape import TensorShape
 from model_compression_toolkit.constants import SIGNED
 from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
 
 from model_compression_toolkit.qat import TrainingMethod
 
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from mct_quantizers import QuantizationTarget, mark_quantizer, KerasQuantizationWrapper
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
+from mct_quantizers import QuantizationTarget, mark_quantizer
 from model_compression_toolkit.qat.common import THRESHOLD_TENSOR
 from model_compression_toolkit import constants as C
 
 from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig
 from mct_quantizers.keras.quantizers import WeightsPOTInferableQuantizer, WeightsSymmetricInferableQuantizer, \
     ActivationPOTInferableQuantizer, ActivationSymmetricInferableQuantizer
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEWeightQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer inputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
@@ -79,15 +80,15 @@
         self.min = delta * min_int
         self.max = delta * max_int
 
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: KerasQuantizationWrapper):
+                                layer: KerasTrainableQuantizationWrapper):
         """
         Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
             tensor_shape: tensor shape of the quantized tensor.
             name: Tensor name.
             layer: Layer to quantize.
@@ -168,15 +169,15 @@
                                                       per_channel=self.per_channel,
                                                       channel_axis=self.channel_axis,
                                                       input_rank=len(self.threshold_shape))
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEActivationQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer outputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
@@ -201,15 +202,15 @@
         max_int = (2 ** (self.num_bits - int(self.signed))) - 1
         self.min = delta * min_int
         self.max = delta * max_int
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: KerasQuantizationWrapper):
+                                layer: KerasTrainableQuantizationWrapper):
         """
         Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
             tensor_shape: tensor shape of the quantized tensor.
             name: Tensor name.
             layer: Layer to quantize.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,17 +13,18 @@
 # limitations under the License.
 # ==============================================================================
 import numpy as np
 import tensorflow as tf
 from tensorflow.python.framework.tensor_shape import TensorShape
 from model_compression_toolkit.constants import RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.qat import TrainingMethod
 
-from mct_quantizers import mark_quantizer, QuantizationMethod, QuantizationTarget, KerasQuantizationWrapper
+from mct_quantizers import mark_quantizer, QuantizationMethod, QuantizationTarget
 from mct_quantizers.keras.quantizers import \
     BaseKerasInferableQuantizer, WeightsUniformInferableQuantizer, ActivationUniformInferableQuantizer
 
 from model_compression_toolkit.qat.keras.quantizer.quant_utils import adjust_range_to_include_zero
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 from model_compression_toolkit import constants as C
 
@@ -31,15 +32,15 @@
 from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.UNIFORM],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEUniformWeightQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer inputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
@@ -68,15 +69,15 @@
             self.perm_vec[len(self.min_max_shape) - 1] = self.channel_axis
         else:
             self.perm_vec = None
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: KerasQuantizationWrapper):
+                                layer: KerasTrainableQuantizationWrapper):
         """
         Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
             tensor_shape: tensor shape of the quantized tensor.
             name: Tensor name.
             layer: Layer to quantize.
@@ -145,15 +146,15 @@
                                                 per_channel=self.per_channel,
                                                 channel_axis=self.channel_axis,
                                                 input_rank=len(self.min_max_shape))
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.UNIFORM],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEUniformActivationQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer outputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
@@ -168,15 +169,15 @@
         self.num_bits = quantization_config.activation_n_bits
         self.min_range = quantization_config.activation_quantization_params[C.RANGE_MIN]
         self.max_range = quantization_config.activation_quantization_params[C.RANGE_MAX]
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: KerasQuantizationWrapper):
+                                layer: KerasTrainableQuantizationWrapper):
         """
         Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
             tensor_shape: tensor shape of the quantized tensor.
             name: Tensor name.
             layer: Layer to quantize.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantization_facade.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/runner.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,233 +8,206 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
-from typing import Callable
-from functools import partial
 
-from model_compression_toolkit.constants import FOUND_TORCH, PYTORCH
 
-from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core import common
-from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
-    TargetPlatformCapabilities
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
-from model_compression_toolkit.ptq.runner import ptq_runner
-
-if FOUND_TORCH:
-    import torch.nn as nn
-    from torch.nn import Module
-    from mct_quantizers import PytorchActivationQuantizationHolder
-    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
-    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-    from mct_quantizers import PytorchQuantizationWrapper
-    from model_compression_toolkit import get_target_platform_capabilities
-    from model_compression_toolkit.qat.common.qat_config import QATConfig
-    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import get_activation_quantizer_holder
-    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import quantization_builder
-
-    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
-
-
-    def qat_wrapper(n: common.BaseNode,
-                    module: nn.Module,
-                    qat_config: QATConfig):
-        """
-        A function which takes a computational graph node and a pytorch module and perform the quantization wrapping
-        Args:
-            n: A node of mct graph.
-            module: A Pytorch module
-            qat_config (QATConfig): QAT configuration
-        Returns: Wrapped layer
-
-        """
-        if is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
-            weights_quantizers, _ = quantization_builder(n, qat_config, DEFAULT_PYTORCH_INFO)
-            if len(weights_quantizers) > 0:
-                return PytorchQuantizationWrapper(module, weights_quantizers)
-        return module
-
-
-    def pytorch_quantization_aware_training_init(in_model: Module,
-                                                 representative_data_gen: Callable,
-                                                 target_kpi: KPI = None,
-                                                 core_config: CoreConfig = CoreConfig(),
-                                                 qat_config: QATConfig = QATConfig(),
-                                                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                                                 target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
-        """
-         Prepare a trained Pytorch model for quantization aware training. First the model quantization is optimized
-         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
-         quantized using a symmetric quantization thresholds (power of two).
-         The model is first optimized using several transformations (e.g. BatchNormalization folding to
-         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-         being collected for each layer's output (and input, depends on the quantization configuration).
-         For each possible bit width (per layer) a threshold is then being calculated using the collected
-         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
-         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
-         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
-         quantization wrapper's weight quantizer.
-         In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
-         is set (in bytes).
-
-         Args:
-             in_model (Model): Pytorch model to quantize.
-             representative_data_gen (Callable): Dataset used for initial calibration.
-             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
-             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-             qat_config (QATConfig): QAT configuration
-             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).  `Default Pytorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Pytorch model according to.
-
-         Returns:
-
-             A quantized model.
-             User information that may be needed to handle the quantized model.
-
-         Examples:
-
-             Import MCT:
-
-             >>> import model_compression_toolkit as mct
-
-             Import a Pytorch model:
-
-             >>> from torchvision.models import mobilenet_v2
-             >>> model = mobilenet_v2(pretrained=True)
-
-            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
-            In this example a random dataset of 10 batches each containing 4 images is used.
-
-            >>> import numpy as np
-            >>> num_calibration_batches = 10
-            >>> def repr_datagen():
-            >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 3, 224, 224))]
-
-             Create a MCT core config, containing the quantization configuration:
-
-             >>> config = mct.core.CoreConfig()
-
-             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-             quantized model. Now the model contains quantizer wrappers for fine tunning the weights:
-
-             >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
-
-             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
-
-         """
-
-        if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                             "MixedPrecisionQuantizationConfigV2. Please use pytorch_post_training_quantization API,"
-                             "or pass a valid mixed precision configuration.")
-
-            Logger.info("Using experimental mixed-precision quantization. "
-                        "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
-
-        fw_impl = PytorchImplementation()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=DEFAULT_PYTORCH_INFO,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
+from typing import Callable, Tuple, Any, List, Dict
 
-        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
+import numpy as np
 
-        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
-
-        qat_model, user_info = PyTorchModelBuilder(graph=tg,
-                                                   fw_info=fw_info,
-                                                   wrapper=_qat_wrapper,
-                                                   get_activation_quantizer_holder_fn=partial(
-                                                       get_activation_quantizer_holder,
-                                                       qat_config=qat_config)).build_model()
-
-        user_info.mixed_precision_cfg = bit_widths_config
-
-        # Remove fw_info from graph to enable saving the pytorch model (fw_info can not be pickled)
-        delattr(qat_model.graph, 'fw_info')
-
-        return qat_model, user_info
-
-
-    def pytorch_quantization_aware_training_finalize(in_model: Module):
-        """
-         Convert a model fine-tuned by the user to a network with QuantizeWrappers containing
-         InferableQuantizers, that quantizes both the layers weights and outputs
-
-         Args:
-             in_model (Model): Pytorch model to remove QuantizeWrappers.
-
-         Returns:
-             A quantized model with QuantizeWrappers and InferableQuantizers.
-
-         Examples:
-
-             Import MCT:
-
-             >>> import model_compression_toolkit as mct
-
-             Import a Pytorch model:
-
-             >>> from torchvision.models import mobilenet_v2
-             >>> model = mobilenet_v2(pretrained=True)
-
-             Create a random dataset generator:
-
-             >>> import numpy as np
-             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
-
-             Create a MCT core config, containing the quantization configuration:
-
-             >>> config = mct.core.CoreConfig()
-
-             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-             quantized model:
-
-             >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
-
-             Use the quantized model for fine-tuning. Finally, remove the quantizer wrappers and keep a quantize model ready for inference.
-
-             >>> quantized_model = mct.pytorch_quantization_aware_training_finalize(quantized_model)
-
-         """
-        for _, layer in in_model.named_children():
-            if isinstance(layer, (PytorchQuantizationWrapper, PytorchActivationQuantizationHolder)):
-                layer.convert_to_inferable_quantizers()
-
-        return in_model
-
-
-else:
-    # If torch is not installed,
-    # we raise an exception when trying to use these functions.
-    def pytorch_quantization_aware_training_init(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_quantization_aware_training_init. '
-                        'Could not find the torch package.')  # pragma: no cover
-
-
-    def pytorch_quantization_aware_training_finalize(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_quantization_aware_training_finalize. '
-                        'Could not find the torch package.')  # pragma: no cover
+from model_compression_toolkit.core.common import FrameworkInfo
+from model_compression_toolkit.core.common.hessian.hessian_info_service import HessianInfoService
+from model_compression_toolkit.core.graph_prep_runner import graph_preparation_runner
+from model_compression_toolkit.core.quantization_prep_runner import quantization_preparation_runner
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.mixed_precision.bit_width_setter import set_bit_widths
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_functions_mapping import ru_functions_mapping
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_facade import search_bit_width
+from model_compression_toolkit.core.common.network_editors.edit_network import edit_network_graph
+from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
+    WeightsFinalBitwidthConfigVisualizer, \
+    ActivationFinalBitwidthConfigVisualizer
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter, \
+    finalize_bitwidth_in_tb
+
+
+def core_runner(in_model: Any,
+                representative_data_gen: Callable,
+                core_config: CoreConfig,
+                fw_info: FrameworkInfo,
+                fw_impl: FrameworkImplementation,
+                tpc: TargetPlatformCapabilities,
+                target_resource_utilization: ResourceUtilization = None,
+                tb_w: TensorboardWriter = None):
+    """
+    Quantize a trained model using post-training quantization.
+    First, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding
+    layers).
+    Second, statistics (e.g. min/max, histogram, etc.) are collected for each layer's output
+    (and input, depends on the quantization configuration) using a given representative dataset.
+    Next, quantization parameters are calculated using the collected statistics
+    (both coefficients and activations by default).
+
+    Args:
+        in_model: Model to quantize.
+        representative_data_gen: Dataset used for calibration.
+        core_config: CoreConfig containing parameters of how the model should be quantized
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
+        groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tpc: TargetPlatformCapabilities object that models the inference target platform and
+                                              the attached framework operator's information.
+        target_resource_utilization: ResourceUtilization to constraint the search of the mixed-precision configuration for the model.
+        tb_w: TensorboardWriter object for logging
+
+    Returns:
+        An internal graph representation of the input model.
+
+    """
+
+    # Warn is representative dataset has batch-size == 1
+    batch_data = iter(representative_data_gen()).__next__()
+    if isinstance(batch_data, list):
+        batch_data = batch_data[0]
+    if batch_data.shape[0] == 1:
+        Logger.warning('representative_data_gen generates a batch size of 1 which can be slow for optimization:'
+                       ' consider increasing the batch size')
+
+    # Checking whether to run mixed precision quantization
+    if target_resource_utilization is not None:
+        if core_config.mixed_precision_config is None:
+            Logger.critical("Provided an initialized target_resource_utilization, that means that mixed precision quantization is "
+                            "enabled, but the provided MixedPrecisionQuantizationConfig is None.")
+        core_config.mixed_precision_config.set_mixed_precision_enable()
+
+    graph = graph_preparation_runner(in_model,
+                                     representative_data_gen,
+                                     core_config.quantization_config,
+                                     fw_info,
+                                     fw_impl,
+                                     tpc,
+                                     tb_w,
+                                     mixed_precision_enable=core_config.mixed_precision_enable)
+
+    hessian_info_service = HessianInfoService(graph=graph,
+                                              representative_dataset=representative_data_gen,
+                                              fw_impl=fw_impl)
+
+    tg = quantization_preparation_runner(graph=graph,
+                                         representative_data_gen=representative_data_gen,
+                                         core_config=core_config,
+                                         fw_info=fw_info,
+                                         fw_impl=fw_impl,
+                                         tb_w=tb_w)
+
+    ######################################
+    # Finalize bit widths
+    ######################################
+    if core_config.mixed_precision_enable:
+        if core_config.mixed_precision_config.configuration_overwrite is None:
+
+            bit_widths_config = search_bit_width(tg,
+                                                 fw_info,
+                                                 fw_impl,
+                                                 target_resource_utilization,
+                                                 core_config.mixed_precision_config,
+                                                 representative_data_gen,
+                                                 hessian_info_service=hessian_info_service)
+        else:
+            Logger.warning(
+                f'Mixed Precision has overwrite bit-width configuration{core_config.mixed_precision_config.configuration_overwrite}')
+            bit_widths_config = core_config.mixed_precision_config.configuration_overwrite
+
+    else:
+        bit_widths_config = []
+
+    tg = set_bit_widths(core_config.mixed_precision_enable,
+                        tg,
+                        bit_widths_config)
+
+    # Edit the graph again after finalizing the configurations.
+    # This is since some actions regard the final configuration and should be edited.
+    edit_network_graph(tg, fw_info, core_config.debug_config.network_editor)
+
+    _set_final_resource_utilization(graph=tg,
+                                    final_bit_widths_config=bit_widths_config,
+                                    ru_functions_dict=ru_functions_mapping,
+                                    fw_info=fw_info,
+                                    fw_impl=fw_impl)
+
+    if core_config.mixed_precision_enable:
+        # Retrieve lists of tuples (node, node's final weights/activation bitwidth)
+        weights_conf_nodes_bitwidth = tg.get_final_weights_config(fw_info)
+        activation_conf_nodes_bitwidth = tg.get_final_activation_config()
+
+        if len(weights_conf_nodes_bitwidth) > 0:
+            Logger.info(
+                f'Final weights bit-width configuration: {[node_b[1] for node_b in weights_conf_nodes_bitwidth]}')
+
+        if len(activation_conf_nodes_bitwidth) > 0:
+            Logger.info(
+                f'Final activation bit-width configuration: {[node_b[1] for node_b in activation_conf_nodes_bitwidth]}')
+
+        if tb_w is not None:
+            finalize_bitwidth_in_tb(tb_w, weights_conf_nodes_bitwidth, activation_conf_nodes_bitwidth)
+
+    return tg, bit_widths_config, hessian_info_service
+
+
+def _set_final_resource_utilization(graph: Graph,
+                                    final_bit_widths_config: List[int],
+                                    ru_functions_dict: Dict[RUTarget, Tuple[MpRuMetric, MpRuAggregation]],
+                                    fw_info: FrameworkInfo,
+                                    fw_impl: FrameworkImplementation):
+    """
+    Computing the resource utilization of the model according to the final bit-width configuration,
+    and setting it (inplace) in the graph's UserInfo field.
+
+    Args:
+        graph: Graph to compute the resource utilization for.
+        final_bit_widths_config: The final bit-width configuration to quantize the model accordingly.
+        ru_functions_dict: A mapping between a RUTarget and a pair of resource utilization method and resource utilization aggregation functions.
+        fw_info: A FrameworkInfo object.
+        fw_impl: FrameworkImplementation object with specific framework methods implementation.
+
+    """
+
+    final_ru_dict = {}
+    for ru_target, ru_funcs in ru_functions_dict.items():
+        ru_method, ru_aggr = ru_funcs
+        if ru_target == RUTarget.BOPS:
+            final_ru_dict[ru_target] = \
+            ru_aggr(ru_method(final_bit_widths_config, graph, fw_info, fw_impl, False), False)[0]
+        else:
+            non_conf_ru = ru_method([], graph, fw_info, fw_impl)
+            conf_ru = ru_method(final_bit_widths_config, graph, fw_info, fw_impl)
+            if len(final_bit_widths_config) > 0 and len(non_conf_ru) > 0:
+                final_ru_dict[ru_target] = ru_aggr(np.concatenate([conf_ru, non_conf_ru]), False)[0]
+            elif len(final_bit_widths_config) > 0 and len(non_conf_ru) == 0:
+                final_ru_dict[ru_target] = ru_aggr(conf_ru, False)[0]
+            elif len(final_bit_widths_config) == 0 and len(non_conf_ru) > 0:
+                # final_bit_widths_config == 0 ==> no configurable nodes,
+                # thus, ru can be computed from non_conf_ru alone
+                final_ru_dict[ru_target] = ru_aggr(non_conf_ru, False)[0]
+            else:
+                # No relevant nodes have been quantized with affect on the given target - since we only consider
+                # in the model's final size the quantized layers size, this means that the final size for this target
+                # is zero.
+                Logger.warning(f"No relevant quantized layers for the ru target {ru_target} were found, the recorded"
+                               f"final ru for this target would be 0.")
+                final_ru_dict[ru_target] = 0
+
+    final_ru = ResourceUtilization()
+    final_ru.set_resource_utilization_by_target(final_ru_dict)
+    graph.user_info.final_resource_utilization = final_ru
+    graph.user_info.mixed_precision_cfg = final_bit_widths_config
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,8 +10,10 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.symmetric_ste
-import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.pytorch.quantizer.lsq.symmetric_lsq
+import model_compression_toolkit.qat.pytorch.quantizer.lsq.uniform_lsq
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py`

 * *Files 19% similar despite different names*

```diff
@@ -40,10 +40,9 @@
             super().__init__(quantization_config)
 
 else:
     class BasePytorchQATTrainableQuantizer(BasePytorchTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             super().__init__(quantization_config)
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchQATTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("Installation of PyTorch is required to use BasePytorchQATTrainableQuantizer. "
+                            "The 'torch' package was not found.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,94 +8,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import List, Dict, Tuple, Callable
-
-from mct_quantizers import PytorchActivationQuantizationHolder, QuantizationTarget
+from typing import Tuple, Dict, List, Callable
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.qat.common.qat_config import QATConfig
+from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
+from mct_quantizers import QuantizationTarget, KerasActivationQuantizationHolder
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
-    get_trainable_quantizer_quantization_candidates, get_trainable_quantizer_weights_config, \
-    get_trainable_quantizer_activation_config
-from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
+    get_trainable_quantizer_weights_config, get_trainable_quantizer_activation_config, \
+    get_trainable_quantizer_quantization_candidates
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
+
 def get_activation_quantizer_holder(n: common.BaseNode,
                                     qat_config: QATConfig) -> Callable:
     """
-    Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
+    Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
     If the layer is not supposed to be wrapped with activation quantizers - return None.
 
     Args:
-        n: Node for which to retrieve anActivationQuantizationHolder to attach to its output.
-        qat_config: QAT configuration (for example, training methods).
+        n: Node to get KerasActivationQuantizationHolder to attach in its output.
+        qat_config: Configuration of QAT (such as training methods for example).
 
     Returns:
-        A ActivationQuantizationHolder layer for the node's activation quantization.
+        A KerasActivationQuantizationHolder layer for the node activation quantization.
     """
     _, activation_quantizers = quantization_builder(n,
-                                                    qat_config,
-                                                    DEFAULT_PYTORCH_INFO)
+                                                    qat_config)
 
     # Holder by definition uses a single quantizer for the activation quantization
     # thus we make sure this is the only possible case (unless it's a node with no activation
     # quantization, which in this case has an empty list).
     if len(activation_quantizers) == 1:
-        return PytorchActivationQuantizationHolder(activation_quantizers[0])
-    Logger.error(f'ActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}')
+        return KerasActivationQuantizationHolder(activation_quantizers[0])
+    Logger.critical(f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}.')
 
 
 def quantization_builder(n: common.BaseNode,
                          qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
-                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
-                                    List[BasePytorchQATTrainableQuantizer]]:
+                         kernel_attr: str = None,
+                         ) -> Tuple[Dict[str, BaseKerasQATTrainableQuantizer], List[BaseKerasQATTrainableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration.
 
     Args:
         n: Node to build its QuantizeConfig.
         qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
+
 
     Returns:
         weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.).
+        activation_quantizers: A list of activations quantization, one for each layer output.
     """
     if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
+        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n, kernel_attr)
     else:
         wq_cand, aq_cand = None, None
 
     weight_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during QAT
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
+
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Weights,
                                                         qat_config.weight_training_method,
                                                         quant_method,
-                                                        BasePytorchQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                           **qat_config.weight_quantizer_params_override)})
+                                                        BaseKerasQATTrainableQuantizer)
+
+        weight_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                      attr_name=kernel_attr,
+                                                                                                      weights_quantization_candidates=wq_cand),
+                                                        **qat_config.weight_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
+        # single output -> normalize to list of output_shapes
+        output_shapes = n.output_shape if isinstance(n.output_shape[0], (list, tuple)) else [n.output_shape]
+
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Activation,
                                                         qat_config.activation_training_method,
                                                         quant_method,
-                                                        BasePytorchQATTrainableQuantizer)
+                                                        BaseKerasQATTrainableQuantizer)
 
         activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)]
+                                                 **qat_config.activation_quantizer_params_override)] * len(output_shapes)
 
     return weight_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,17 +36,30 @@
         max_val: maximum value for clipping
     Returns:
         clipped variable
     """
     return (torch.clip(x, min=min_val, max=max_val) - x).detach() + x
 
 
-def fix_range_to_include_zero(range_min: torch.Tensor,
-                              range_max: torch.Tensor,
-                              n_bits: int) -> Tuple[torch.Tensor, torch.Tensor]:
+def grad_scale(x: torch.Tensor, scale=1.0) -> torch.Tensor:
+    """
+    Gradient scale
+    Args:
+        x: input variable
+        scale: scale factor
+    Returns:
+        x in forward and x*scale in backward (for scaling the gradients).
+    """
+    x_scaled = x * scale
+    return (x - x_scaled).detach() + x_scaled
+
+
+def adjust_range_to_include_zero(range_min: torch.Tensor,
+                                 range_max: torch.Tensor,
+                                 n_bits: int) -> Tuple[torch.Tensor, torch.Tensor]:
     """
     Adjusting the quantization range to include representation of 0.0 in the quantization grid.
     If quantization per-channel, then range_min and range_max should be tensors in the specific shape that allows
     quantization along the channel_axis.
     Args:
         range_min: min bound of the quantization range (before adjustment).
         range_max: max bound of the quantization range (before adjustment).
@@ -116,15 +129,15 @@
         range_min: minimum bound of the range for quantization (or array of min values per channel).
         range_max: maximum bound of the range for quantization (or array of max values per channel).
         n_bits: Number of bits to quantize the tensor.
     Returns:
         Quantized data.
     """
     # adjusts the quantization range so the quantization grid includes zero.
-    a, b = fix_range_to_include_zero(range_min, range_max, n_bits)
+    a, b = adjust_range_to_include_zero(range_min, range_max, n_bits)
 
     # Compute the step size of quantized values.
     delta_tensor = (b - a) / (2 ** n_bits - 1)
 
     # Apply rounding
     input_tensor_int = ste_round((tensor_data - a) / delta_tensor)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 3% similar despite different names*

```diff
@@ -34,15 +34,15 @@
 from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEWeightQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer weights.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
@@ -115,28 +115,28 @@
         Returns:
             A pytorch inferable quanizer object.
         """
         np_threshold = self.get_quantizer_variable(THRESHOLD_TENSOR).cpu().detach().numpy().flatten()
         if self.power_of_two:
             pot_threshold = 2 ** np.ceil(np.log2(np_threshold))
             return WeightsPOTInferableQuantizer(num_bits=self.num_bits,
-                                                threshold=pot_threshold,
+                                                threshold=pot_threshold.tolist(),
                                                 per_channel=self.quantization_config.weights_per_channel_threshold,
                                                 channel_axis=self.quantization_config.weights_channels_axis)
         else:
             return WeightsSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                      threshold=np_threshold,
+                                                      threshold=np_threshold.tolist(),
                                                       per_channel=self.quantization_config.weights_per_channel_threshold,
                                                       channel_axis=self.quantization_config.weights_channels_axis)
 
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
-                quantizer_type=TrainingMethod.STE)
+                identifier=TrainingMethod.STE)
 class STEActivationQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer activations.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
@@ -194,13 +194,13 @@
         Returns:
             A pytorch inferable quanizer object.
         """
         np_threshold = self.get_quantizer_variable(THRESHOLD_TENSOR).cpu().detach().numpy()
         if self.power_of_two:
             pot_threshold = np.power(2.0, np.ceil(np.log2(np_threshold)))
             return ActivationPOTInferableQuantizer(num_bits=self.num_bits,
-                                                   threshold=pot_threshold,
+                                                   threshold=pot_threshold.tolist(),
                                                    signed=self.sign)
         else:
             return ActivationSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                         threshold=np_threshold,
+                                                         threshold=np_threshold.tolist(),
                                                          signed=self.sign)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/constants.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/constants.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,19 +9,12 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-# TP Model constants
-OPS_SET_LIST = 'ops_set_list'
-
-# Version
-LATEST = 'latest'
-
-
-# Supported TP models names:
-DEFAULT_TP_MODEL = 'default'
-IMX500_TP_MODEL = 'imx500'
-TFLITE_TP_MODEL = 'tflite'
-QNNPACK_TP_MODEL = 'qnnpack'
+# Quantizers constants (for GPTQ, QAT, etc.)
+FQ_MIN = "min"
+FQ_MAX = "max"
+THRESHOLD_TENSOR = "ptq_threshold_tensor"
+WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/immutable.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/immutable.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,14 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any
 
+from model_compression_toolkit.logger import Logger
+
 
 class ImmutableClass(object):
     """
     Class to make inherits classes immutable.
     """
     _initialized = False
 
@@ -32,22 +34,22 @@
 
         Args:
             *args: Arguments to set to attribute.
             **kwargs: Keyword-arguments to set to attribute.
 
         """
         if self._initialized:
-            raise Exception('Immutable class. Can\'t edit attributes')
+            Logger.critical("Immutable class. Can't edit attributes.")
         else:
             object.__setattr__(self,
                                *args,
                                **kwargs)
 
     def initialized_done(self):
         """
 
         Method to use when object should be immutable.
 
         """
         if self._initialized:
-            raise Exception('reinitialized')  # Can not get finalized again.
+            Logger.critical('Object reinitialization error: object cannot be finalized again.')  # Can not get finalized again.
         self._initialized = True  # Finalize object.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,20 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.target_platform_capabilities.target_platform.fusing import Fusing
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
-    TargetPlatformCapabilities, OperationsSetToLayers, Smaller, SmallerEq, NotEq, Eq, GreaterEq, Greater, LayerFilterParams, OperationsToLayers, get_current_tpc
-
-from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import \
-    get_default_quantization_config_options, TargetPlatformModel
-
-from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, \
-    QuantizationConfigOptions
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import AttributeFilter
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities, OperationsSetToLayers, Smaller, SmallerEq, NotEq, Eq, GreaterEq, Greater, LayerFilterParams, OperationsToLayers, get_current_tpc
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import get_default_quantization_config_options, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, QuantizationConfigOptions, AttributeQuantizationConfig
 from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorsSet, OperatorSetConcat
 
 from mct_quantizers import QuantizationMethod
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,15 @@
     def get(self):
         """
 
         Returns: The current TargetPlatformModel that is being defined.
 
         """
         if self.tp_model is None:
-            Logger.error('Target platform model is not initialized.')  # pragma: no cover
+            Logger.critical('Target platform model is not initialized.')  # pragma: no cover
         return self.tp_model
 
     def reset(self):
         """
 
         Reset the current TargetPlatformModel so a new TargetPlatformModel can be wrapped and
         used as the current TargetPlatformModel object.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,41 +10,76 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from typing import Any
+from typing import Any, List, Union
 
-from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat, \
+    OperatorsSet
 from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model_component import TargetPlatformModelComponent
 
 
 class Fusing(TargetPlatformModelComponent):
-
-    def __init__(self, operator_groups_list, name=None):
+    """
+     Fusing defines a list of operators that should be combined and treated as a single operator,
+     hence no quantization is applied between them.
+    """
+
+    def __init__(self,
+                 operator_groups_list: List[Union[OperatorsSet, OperatorSetConcat]],
+                 name: str = None):
+        """
+        Args:
+            operator_groups_list (List[Union[OperatorsSet, OperatorSetConcat]]): A list of operator groups, each being either an OperatorSetConcat or an OperatorsSet.
+            name (str): The name for the Fusing instance. If not provided, it's generated from the operator groups' names.
+        """
         assert isinstance(operator_groups_list,
                           list), f'List of operator groups should be of type list but is {type(operator_groups_list)}'
         assert len(operator_groups_list) >= 2, f'Fusing can not be created for a single operators group'
+
+        # Generate a name from the operator groups if no name is provided
         if name is None:
             name = '_'.join([x.name for x in operator_groups_list])
+
         super().__init__(name)
         self.operator_groups_list = operator_groups_list
 
-    def contains(self, other: Any):
+    def contains(self, other: Any) -> bool:
+        """
+        Determines if the current Fusing instance contains another Fusing instance.
+
+        Args:
+            other: The other Fusing instance to check against.
+
+        Returns:
+            A boolean indicating whether the other instance is contained within this one.
+        """
         if not isinstance(other, Fusing):
             return False
+
+        # Check for containment by comparing operator groups
         for i in range(len(self.operator_groups_list) - len(other.operator_groups_list) + 1):
             for j in range(len(other.operator_groups_list)):
-                if self.operator_groups_list[i + j] != other.operator_groups_list[j] and not (isinstance(self.operator_groups_list[i + j], OperatorSetConcat) and (other.operator_groups_list[j] in self.operator_groups_list[i + j].op_set_list)):
+                if self.operator_groups_list[i + j] != other.operator_groups_list[j] and not (
+                        isinstance(self.operator_groups_list[i + j], OperatorSetConcat) and (
+                        other.operator_groups_list[j] in self.operator_groups_list[i + j].op_set_list)):
                     break
             else:
+                # If all checks pass, the other Fusing instance is contained
                 return True
+        # Other Fusing instance is not contained
         return False
 
-
     def get_info(self):
+        """
+        Retrieves information about the Fusing instance, including its name and the sequence of operator groups.
+
+        Returns:
+            A dictionary with the Fusing instance's name as the key and the sequence of operator groups as the value,
+            or just the sequence of operator groups if no name is set.
+        """
         if self.name is not None:
             return {self.name: ' -> '.join([x.name for x in self.operator_groups_list])}
-        return ' -> '.join([x.name for x in self.operator_groups_list])
-
+        return ' -> '.join([x.name for x in self.operator_groups_list])
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/reader/node_holders.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,31 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from enum import Enum
 
 
-class QuantizationFormat(Enum):
-    FAKELY_QUANT = 0
-    INT8 = 1
+import torch
+
+from model_compression_toolkit.core.pytorch.constants import PLACEHOLDER
+
+
+class DummyPlaceHolder(torch.nn.Module):
+    """
+    Class for PlaceHolder operator since a Pytorch model doesn't have one but FX does.
+    """
+
+    def __name__(self):
+        return PLACEHOLDER
+
+    def forward(self, x):
+        return x
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py`

 * *Files 19% similar despite different names*

```diff
@@ -72,14 +72,15 @@
         self.name = name
         self.operator_set = []
         assert isinstance(default_qco, QuantizationConfigOptions)
         assert len(default_qco.quantization_config_list) == 1, \
             f'Default QuantizationConfigOptions must contain only one option'
         self.default_qco = default_qco
         self.fusing_patterns = []
+        self.is_simd_padding = False
 
     def get_config_options_by_operators_set(self,
                                             operators_set_name: str) -> QuantizationConfigOptions:
         """
         Get the QuantizationConfigOptions of a OperatorsSet by the OperatorsSet name.
         If the name is not in the model, the default QuantizationConfigOptions is returned.
 
@@ -151,15 +152,15 @@
 
         """
         if isinstance(tp_model_component, Fusing):
             self.fusing_patterns.append(tp_model_component)
         elif isinstance(tp_model_component, OperatorsSetBase):
             self.operator_set.append(tp_model_component)
         else:
-            raise Exception(f'Trying to append an unfamiliar TargetPlatformModelComponent of type: {type(tp_model_component)}')
+            Logger.critical(f'Attempted to append an unrecognized TargetPlatformModelComponent of type: {type(tp_model_component)}.')
 
     def __enter__(self):
         """
         Start defining the TargetPlatformModel using 'with'.
 
         Returns: Initialized TargetPlatformModel object.
 
@@ -187,15 +188,15 @@
         Assert model is valid.
         Model is invalid if, for example, it contains multiple operator sets with the same name,
         as their names should be unique.
 
         """
         opsets_names = [op.name for op in self.operator_set]
         if (len(set(opsets_names)) != len(opsets_names)):
-            Logger.error(f'OperatorsSet must have unique names')
+            Logger.critical(f'Operator Sets must have unique names.')
 
     def get_default_config(self) -> OpQuantizationConfig:
         """
 
         Returns:
 
         """
@@ -220,15 +221,19 @@
         """
 
         Display the TargetPlatformModel.
 
         """
         pprint.pprint(self.get_info(), sort_dicts=False)
 
-    def set_quantization_format(self,
-                                quantization_format: Any):
+    def set_simd_padding(self,
+                         is_simd_padding: bool):
         """
-        Set quantization format.
+        Set flag is_simd_padding to indicate whether this TP model defines
+        that padding due to SIMD constrains occurs.
+
         Args:
-            quantization_format: A quantization format (fake-quant, int8 etc.) from enum QuantizationFormat.
+            is_simd_padding: Whether this TP model defines that padding due to SIMD constrains occurs.
+
         """
-        self.quantization_format = quantization_format
+        self.is_simd_padding = is_simd_padding
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py`

 * *Files 2% similar despite different names*

```diff
@@ -83,29 +83,29 @@
             other: Filter to add to self with logic OR.
 
         Returns:
             OrAttributeFilter that filters with OR between the current AttributeFilter and the passed AttributeFilter.
         """
 
         if not isinstance(other, AttributeFilter):
-            Logger.error("Not an attribute filter. Can not run an OR operation.")  # pragma: no cover
+            Logger.critical("Not an attribute filter. Cannot perform an 'OR' operation.")  # pragma: no cover
         return OrAttributeFilter(self, other)
 
     def __and__(self, other: Any):
         """
         Create a filter that combines multiple AttributeFilters with a logic AND between them.
 
         Args:
             other: Filter to add to self with logic AND.
 
         Returns:
             AndAttributeFilter that filters with AND between the current AttributeFilter and the passed AttributeFilter.
         """
         if not isinstance(other, AttributeFilter):
-            Logger.error("Not an attribute filter. Can not run an AND operation.")  # pragma: no cover
+            Logger.critical("Not an attribute filter. Can not perform an 'AND' operation.")  # pragma: no cover
         return AndAttributeFilter(self, other)
 
     def match(self,
               layer_config: Dict[str, Any]) -> bool:
         """
         Check whether the passed configuration matches the filter.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from model_compression_toolkit.logger import Logger
 
 
 def get_current_tpc():
     """
 
     Returns: The current TargetPlatformCapabilities that is being used and accessed.
 
@@ -34,15 +35,15 @@
     def get(self):
         """
 
         Returns: The current TargetPlatformCapabilities that is being defined.
 
         """
         if self.tpc is None:
-            raise Exception('TargetPlatformCapabilities is not initialized.')
+            Logger.critical("'TargetPlatformCapabilities' (TPC) instance is not initialized.")
         return self.tpc
 
     def reset(self):
         """
 
         Reset the current TargetPlatformCapabilities so a new TargetPlatformCapabilities can be wrapped and
         used as the current TargetPlatformCapabilities object.
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,38 +9,42 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import List, Any
+from typing import List, Any, Dict
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.current_tpc import  _current_tpc
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
 from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat, \
     OperatorsSetBase
-
+from model_compression_toolkit import DefaultDict
 
 
 class OperationsSetToLayers(TargetPlatformCapabilitiesComponent):
     """
     Associate an OperatorsSet to a list of framework's layers.
     """
     def __init__(self,
                  op_set_name: str,
-                 layers: List[Any]):
+                 layers: List[Any],
+                 attr_mapping: Dict[str, DefaultDict] = None):
         """
 
         Args:
             op_set_name (str): Name of OperatorsSet to associate with layers.
             layers (List[Any]): List of layers/FilterLayerParams to associate with OperatorsSet.
+            attr_mapping (Dict[str, DefaultDict]): A mapping between a general attribute name to a DefaultDict that maps a layer type to the layer's framework name of this attribute.
+
         """
         self.layers = layers
+        self.attr_mapping = attr_mapping
         super(OperationsSetToLayers, self).__init__(name=op_set_name)
         _current_tpc.get().remove_opset_from_not_used_list(op_set_name)
 
     def __repr__(self) -> str:
         """
 
         Returns: String to represent the mapping from an OperatorsSet's label to the list of layers.
@@ -87,14 +91,26 @@
             layers = []
             for o in op.op_set_list:
                 layers.extend(self.get_layers_by_op(o))
             return layers
         Logger.warning(f'{op.name} is not in model.')
         return []
 
+    def get_layers(self) -> Any:
+        """
+        Get list of layers of all OperatorsSet objects.
+
+        Returns:
+            List of Layers that are associated with the passed OperatorsSet object.
+        """
+        layers = []
+        for o in self.op_sets_to_layers:
+            layers.extend(o.layers)
+        return layers
+
     def __add__(self,
                 op_set_to_layers: OperationsSetToLayers):
         """
         Add a OperationsSetToLayers to self's OperationsSetToLayers existing OperationsSetToLayers objects.
         Args:
             op_set_to_layers: OperationsSetToLayers to add.
 
@@ -127,11 +143,11 @@
             assert not (ops2layers.name in existing_opset_names), f'OperationsSetToLayers names should be unique, but {ops2layers.name} appears to violate it.'
             existing_opset_names.append(ops2layers.name)
 
             # Assert that a layer does not appear in more than a single OperatorsSet in the TargetPlatformModel.
             for layer in ops2layers.layers:
                 qco_by_opset_name = _current_tpc.get().tp_model.get_config_options_by_operators_set(ops2layers.name)
                 if layer in existing_layers:
-                    Logger.error(f'Found layer {layer.__name__} in more than one '
+                    Logger.critical(f'Found layer {layer.__name__} in more than one '
                                  f'OperatorsSet')  # pragma: no cover
                 else:
                     existing_layers.update({layer: qco_by_opset_name})
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py`

 * *Files 14% similar despite different names*

```diff
@@ -71,14 +71,23 @@
         """
         opset = self.tp_model.get_opset_by_name(opset_name)
         if opset is None:
             Logger.warning(f'{opset_name} was not found in TargetPlatformCapabilities.')
             return None
         return self.get_layers_by_opset(opset)
 
+    def get_layers(self) -> List[Any]:
+        """
+        Get a list of layers of all OperatorsSet objects.
+
+        Returns:
+            List of layers/LayerFilterParams in the TPC.
+        """
+        return self.op_sets_to_layers.get_layers()
+
     def get_layers_by_opset(self, op: OperatorsSetBase) -> List[Any]:
         """
         Get a list of layers that are attached to an OperatorsSet by the OperatorsSet object.
 
         Args:
             op: OperatorsSet object to get its layers.
 
@@ -126,16 +135,15 @@
         Args:
             tpc_component: Component to append to TargetPlatformCapabilities.
 
         """
         if isinstance(tpc_component, OperationsSetToLayers):
             self.op_sets_to_layers += tpc_component
         else:
-            Logger.error(f'Trying to append an unfamiliar TargetPlatformCapabilitiesComponent of type: '
-                         f'{type(tpc_component)}')  # pragma: no cover
+            Logger.critical(f"Attempt to append an unrecognized 'TargetPlatformCapabilitiesComponent' of type: '{type(tpc_component)}'. Ensure the component is compatible.")  # pragma: no cover
 
     def __enter__(self):
         """
         Init a TargetPlatformCapabilities object.
         """
         _current_tpc.set(self)
         return self
@@ -175,14 +183,24 @@
         layer2qco = {}
         filterlayer2qco = {}
         for op2layers in self.op_sets_to_layers.op_sets_to_layers:
             for l in op2layers.layers:
                 qco = self.tp_model.get_config_options_by_operators_set(op2layers.name)
                 if qco is None:
                     qco = self.tp_model.default_qco
+
+                # here, we need to take care of mapping a general attribute name into a framework and
+                # layer type specific attribute name.
+                # attr_mapping is a mapping between an attribute generic name to a dictionary that maps each
+                # layer type to its framework-specific attribute name.
+                # in the loop below, v is the inner dictionary.
+                layer_attrs_mapping = None if op2layers.attr_mapping is None else \
+                    {k: v.get(l) for k, v in op2layers.attr_mapping.items()}
+                qco = qco.clone_and_map_weights_attr_keys(layer_attrs_mapping)
+
                 if isinstance(l, LayerFilterParams):
                     filterlayer2qco.update({l: qco})
                 else:
                     layer2qco.update({l: qco})
         return layer2qco, filterlayer2qco
 
     def remove_fusing_names_from_not_used_list(self):
@@ -210,7 +228,16 @@
         """
 
         Log warnings regards unused opsets.
 
         """
         for op in self.__tp_model_opsets_not_used:
             Logger.warning(f'{op} is defined in TargetPlatformModel, but is not used in TargetPlatformCapabilities.')
+
+    @property
+    def is_simd_padding(self) -> bool:
+        """
+
+        Returns: Check if the TP model defines that padding due to SIMD constrains occurs.
+
+        """
+        return self.tp_model.is_simd_padding
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,17 +9,14 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH
-
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
-
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
 if FOUND_TF:
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_keras import get_keras_tpc as get_keras_tpc_latest
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_keras import generate_keras_tpc
-
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import generate_keras_tpc
 if FOUND_TORCH:
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_pytorch import generate_pytorch_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import generate_pytorch_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,18 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,49 +32,90 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='default_tp_model')
+                             name='qnnpack_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
-    # Create a quantization config.
-    # A quantization configuration defines how an operator
-    # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
-        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        activation_n_bits=8,
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
         weights_n_bits=8,
-        weights_per_channel_threshold=True,
+        weights_per_channel_threshold=False,
         enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # Create a quantization config. A quantization configuration defines how an operator
+    # should be quantized on the modeled hardware.
+    # For qnnpack backend, Pytorch uses a QConfig with torch.per_tensor_affine for
+    # activations quantization and a torch.per_tensor_symmetric quantization scheme
+    # for weights quantization (https://pytorch.org/docs/stable/quantization.html#natively-supported-backends):
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
 
-    mixed_precision_cfg_list = [] # No mixed precision
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=tp.QuantizationMethod.UNIFORM,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=None
+    )
 
-    return eight_bits, mixed_precision_cfg_list
+    mixed_precision_cfg_list = []  # No mixed precision
+
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -97,37 +140,27 @@
 
     # Create a TargetPlatformModel and set its default quantization config.
     # This default configuration will be used for all operations
     # unless specified otherwise (see OperatorsSet, for example):
     generated_tpc = tp.TargetPlatformModel(default_configuration_options, name=name)
 
     # To start defining the model's components (such as operator sets, and fusing patterns),
-    # use 'with' the TargetPlatformModel instance, and create them as below:
+    # use 'with' the target platform model instance, and create them as below:
     with generated_tpc:
-        # Create an OperatorsSet to represent a set of operations.
-        # Each OperatorsSet has a unique label.
-        # If a quantization configuration options is passed, these options will
-        # be used for operations that will be attached to this set's label.
-        # Otherwise, it will be a configure-less set (used in fusing):
-
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
-        # May suit for operations like: Dropout, Reshape, etc.
-        tp.OperatorsSet("NoQuantization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                             enable_weights_quantization=False,
-                             enable_activation_quantization=False))
-
-        # Define operator sets that use mixed_precision_configuration_options:
+        # Combine operations/modules into a single module.
+        # Pytorch supports the next fusing patterns:
+        # [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]
+        # Source: # https://pytorch.org/docs/stable/quantization.html#model-preparation-for-quantization-eager-mode
         conv = tp.OperatorsSet("Conv")
-
-        # Define operations sets without quantization configuration
-        # options (useful for creating fusing patterns, for example):
-        any_relu = tp.OperatorsSet("AnyReLU")
+        batchnorm = tp.OperatorsSet("BatchNorm")
+        relu = tp.OperatorsSet("Relu")
+        linear = tp.OperatorsSet("Linear")
 
         # ------------------- #
         # Fusions
         # ------------------- #
-        tp.Fusing([conv, any_relu])
+        tp.Fusing([conv, batchnorm, relu])
+        tp.Fusing([conv, batchnorm])
+        tp.Fusing([conv, relu])
+        tp.Fusing([linear, relu])
 
     return generated_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,38 +9,40 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
+
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Reshape, ZeroPadding2D, \
-        Dropout, \
-        MaxPooling2D, Activation, ReLU, Flatten, Cropping2D
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, \
+    KERAS_DEPTHWISE_KERNEL, BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
+
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, Activation
 else:
-    from keras.layers import Conv2D, DepthwiseConv2D, Reshape, ZeroPadding2D, \
-        Dropout, MaxPooling2D, Activation, ReLU, Flatten, Cropping2D
+    from keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, Activation
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
+    qnnpack_tp_model = get_tp_model()
+    return generate_keras_tpc(name='qnnpack_keras', tp_model=qnnpack_tp_model)
 
 
 def generate_keras_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
 
     Args:
@@ -51,31 +53,38 @@
     """
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model,
                                               name=name,
                                               version=TPC_VERSION)
 
     with keras_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Reshape,
-                                                    tf.reshape,
-                                                    Flatten,
-                                                    Cropping2D,
-                                                    ZeroPadding2D,
-                                                    Dropout,
-                                                    MaxPooling2D,
-                                                    tf.split,
-                                                    tf.quantization.fake_quant_with_min_max_vars,
-                                                    tf.math.argmax,
-                                                    tf.shape,
-                                                    tf.__operators__.getitem,
-                                                    tf.compat.v1.shape])
-
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d])
-        tp.OperationsSetToLayers("AnyReLU", [tf.nn.relu,
-                                             tf.nn.relu6,
-                                             tp.LayerFilterParams(ReLU, negative_slope=0.0),
-                                             tp.LayerFilterParams(Activation, activation="relu")])
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+
+        tp.OperationsSetToLayers("Linear", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
+
+        tp.OperationsSetToLayers("BatchNorm", [BatchNormalization,
+                                               tf.nn.batch_normalization])
+
+        tp.OperationsSetToLayers("Relu", [tf.nn.relu,
+                                          tf.nn.relu6,
+                                          tp.LayerFilterParams(ReLU, negative_slope=0.0),
+                                          tp.LayerFilterParams(Activation, activation="relu")])
 
     return keras_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v2'
+__version__ = 'v1'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py`

 * *Files 20% similar despite different names*

```diff
@@ -11,18 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import BIAS_ATTR, KERNEL_ATTR
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    QuantizationMethod, AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,49 +32,88 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='default_tp_model')
+                             name='tflite_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
 
-    mixed_precision_cfg_list = [] # No mixed precision
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=QuantizationMethod.UNIFORM,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=None
+    )
+
+    mixed_precision_cfg_list = []  # No mixed precision
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -99,43 +140,59 @@
     # This default configuration will be used for all operations
     # unless specified otherwise (see OperatorsSet, for example):
     generated_tpc = tp.TargetPlatformModel(default_configuration_options, name=name)
 
     # To start defining the model's components (such as operator sets, and fusing patterns),
     # use 'with' the TargetPlatformModel instance, and create them as below:
     with generated_tpc:
-        # Create an OperatorsSet to represent a set of operations.
-        # Each OperatorsSet has a unique label.
-        # If a quantization configuration options is passed, these options will
-        # be used for operations that will be attached to this set's label.
-        # Otherwise, it will be a configure-less set (used in fusing):
-
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
-        # May suit for operations like: Dropout, Reshape, etc.
+        # In TFLite, the quantized operator specifications constraint operators quantization
+        # differently. For more details:
+        # https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications
         tp.OperatorsSet("NoQuantization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                             enable_weights_quantization=False,
-                             enable_activation_quantization=False))
-
-        conv = tp.OperatorsSet("Conv")
-        fc = tp.OperatorsSet("FullyConnected")
-        any_relu = tp.OperatorsSet("AnyReLU")
-        prelu = tp.OperatorsSet("PReLU")
-        swish = tp.OperatorsSet("Swish")
-        sigmoid = tp.OperatorsSet("Sigmoid")
-        tanh = tp.OperatorsSet("Tanh")
-
-        # Combine multiple operators into a single operator to avoid quantization between
-        # them. To do this we define fusing patterns using the OperatorsSets that were created.
-        # To group multiple sets with regard to fusing, an OperatorSetConcat can be created
-        activations_after_conv_to_fuse = tp.OperatorSetConcat(any_relu, swish, prelu, sigmoid, tanh)
-        activations_after_fc_to_fuse = tp.OperatorSetConcat(any_relu, tanh, sigmoid)
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            quantization_preserving=True))
 
+        fc_qco = tp.get_default_quantization_config_options()
+        fc = tp.OperatorsSet("FullyConnected",
+                             fc_qco.clone_and_edit_weight_attribute(weights_per_channel_threshold=False))
+
+        tp.OperatorsSet("L2Normalization",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
+        tp.OperatorsSet("LogSoftmax",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=127, fixed_scale=16 / 256))
+        tp.OperatorsSet("Tanh",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
+        tp.OperatorsSet("Softmax",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
+        tp.OperatorsSet("Logistic",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
+
+        conv2d = tp.OperatorsSet("Conv2d")
+        kernel = tp.OperatorSetConcat(conv2d, fc)
+
+        relu = tp.OperatorsSet("Relu")
+        elu = tp.OperatorsSet("Elu")
+        activations_to_fuse = tp.OperatorSetConcat(relu, elu)
+
+        batch_norm = tp.OperatorsSet("BatchNorm")
+        bias_add = tp.OperatorsSet("BiasAdd")
+        add = tp.OperatorsSet("Add")
+        squeeze = tp.OperatorsSet("Squeeze",
+                                  qc_options=tp.get_default_quantization_config_options().clone_and_edit(
+                                      quantization_preserving=True))
         # ------------------- #
         # Fusions
         # ------------------- #
-        tp.Fusing([conv, activations_after_conv_to_fuse])
-        tp.Fusing([fc, activations_after_fc_to_fuse])
+        # Source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper
+        tp.Fusing([kernel, bias_add])
+        tp.Fusing([kernel, bias_add, activations_to_fuse])
+        tp.Fusing([conv2d, batch_norm, activations_to_fuse])
+        tp.Fusing([conv2d, squeeze, activations_to_fuse])
+        tp.Fusing([batch_norm, activations_to_fuse])
+        tp.Fusing([batch_norm, add, activations_to_fuse])
 
     return generated_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,87 +1,128 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
-
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
-        Dropout, \
-        MaxPooling2D, Activation, ReLU, PReLU, Flatten, Cropping2D
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.constants import FOUND_SONY_CUSTOM_LAYERS
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_DEPTHWISE_KERNEL, \
+    KERAS_KERNEL, BIAS_ATTR, BIAS
+
+if FOUND_SONY_CUSTOM_LAYERS:
+    from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess
+
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
+        MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
+        Conv2DTranspose
 else:
-    from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
-        Dropout, MaxPooling2D, Activation, ReLU, PReLU, Flatten, Cropping2D
+    from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
+        MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
+        Conv2DTranspose
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
+    imx500_tpc_tp_model = get_tp_model()
+    return generate_keras_tpc(name='imx500_tpc_keras_tpc', tp_model=imx500_tpc_tp_model)
 
 
 def generate_keras_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
 
     Args:
         name: Name of the TargetPlatformCapabilities.
         tp_model: TargetPlatformModel object.
 
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
-    keras_tpc = tp.TargetPlatformCapabilities(tp_model,
-                                              name=name,
-                                              version=TPC_VERSION)
+    keras_tpc = tp.TargetPlatformCapabilities(tp_model, name=name, version=TPC_VERSION)
+
+    no_quant_list = [Reshape,
+                     tf.reshape,
+                     Permute,
+                     tf.transpose,
+                     Flatten,
+                     Cropping2D,
+                     ZeroPadding2D,
+                     Dropout,
+                     MaxPooling2D,
+                     tf.split,
+                     tf.quantization.fake_quant_with_min_max_vars,
+                     tf.math.argmax,
+                     tf.shape,
+                     tf.math.equal,
+                     tf.gather,
+                     tf.cast,
+                     tf.unstack,
+                     tf.compat.v1.gather,
+                     tf.nn.top_k,
+                     tf.__operators__.getitem,
+                     tf.image.combined_non_max_suppression,
+                     tf.compat.v1.shape]
+
+    if FOUND_SONY_CUSTOM_LAYERS:
+        no_quant_list.append(SSDPostProcess)
 
     with keras_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Reshape,
-                                                    tf.reshape,
-                                                    Flatten,
-                                                    Cropping2D,
-                                                    ZeroPadding2D,
-                                                    Dropout,
-                                                    MaxPooling2D,
-                                                    tf.split,
-                                                    tf.quantization.fake_quant_with_min_max_vars,
-                                                    tf.math.argmax,
-                                                    tf.shape,
-                                                    tf.__operators__.getitem,
-                                                    tf.compat.v1.shape])
-
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d])
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+        tp.OperationsSetToLayers("NoQuantization", no_quant_list)
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("AnyReLU", [tf.nn.relu,
                                              tf.nn.relu6,
-                                             tp.LayerFilterParams(ReLU, negative_slope=0.0),
-                                             tp.LayerFilterParams(Activation, activation="relu")])
+                                             tf.nn.leaky_relu,
+                                             ReLU,
+                                             LeakyReLU,
+                                             tp.LayerFilterParams(Activation, activation="relu"),
+                                             tp.LayerFilterParams(Activation, activation="leaky_relu")])
+        tp.OperationsSetToLayers("Add", [tf.add, Add])
+        tp.OperationsSetToLayers("Sub", [tf.subtract, Subtract])
+        tp.OperationsSetToLayers("Mul", [tf.math.multiply, Multiply])
+        tp.OperationsSetToLayers("Div", [tf.math.divide])
         tp.OperationsSetToLayers("PReLU", [PReLU])
         tp.OperationsSetToLayers("Swish", [tf.nn.swish, tp.LayerFilterParams(Activation, activation="swish")])
         tp.OperationsSetToLayers("Sigmoid", [tf.nn.sigmoid, tp.LayerFilterParams(Activation, activation="sigmoid")])
         tp.OperationsSetToLayers("Tanh", [tf.nn.tanh, tp.LayerFilterParams(Activation, activation="tanh")])
 
     return keras_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,38 +8,35 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-import operator
-
 import torch
-from torch import flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk
-from torch.nn import Conv2d, Linear, BatchNorm2d
-from torch.nn import Dropout, Flatten, Hardtanh
-from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish
-from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish
+from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d, Hardtanh, ReLU, ReLU6
+from torch.nn.functional import relu, relu6, hardtanh
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='default_pytorch_tpc', tp_model=default_tp_model)
+    qnnpack_pytorch = get_tp_model()
+    return generate_pytorch_tpc(name='qnnpack_pytorch', tp_model=qnnpack_pytorch)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
@@ -47,35 +44,36 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Dropout,
-                                                    Flatten,
-                                                    dropout,
-                                                    flatten,
-                                                    split,
-                                                    operator.getitem,
-                                                    reshape,
-                                                    unsqueeze,
-                                                    BatchNorm2d,
-                                                    chunk,
-                                                    torch.Tensor.size])
-
-        tp.OperationsSetToLayers("Conv", [Conv2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
-        tp.OperationsSetToLayers("AnyReLU", [torch.relu,
-                                             ReLU,
-                                             ReLU6,
-                                             relu,
-                                             relu6,
-                                             tp.LayerFilterParams(Hardtanh, min_val=0),
-                                             tp.LayerFilterParams(hardtanh, min_val=0)])
-        tp.OperationsSetToLayers("PReLU", [PReLU, prelu])
-        tp.OperationsSetToLayers("Swish", [SiLU, silu, Hardswish, hardswish])
-        tp.OperationsSetToLayers("Sigmoid", [Sigmoid, sigmoid])
-        tp.OperationsSetToLayers("Tanh", [Tanh, tanh])
+        tp.OperationsSetToLayers("Conv", [Conv2d,
+                                          torch.nn.functional.conv2d,
+                                          ConvTranspose2d,
+                                          torch.nn.functional.conv_transpose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+
+        tp.OperationsSetToLayers("Linear", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+
+        tp.OperationsSetToLayers("BatchNorm", [BatchNorm2d])
+
+        tp.OperationsSetToLayers("Relu", [torch.relu,
+                                          ReLU,
+                                          ReLU6,
+                                          relu,
+                                          relu6,
+                                          tp.LayerFilterParams(Hardtanh, min_val=0),
+                                          tp.LayerFilterParams(hardtanh, min_val=0)])
 
     return pytorch_tpc
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v3'
+__version__ = 'v1_lut'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, WEIGHTS_N_BITS
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
+
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,56 +33,96 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='default_tp_model')
+                             name='imx500_pot_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
+
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
 
     # To quantize a model using mixed-precision, create
     # a list with more than one OpQuantizationConfig.
     # In this example, we quantize some operations' weights
     # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
     # to quantize the operations' activations using LUT.
-    four_bits = eight_bits.clone_and_edit(weights_n_bits=4)
-    two_bits = eight_bits.clone_and_edit(weights_n_bits=2)
-    mixed_precision_cfg_list = [eight_bits, four_bits, two_bits]
+    four_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 4}},
+                                                 simd_size=linear_eight_bits.simd_size * 2)
+    two_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 2}},
+                                                simd_size=linear_eight_bits.simd_size * 4)
+    mixed_precision_cfg_list = [linear_eight_bits, four_bits, two_bits]
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -112,26 +155,23 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
         # May suit for operations like: Dropout, Reshape, etc.
+        default_qco = tp.get_default_quantization_config_options()
         tp.OperatorsSet("NoQuantization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                             enable_weights_quantization=False,
-                             enable_activation_quantization=False))
+                        default_qco.clone_and_edit(enable_activation_quantization=False)
+                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
         mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
-                                                                              base_config=base_config)
+                                                                             base_config=base_config)
 
         # Define operator sets that use mixed_precision_configuration_options:
         conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
         fc = tp.OperatorsSet("FullyConnected", mixed_precision_configuration_options)
 
         # Define operations sets without quantization configuration
         # options (useful for creating fusing patterns, for example):
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v3_lut'
+__version__ = 'v1'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, WEIGHTS_N_BITS, \
+    WEIGHTS_QUANTIZATION_METHOD
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
-    TargetPlatformModel, QuantizationMethod
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,57 +33,100 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='lut_mp_tp_model')
+                             name='imx500_lut_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization with non-uniform quantizer for 2 and 4 bit candidates.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
 
-    # Creating OpQuantizationConfig list to quantize a model using mixed-precision with non-uniform quantizer
-    # for 2 and 4 bit candidates (with Lookup-Table method).
-    four_bits_lut = eight_bits.clone_and_edit(weights_n_bits=4,
-                                              weights_quantization_method=QuantizationMethod.LUT_POT_QUANTIZER)
-    two_bits_lut = eight_bits.clone_and_edit(weights_n_bits=2,
-                                             weights_quantization_method=QuantizationMethod.LUT_POT_QUANTIZER)
-    mixed_precision_cfg_list = [eight_bits, four_bits_lut, two_bits_lut]
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
 
-    return eight_bits, mixed_precision_cfg_list
+    # To quantize a model using mixed-precision, create
+    # a list with more than one OpQuantizationConfig.
+    # In this example, we quantize some operations' weights
+    # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
+    # to quantize the operations' activations using LUT.
+    four_bits_lut = linear_eight_bits.clone_and_edit(
+        attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 4,
+                                    WEIGHTS_QUANTIZATION_METHOD: tp.QuantizationMethod.LUT_SYM_QUANTIZER}},
+        simd_size=linear_eight_bits.simd_size * 2)
+    two_bits_lut = linear_eight_bits.clone_and_edit(
+        attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 2,
+                                    WEIGHTS_QUANTIZATION_METHOD: tp.QuantizationMethod.LUT_SYM_QUANTIZER}},
+        simd_size=linear_eight_bits.simd_size * 4)
+    mixed_precision_cfg_list = [linear_eight_bits, four_bits_lut, two_bits_lut]
+
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -113,22 +159,19 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
         # May suit for operations like: Dropout, Reshape, etc.
+        default_qco = tp.get_default_quantization_config_options()
         tp.OperatorsSet("NoQuantization",
-                        tp.get_default_quantization_config_options().clone_and_edit(
-                            enable_weights_quantization=False,
-                            enable_activation_quantization=False))
+                        default_qco.clone_and_edit(enable_activation_quantization=False)
+                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
         mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
                                                                              base_config=base_config)
 
         # Define operator sets that use mixed_precision_configuration_options:
         conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v4'
+__version__ = 'v1_pot'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, WEIGHTS_N_BITS
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,56 +32,102 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='default_tp_model')
+                             name='imx500_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # TODO: currently, we don't want to quantize any attribute but the kernel by default,
+    #  to preserve the current behavior of MCT, so quantization is disabled for all other attributes.
+    #  Other quantization parameters are set to what we eventually want to quantize by default
+    #  when we enable multi-attributes quantization - THIS NEED TO BE MODIFIED IN ALL TP MODELS!
+
+    # define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,  # TODO: this will changed to True once implementing multi-attributes quantization
+        lut_values_bitwidth=None)
+
+    # define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
+
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
 
     # To quantize a model using mixed-precision, create
     # a list with more than one OpQuantizationConfig.
     # In this example, we quantize some operations' weights
     # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
     # to quantize the operations' activations using LUT.
-    four_bits = eight_bits.clone_and_edit(weights_n_bits=4)
-    two_bits = eight_bits.clone_and_edit(weights_n_bits=2)
-    mixed_precision_cfg_list = [eight_bits, four_bits, two_bits]
+    four_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 4}},
+                                                 simd_size=linear_eight_bits.simd_size * 2)
+    two_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 2}},
+                                                simd_size=linear_eight_bits.simd_size * 4)
+
+    mixed_precision_cfg_list = [linear_eight_bits, four_bits, two_bits]
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -112,26 +160,25 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+        generated_tpc.set_simd_padding(is_simd_padding=True)
 
         # May suit for operations like: Dropout, Reshape, etc.
+        default_qco = tp.get_default_quantization_config_options()
         tp.OperatorsSet("NoQuantization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                             enable_weights_quantization=False,
-                             enable_activation_quantization=False))
+                        default_qco.clone_and_edit(enable_activation_quantization=False)
+                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
         mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
-                                                                              base_config=base_config)
+                                                                             base_config=base_config)
 
         # Define operator sets that use mixed_precision_configuration_options:
         conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
         fc = tp.OperatorsSet("FullyConnected", mixed_precision_configuration_options)
 
         # Define operations sets without quantization configuration
         # options (useful for creating fusing patterns, for example):
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,37 +11,45 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.constants import FOUND_SONY_CUSTOM_LAYERS
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, \
+    KERAS_DEPTHWISE_KERNEL, BIAS
+
+if FOUND_SONY_CUSTOM_LAYERS:
+    from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess
+
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
+    imx500_tpc_tp_model = get_tp_model()
+    return generate_keras_tpc(name='imx500_tpc_keras_tpc', tp_model=imx500_tpc_tp_model)
 
 
 def generate_keras_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
 
     Args:
@@ -49,43 +57,63 @@
         tp_model: TargetPlatformModel object.
 
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model, name=name, version=TPC_VERSION)
 
+    no_quant_list = [Reshape,
+                     tf.reshape,
+                     Permute,
+                     tf.transpose,
+                     Flatten,
+                     Cropping2D,
+                     ZeroPadding2D,
+                     Dropout,
+                     MaxPooling2D,
+                     tf.split,
+                     tf.quantization.fake_quant_with_min_max_vars,
+                     tf.math.argmax,
+                     tf.shape,
+                     tf.math.equal,
+                     tf.gather,
+                     tf.cast,
+                     tf.unstack,
+                     tf.compat.v1.gather,
+                     tf.nn.top_k,
+                     tf.__operators__.getitem,
+                     tf.image.combined_non_max_suppression,
+                     tf.compat.v1.shape]
+
+    if FOUND_SONY_CUSTOM_LAYERS:
+        no_quant_list.append(SSDPostProcess)
+
     with keras_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Reshape,
-                                                    tf.reshape,
-                                                    Permute,
-                                                    tf.transpose,
-                                                    Flatten,
-                                                    Cropping2D,
-                                                    ZeroPadding2D,
-                                                    Dropout,
-                                                    MaxPooling2D,
-                                                    tf.split,
-                                                    tf.quantization.fake_quant_with_min_max_vars,
-                                                    tf.math.argmax,
-                                                    tf.shape,
-                                                    tf.math.equal,
-                                                    tf.gather,
-                                                    tf.cast,
-                                                    tf.compat.v1.gather,
-                                                    tf.nn.top_k,
-                                                    tf.__operators__.getitem,
-                                                    tf.compat.v1.shape])
-
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          Conv2DTranspose,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d,
-                                          tf.nn.conv2d_transpose])
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+        tp.OperationsSetToLayers("NoQuantization", no_quant_list)
+
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("AnyReLU", [tf.nn.relu,
                                              tf.nn.relu6,
                                              tf.nn.leaky_relu,
                                              ReLU,
                                              LeakyReLU,
                                              tp.LayerFilterParams(Activation, activation="relu"),
                                              tp.LayerFilterParams(Activation, activation="leaky_relu")])
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,35 +12,39 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
-from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, \
-    permute, transpose, equal, gather, topk
+from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot import (
+    __version__ as TPC_VERSION)
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='default_pytorch_tpc', tp_model=default_tp_model)
+    imx500_pot_tpc_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='imx500_pot_tpc_pytorch_tpc', tp_model=imx500_pot_tpc_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
@@ -48,14 +52,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -64,19 +76,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/keras/pruning/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v4_lut'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/core/pytorch/pruning/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-__version__ = 'v5'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,34 +13,37 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, PYTORCH_KERNEL, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='default_pytorch_tpc', tp_model=default_tp_model)
+    imx500_tpc_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='imx500_tpc_pytorch_tpc', tp_model=imx500_tpc_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
@@ -48,14 +51,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -64,19 +75,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,25 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.target_platform_capabilities import \
-    tpc_dict as default_tpc_dict
 from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.target_platform_capabilities import \
     tpc_dict as imx500_tpc_dict
 from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.target_platform_capabilities import \
     tpc_dict as tflite_tpc_dict
 from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.target_platform_capabilities import \
     tpc_dict as qnnpack_tpc_dict
 from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL, IMX500_TP_MODEL, TFLITE_TP_MODEL, QNNPACK_TP_MODEL,  LATEST
 
-tpc_dict = {DEFAULT_TP_MODEL: default_tpc_dict,
+tpc_dict = {DEFAULT_TP_MODEL: imx500_tpc_dict,
             IMX500_TP_MODEL: imx500_tpc_dict,
             TFLITE_TP_MODEL: tflite_tpc_dict,
             QNNPACK_TP_MODEL: qnnpack_tpc_dict}
 
 
 def get_target_platform_capabilities(fw_name: str,
                                      target_platform_name: str,
@@ -47,15 +45,15 @@
         A TargetPlatformCapabilities object that models the hardware and attaches
         a framework information to it.
     """
     assert target_platform_name in tpc_dict, f'Target platform {target_platform_name} is not defined!'
     fw_tpc = tpc_dict.get(target_platform_name)
     assert fw_name in fw_tpc, f'Framework {fw_name} is not supported in {target_platform_name}. Please make sure the relevant ' \
                               f'packages are installed when using MCT for optimizing a {fw_name} model. ' \
-                              f'For Tensorflow, please install tensorflow and tensorflow-model-optimization. ' \
+                              f'For Tensorflow, please install tensorflow. ' \
                               f'For PyTorch, please install torch.'
     tpc_versions = fw_tpc.get(fw_name)
     if target_platform_version is None:
         target_platform_version = LATEST
     else:
         assert target_platform_version in tpc_versions, f'TPC version {target_platform_version} is not supported for framework {fw_name}'
     return tpc_versions.get(target_platform_version)
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,34 +12,35 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH
 from model_compression_toolkit.target_platform_capabilities.constants import LATEST
 
+
 ###############################
 # Build Tensorflow TPC models
 ###############################
 keras_tpc_models_dict = None
 if FOUND_TF:
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.latest import get_keras_tpc_latest
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.latest import get_keras_tpc_latest
 
     # Keras: TPC versioning
     keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
                              LATEST: get_keras_tpc_latest()}
 
 ###############################
 # Build Pytorch TPC models
 ###############################
 pytorch_tpc_models_dict = None
 if FOUND_TORCH:
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.latest import get_pytorch_tpc_latest
-    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.latest import get_pytorch_tpc_latest
 
     # Pytorch: TPC versioning
     pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
                                LATEST: get_pytorch_tpc_latest()}
 
 tpc_dict = {TENSORFLOW: keras_tpc_models_dict,
             PYTORCH: pytorch_tpc_models_dict}
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,9 +8,7 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-__version__ = 'v1'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py`

 * *Files 24% similar despite different names*

```diff
@@ -13,23 +13,26 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
@@ -48,14 +51,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -64,19 +75,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/keras/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-__version__ = 'v1'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/qat/keras/quantizer/lsq/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-__version__ = 'v1'
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,24 +11,28 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
 
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.keras.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, \
-        DepthwiseConv2D, MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, BIAS
+
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, DepthwiseConv2D, \
+        MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 else:
     from keras.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, DepthwiseConv2D, \
         MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 
 from tensorflow.python.keras.layers.core import SlicingOpLambda
 from tensorflow.python.ops.image_ops_impl import ResizeMethod
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import Eq
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import \
+    Eq
 
 from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
 from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
@@ -55,39 +59,48 @@
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model,
                                               name=name,
                                               version=TPC_VERSION)
 
     with keras_tpc:
         tp.OperationsSetToLayers("NoQuantization", [AveragePooling2D,
-                                                                tf.nn.avg_pool2d,
-                                                                Concatenate,
-                                                                tf.concat,
-                                                                MaxPooling2D,
-                                                                Multiply,
-                                                                tf.multiply,
-                                                                Reshape,
-                                                                tf.reshape,
-                                                                tp.LayerFilterParams(tf.image.resize,
-                                                                                     method=ResizeMethod.BILINEAR),
-                                                                tf.nn.space_to_depth,
-                                                                ZeroPadding2D,
-                                                                tf.gather,
-                                                                tf.compat.v1.batch_to_space_nd,
-                                                                tf.space_to_batch_nd,
-                                                                tf.transpose,
-                                                                tf.maximum,
-                                                                Maximum,
-                                                                tf.minimum,
-                                                                Minimum,
-                                                                tf.pad,
-                                                                tf.slice,
-                                                                SlicingOpLambda])
-
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+                                                    tf.nn.avg_pool2d,
+                                                    Concatenate,
+                                                    tf.concat,
+                                                    MaxPooling2D,
+                                                    Multiply,
+                                                    tf.multiply,
+                                                    Reshape,
+                                                    tf.reshape,
+                                                    tp.LayerFilterParams(tf.image.resize,
+                                                                         method=ResizeMethod.BILINEAR),
+                                                    tf.nn.space_to_depth,
+                                                    ZeroPadding2D,
+                                                    tf.unstack,
+                                                    tf.gather,
+                                                    tf.compat.v1.batch_to_space_nd,
+                                                    tf.space_to_batch_nd,
+                                                    tf.transpose,
+                                                    tf.maximum,
+                                                    Maximum,
+                                                    tf.minimum,
+                                                    Minimum,
+                                                    tf.pad,
+                                                    tf.slice,
+                                                    SlicingOpLambda])
+
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("L2Normalization", [tf.math.l2_normalize])
         tp.OperationsSetToLayers("LogSoftmax", [tf.nn.log_softmax])
         tp.OperationsSetToLayers("Tanh", [tf.nn.tanh, tp.LayerFilterParams(Activation, activation="tanh")])
         tp.OperationsSetToLayers("Softmax", [tf.nn.softmax,
                                              Softmax,
                                              tp.LayerFilterParams(Activation, activation="softmax")])
         tp.OperationsSetToLayers("Logistic", [tf.sigmoid, tp.LayerFilterParams(Activation, activation="sigmoid")])
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,8 +11,9 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
 from model_compression_toolkit.trainable_infrastructure.keras.base_keras_quantizer import BaseKerasTrainableQuantizer
-from model_compression_toolkit.trainable_infrastructure.pytorch.base_pytorch_quantizer import BasePytorchTrainableQuantizer
+from model_compression_toolkit.trainable_infrastructure.pytorch.base_pytorch_quantizer import BasePytorchTrainableQuantizer
+from model_compression_toolkit.trainable_infrastructure.keras.quantize_wrapper import KerasTrainableQuantizationWrapper
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/common/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -51,42 +51,42 @@
             quantization_config: quantizer config class contains all the information about the quantizer configuration.
         """
 
         # verify the quantizer class that inherits this class only has a config argument and key-word arguments
         for i, (k, v) in enumerate(self.get_sig().parameters.items()):
             if i == 0:
                 if v.annotation not in [TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]:
-                    Logger.error(f"First parameter must be either TrainableQuantizerWeightsConfig or TrainableQuantizerActivationConfig")  # pragma: no cover
+                    Logger.critical(f"The first parameter must be either 'TrainableQuantizerWeightsConfig' or 'TrainableQuantizerActivationConfig'.")  # pragma: no cover
             elif v.default is v.empty:
-                Logger.error(f"Parameter {k} doesn't have a default value")  # pragma: no cover
+                Logger.critical(f"Parameter '{k}' lacks a default value.")  # pragma: no cover
 
         super(BaseTrainableQuantizer, self).__init__()
         self.quantization_config = quantization_config
 
         # Inherited class should be decorated with @mark_quantizer decorator, and define the following static properties
         static_quantization_method = getattr(self, QUANTIZATION_METHOD, None)
         static_quantization_target = getattr(self, QUANTIZATION_TARGET, None)
 
         if static_quantization_method is None or static_quantization_target is None:
-            Logger.error("A quantizer class that inherit from BaseTrainableQuantizer is not defined appropriately."
-                         "Either it misses the @mark_quantizer decorator or the decorator is not used correctly.")
+            Logger.critical("Quantizer class inheriting from 'BaseTrainableQuantizer' is improperly defined. "
+                            "Ensure it includes the '@mark_quantizer' decorator and is correctly applied.")
 
         if static_quantization_target == QuantizationTarget.Weights:
             self.validate_weights()
             if self.quantization_config.weights_quantization_method not in static_quantization_method:
-                Logger.error(
-                    f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.weights_quantization_method}')
+                Logger.critical(
+                    f"Quantization method mismatch. Expected methods: {static_quantization_method}, received: {self.quantization_config.weights_quantization_method}.")
         elif static_quantization_target == QuantizationTarget.Activation:
             self.validate_activation()
             if self.quantization_config.activation_quantization_method not in static_quantization_method:
-                Logger.error(
-                    f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.activation_quantization_method}')
+                Logger.critical(
+                    f"Quantization method mismatch. Expected methods: {static_quantization_method}, received: {self.quantization_config.activation_quantization_method}.")
         else:
-            Logger.error(
-                f'Unknown Quantization Part:{static_quantization_target}')  # pragma: no cover
+            Logger.critical(
+                f"Unrecognized 'QuantizationTarget': {static_quantization_target}.")  # pragma: no cover
 
         self.quantizer_parameters = {}
 
     @classmethod
     def get_sig(cls):
         return signature(cls)
 
@@ -141,23 +141,23 @@
     def validate_weights(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
 
         """
         if self.activation_quantization() or not self.weights_quantization():
-            Logger.error(f'Expect weight quantization got activation')
+            Logger.critical(f'Expected weight quantization configuration; received activation quantization instead.')
 
     def validate_activation(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
         """
         if not self.activation_quantization() or self.weights_quantization():
-            Logger.error(f'Expect activation quantization got weight')
+            Logger.critical(f'Expected activation quantization configuration; received weight quantization instead.')
 
     def convert2inferable(self) -> BaseInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseInferableQuantizer object.
@@ -179,15 +179,15 @@
 
         Returns:
             trainable variable
         """
         if name in self.quantizer_parameters:
             return self.quantizer_parameters[name][VAR]
         else:
-            Logger.error(f'Variable {name} is not exist in quantizers parameters!') # pragma: no cover
+            Logger.critical(f"Variable '{name}' does not exist in quantizer parameters.") # pragma: no cover
 
 
     @abstractmethod
     def get_trainable_variables(self, group: VariableGroup) -> List[Any]:
         """
         Get trainable parameters with specific group from quantizer
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/constants.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/pruning/pytorch/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,20 +1,14 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
-
-# Quantizers constants (for GPTQ, QAT, etc.)
-FQ_MIN = "min"
-FQ_MAX = "max"
-THRESHOLD_TENSOR = "ptq_threshold_tensor"
-WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,37 +17,42 @@
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig, TrainableQuantizerCandidateConfig
 
 
 def get_trainable_quantizer_weights_config(
         n: BaseNode,
+        attr_name: str,
         weights_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
 ) -> TrainableQuantizerWeightsConfig:
     """
     Returns the relevant configuration for weights trainable quantizer
 
     Args:
         n: BaseNode - the node to build a trainable quantizer from.
+        attr_name: Attribute name to get its weights quantizer configuration.
         weights_quantization_candidates: A list of weights quantizer config candidates.
 
     Returns:
          TrainableQuantizerWeightsConfig: an object that contains the quantizer configuration
     """
     if n.final_weights_quantization_cfg is None:
-        Logger.error(f'Node must have final_weights_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
-
-    final_cfg = n.final_weights_quantization_cfg
-    return TrainableQuantizerWeightsConfig(final_cfg.weights_quantization_method,
-                                           final_cfg.weights_n_bits,
-                                           final_cfg.weights_quantization_params,
-                                           final_cfg.enable_weights_quantization,
-                                           final_cfg.weights_channels_axis,
-                                           final_cfg.weights_per_channel_threshold,
-                                           final_cfg.min_threshold,
+        Logger.critical(
+            "The node requires a 'final_weights_quantization_cfg' configuration to build a "
+            "quantizer. Please ensure this configuration is set for the node.")# pragma: no cover
+
+    final_node_cfg = n.final_weights_quantization_cfg
+    final_attr_cfg = final_node_cfg.get_attr_config(attr_name)
+    return TrainableQuantizerWeightsConfig(final_attr_cfg.weights_quantization_method,
+                                           final_attr_cfg.weights_n_bits,
+                                           final_attr_cfg.weights_quantization_params,
+                                           final_attr_cfg.enable_weights_quantization,
+                                           final_attr_cfg.weights_channels_axis[0],  # Output channel axis
+                                           final_attr_cfg.weights_per_channel_threshold,
+                                           final_node_cfg.min_threshold,
                                            weights_quantization_candidates)
 
 
 def get_trainable_quantizer_activation_config(
         n: BaseNode,
         activation_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
 ) -> TrainableQuantizerActivationConfig:
@@ -58,60 +63,68 @@
         n: BaseNode - the node to build a trainable quantizer from.
         activation_quantization_candidates: A list of activation quantizer candidates config.
 
     Returns:
          TrainableQuantizerActivationConfig - an object that contains the quantizer configuration
     """
     if n.final_activation_quantization_cfg is None:
-        Logger.error(f'Node must have final_activation_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
+        Logger.critical(
+            "The node requires a 'final_activation_quantization_cfg' configuration to build a "
+            "quantizer. Please ensure this configuration is set for the node.")# pragma: no cover
 
     final_cfg = n.final_activation_quantization_cfg
     return TrainableQuantizerActivationConfig(final_cfg.activation_quantization_method,
                                               final_cfg.activation_n_bits,
                                               final_cfg.activation_quantization_params,
                                               final_cfg.enable_activation_quantization,
                                               final_cfg.min_threshold,
                                               activation_quantization_candidates)
 
 
-def get_trainable_quantizer_quantization_candidates(n: BaseNode):
+def get_trainable_quantizer_quantization_candidates(n: BaseNode, attr: str = None):
     """
     Returns quantization configuration candidates for activation and weights trainable quantizer.
     Checks that the candidates are compatible with trainable quantizer
 
     Args:
         n: BaseNode - the node to build a trainable quantizer from
+        attr: Weights attribute to get its quantization configuration candidates and trainable quantizer.
 
     Returns:
          weights_quantization_candidates - A list of configuration candidates for weights
          activation_quantization_candidates - A list of configuration candidates for activation
     """
-    # all candidates must have the same weights quantization method
-    weights_quantization_methods = set([cfg.weights_quantization_cfg.weights_quantization_method for cfg in n.candidates_quantization_cfg])
-    if len(weights_quantization_methods) > 1:
-        Logger.error(f'Unsupported candidates_quantization_cfg with different weights quantization methods: {weights_quantization_methods}') # pragma: no cover
+
+    if attr is not None:
+        # all candidates must have the same weights quantization method
+        weights_quantization_methods = set([cfg.weights_quantization_cfg.get_attr_config(attr).weights_quantization_method
+             for cfg in n.candidates_quantization_cfg])
+        if len(weights_quantization_methods) > 1:
+            Logger.critical(f"Invalid 'candidates_quantization_cfg': Inconsistent weights "
+                            f"quantization methods detected: {weights_quantization_methods}. "
+                            f"Trainable quantizer requires all candidates to have the same weights "
+                            f"quantization method.")  # pragma: no cover
 
     # all candidates must have the same activation quantization method
-    activation_quantization_methods = set([cfg.activation_quantization_cfg.activation_quantization_method for cfg in n.candidates_quantization_cfg])
+    activation_quantization_methods = set([cfg.activation_quantization_cfg.activation_quantization_method
+                                           for cfg in n.candidates_quantization_cfg])
     if len(activation_quantization_methods) > 1:
-        Logger.error(f'Unsupported candidates_quantization_cfg with different activation quantization methods: {activation_quantization_methods}') # pragma: no cover
+        Logger.critical(f"Invalid 'candidates_quantization_cfg': Inconsistent activation quantization "
+                        f"methods detected: {activation_quantization_methods}. "
+                        f"Trainable quantizer requires all candidates to have the same activation quantization method.")# pragma: no cover
 
     # get unique lists of candidates
-    unique_weights_candidates = n.get_unique_weights_candidates()
+    unique_weights_candidates = n.get_unique_weights_candidates(attr)
     unique_activation_candidates = n.get_unique_activation_candidates()
 
-    # verify all the combinations of weights_n_bits and activation_n_bits are allowed
-    if len(n.candidates_quantization_cfg) != len(unique_weights_candidates) * len(unique_activation_candidates):
-        Logger.error(f'Unsupported candidates_quantization_cfg for a trainable quantizer,'
-                            f'it must contain all the combinations of (weights_n_bits X activations_n_bits)') # pragma: no cover
-
     # generate list of weights quantizer candidates
     weights_cfg_candidates = [TrainableQuantizerCandidateConfig(
-        cfg.weights_quantization_cfg.weights_n_bits,
-        cfg.weights_quantization_cfg.weights_quantization_params) for cfg in unique_weights_candidates]
+        cfg.weights_quantization_cfg.get_attr_config(attr).weights_n_bits,
+        cfg.weights_quantization_cfg.get_attr_config(attr).weights_quantization_params)
+        for cfg in unique_weights_candidates]
 
     # generate list of activation quantizer candidates
     activation_cfg_candidates = [TrainableQuantizerCandidateConfig(
         cfg.activation_quantization_cfg.activation_n_bits,
         cfg.activation_quantization_cfg.activation_quantization_params) for cfg in unique_activation_candidates]
 
     return weights_cfg_candidates, activation_cfg_candidates
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,50 +14,49 @@
 # ==============================================================================
 from typing import Union, Any
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.constants \
-    import QUANTIZATION_TARGET, QUANTIZATION_METHOD, QUANTIZER_TYPE
+    import QUANTIZATION_TARGET, QUANTIZATION_METHOD, QUANTIZER_ID
 from mct_quantizers.common.get_all_subclasses \
     import get_all_subclasses
 
 
 def get_trainable_quantizer_class(quant_target: QuantizationTarget,
-                                  quantizer_type: Union[Any, Any],
+                                  quantizer_id: Any,
                                   quant_method: QuantizationMethod,
                                   quantizer_base_class: type) -> type:
     """
     Searches for a trainable quantizer class that matches the requested QuantizationTarget and QuantizationMethod and
     a task dedicated quantizer type. Exactly one class should be found.
 
     Args:
         quant_target: QuantizationTarget value which indicates what is the target for quantization to
             use the quantizer for.
-        quantizer_type: The type of the quantizer (quantization technique).
-            This can differ, depending on the purpose the quantizer is for.
+        quantizer_id: A unique identifier for the quantizer class.
         quant_method: A list of QuantizationMethod values to indicate all type of quantization methods that the
             quantizer supports.
         quantizer_base_class: A type of quantizer that the requested quantizer should inherit from.
 
     Returns: A class of a quantizer that inherits from BaseKerasQATTrainableQuantizer.
 
     """
     qat_quantizer_classes = get_all_subclasses(quantizer_base_class)
     if len(qat_quantizer_classes) == 0:
-        Logger.error(f"No quantizers were found that inherit from {quantizer_base_class}.")  # pragma: no cover
+        Logger.critical(f"No quantizer classes inherited from {quantizer_base_class} were detected.")  # pragma: no cover
 
     filtered_quantizers = list(filter(lambda q_class: getattr(q_class, QUANTIZATION_TARGET, None) is not None and
                                                       getattr(q_class, QUANTIZATION_TARGET) == quant_target and
                                                       getattr(q_class, QUANTIZATION_METHOD, None) is not None and
                                                        quant_method in getattr(q_class, QUANTIZATION_METHOD, []) and
-                                                      getattr(q_class, QUANTIZER_TYPE, None) == quantizer_type,
+                                                      getattr(q_class, QUANTIZER_ID, None) == quantizer_id,
                                       qat_quantizer_classes))
 
     if len(filtered_quantizers) != 1:
-        Logger.error(f"Found {len(filtered_quantizers)} quantizer for target {quant_target.value} "  # pragma: no cover
-                     f"that matches the requested quantization method {quant_method.name} and "
-                     f"quantizer type {quantizer_type.value} but there should be exactly one."
-                     f"The possible quantizers that were found are {filtered_quantizers}.")
+        Logger.critical(f"Found {len(filtered_quantizers)} quantizers for target {quant_target.value}, "
+                        f"matching the requested quantization method {quant_method.name} and "
+                        f"quantizer type {quantizer_id.value}, but exactly one is required. "
+                        f"Identified quantizers: {filtered_quantizers}.")
 
     return filtered_quantizers[0]
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py`

 * *Files 3% similar despite different names*

```diff
@@ -82,10 +82,9 @@
 
 else:
     class BaseKerasTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
 
             super().__init__(quantization_config)
-            Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                            'when using BaseKerasQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,20 +13,25 @@
 # limitations under the License.
 # ==============================================================================
 import copy
 
 from typing import Any, Union
 from enum import Enum
 
+import numpy as np
+
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerActivationConfig, TrainableQuantizerWeightsConfig
 from mct_quantizers.common import constants as C
 
 
+CONFIG = "config"
+VALUE = "value"
+
 def transform_enum(v: Any):
     """
     If an enum is received it value is return otherwise the input is returned.
     Args:
         v: Any type
 
     Returns: Any
@@ -59,17 +64,21 @@
         in_config:  A config dictionary of trainable quantizer config.
 
     Returns: Trainable quantizer configuration object - TrainableQuantizerWeightsConfig or TrainableQuantizerActivationConfig
 
     """
     in_config = copy.deepcopy(in_config)
     if in_config[C.IS_WEIGHTS]:
+        weights_quantization_params = {}
+        for key, value in in_config[C.WEIGHTS_QUANTIZATION_PARAMS].items():
+            # In TF2.13.0, serialization of numpy array is dictionary with parameters
+            weights_quantization_params.update({key: np.array(value[CONFIG][VALUE] if isinstance(value, dict) else value)})
         return TrainableQuantizerWeightsConfig(weights_quantization_method=QuantizationMethod(in_config[C.WEIGHTS_QUANTIZATION_METHOD]),
                                                weights_n_bits=in_config[C.WEIGHTS_N_BITS],
-                                               weights_quantization_params=in_config[C.WEIGHTS_QUANTIZATION_PARAMS],
+                                               weights_quantization_params=weights_quantization_params,
                                                enable_weights_quantization=in_config[C.ENABLE_WEIGHTS_QUANTIZATION],
                                                weights_channels_axis=in_config[C.WEIGHTS_CHANNELS_AXIS],
                                                weights_per_channel_threshold=in_config[C.WEIGHTS_PER_CHANNEL_THRESHOLD],
                                                min_threshold=in_config[C.MIN_THRESHOLD])
     elif in_config[C.IS_ACTIVATIONS]:
         return TrainableQuantizerActivationConfig(activation_quantization_method=QuantizationMethod(in_config[C.ACTIVATION_QUANTIZATION_METHOD]),
                                                   activation_n_bits=in_config[C.ACTIVATION_N_BITS],
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/load_model.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/load_model.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,14 +20,15 @@
 from model_compression_toolkit.constants import FOUND_TF
 from model_compression_toolkit.logger import Logger
 
 if FOUND_TF:
     import tensorflow as tf
     from tensorflow.python.saved_model.load_options import LoadOptions
     from model_compression_toolkit.trainable_infrastructure import BaseKerasTrainableQuantizer
+    from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
     keras = tf.keras
 
     def keras_load_quantized_model(filepath: str, custom_objects: Any = None, compile: bool = True,
                                    options: LoadOptions = None):
         """
         This function wraps the keras load model and adds trainable quantizers classes to its custom objects.
 
@@ -39,18 +40,20 @@
 
         Returns: A keras Model
 
         """
 
         qi_trainable_custom_objects = {subclass.__name__: subclass for subclass in
                                        get_all_subclasses(BaseKerasTrainableQuantizer)}
+        qi_trainable_custom_objects.update({
+            KerasTrainableQuantizationWrapper.__name__: KerasTrainableQuantizationWrapper})
         all_trainable_names = list(qi_trainable_custom_objects.keys())
         if len(set(all_trainable_names)) < len(all_trainable_names):
-            Logger.error(f"Found multiple quantizers with the same name that inherit from BaseKerasTrainableQuantizer"
-                         f"while trying to load a model.")
+            Logger.critical("Found multiple quantizers with identical names inheriting from "
+                            "'BaseKerasTrainableQuantizer' while trying to load a model.")
 
         qi_custom_objects = {**qi_trainable_custom_objects}
 
         if custom_objects is not None:
             qi_custom_objects.update(custom_objects)
         return mct_quantizers.keras_load_quantized_model(filepath,
                                                          custom_objects=qi_custom_objects, compile=compile,
@@ -65,10 +68,9 @@
             custom_objects: Additional custom objects
             compile: Boolean, whether to compile the model after loading.
             options: Optional `tf.saved_model.LoadOptions` object that specifies options for loading from SavedModel.
 
         Returns: A keras Model
 
         """
-        Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                        'when using keras_load_quantized_model. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        Logger.critical("Tensorflow must be installed to use keras_load_quantized_model. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/data_generation/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py` & `model_compression_toolkit-2.0.0/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -56,10 +56,10 @@
             return quantizer_trainable
 
 else:
     class BasePytorchTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             super().__init__(quantization_config)
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("PyTorch must be installed to use 'BasePytorchTrainableQuantizer'. "
+                            "The 'torch' package is missing.")  # pragma: no cover
+
```

### Comparing `model_compression_toolkit-1.9.1/model_compression_toolkit.egg-info/SOURCES.txt` & `model_compression_toolkit-2.0.0/model_compression_toolkit.egg-info/SOURCES.txt`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 LICENSE.md
 README.md
 setup.cfg
 setup.py
 model_compression_toolkit/__init__.py
 model_compression_toolkit/constants.py
+model_compression_toolkit/defaultdict.py
 model_compression_toolkit/logger.py
 model_compression_toolkit.egg-info/PKG-INFO
 model_compression_toolkit.egg-info/SOURCES.txt
 model_compression_toolkit.egg-info/dependency_links.txt
 model_compression_toolkit.egg-info/requires.txt
 model_compression_toolkit.egg-info/top_level.txt
 model_compression_toolkit/core/__init__.py
 model_compression_toolkit/core/analyzer.py
 model_compression_toolkit/core/exporter.py
+model_compression_toolkit/core/graph_prep_runner.py
+model_compression_toolkit/core/quantization_prep_runner.py
 model_compression_toolkit/core/runner.py
 model_compression_toolkit/core/common/__init__.py
 model_compression_toolkit/core/common/base_substitutions.py
-model_compression_toolkit/core/common/data_loader.py
-model_compression_toolkit/core/common/defaultdict.py
 model_compression_toolkit/core/common/framework_implementation.py
 model_compression_toolkit/core/common/framework_info.py
 model_compression_toolkit/core/common/memory_computation.py
 model_compression_toolkit/core/common/model_builder_mode.py
 model_compression_toolkit/core/common/model_collector.py
 model_compression_toolkit/core/common/model_validation.py
 model_compression_toolkit/core/common/node_prior_info.py
@@ -31,15 +32,14 @@
 model_compression_toolkit/core/common/back2framework/base_model_builder.py
 model_compression_toolkit/core/common/collectors/__init__.py
 model_compression_toolkit/core/common/collectors/base_collector.py
 model_compression_toolkit/core/common/collectors/histogram_collector.py
 model_compression_toolkit/core/common/collectors/mean_collector.py
 model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
 model_compression_toolkit/core/common/collectors/statistics_collector.py
-model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
 model_compression_toolkit/core/common/fusion/__init__.py
 model_compression_toolkit/core/common/fusion/layer_fusing.py
 model_compression_toolkit/core/common/graph/__init__.py
 model_compression_toolkit/core/common/graph/base_graph.py
 model_compression_toolkit/core/common/graph/base_node.py
 model_compression_toolkit/core/common/graph/edge.py
 model_compression_toolkit/core/common/graph/functional_node.py
@@ -49,68 +49,90 @@
 model_compression_toolkit/core/common/graph/memory_graph/__init__.py
 model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
 model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
 model_compression_toolkit/core/common/graph/memory_graph/cut.py
 model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
 model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
 model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
+model_compression_toolkit/core/common/hessian/__init__.py
+model_compression_toolkit/core/common/hessian/hessian_info_service.py
+model_compression_toolkit/core/common/hessian/hessian_info_utils.py
+model_compression_toolkit/core/common/hessian/trace_hessian_calculator.py
+model_compression_toolkit/core/common/hessian/trace_hessian_request.py
 model_compression_toolkit/core/common/matchers/__init__.py
 model_compression_toolkit/core/common/matchers/base_graph_filter.py
 model_compression_toolkit/core/common/matchers/base_matcher.py
 model_compression_toolkit/core/common/matchers/edge_matcher.py
 model_compression_toolkit/core/common/matchers/function.py
 model_compression_toolkit/core/common/matchers/node_matcher.py
 model_compression_toolkit/core/common/matchers/walk_matcher.py
 model_compression_toolkit/core/common/mixed_precision/__init__.py
 model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
+model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py
+model_compression_toolkit/core/common/mixed_precision/configurable_quantizer_utils.py
 model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
 model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
+model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py
 model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py
 model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
 model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
 model_compression_toolkit/core/common/network_editors/__init__.py
 model_compression_toolkit/core/common/network_editors/actions.py
 model_compression_toolkit/core/common/network_editors/edit_network.py
 model_compression_toolkit/core/common/network_editors/node_filters.py
+model_compression_toolkit/core/common/pruning/__init__.py
+model_compression_toolkit/core/common/pruning/channels_grouping.py
+model_compression_toolkit/core/common/pruning/greedy_mask_calculator.py
+model_compression_toolkit/core/common/pruning/memory_calculator.py
+model_compression_toolkit/core/common/pruning/prune_graph.py
+model_compression_toolkit/core/common/pruning/pruner.py
+model_compression_toolkit/core/common/pruning/pruning_config.py
+model_compression_toolkit/core/common/pruning/pruning_framework_implementation.py
+model_compression_toolkit/core/common/pruning/pruning_info.py
+model_compression_toolkit/core/common/pruning/pruning_section.py
+model_compression_toolkit/core/common/pruning/importance_metrics/__init__.py
+model_compression_toolkit/core/common/pruning/importance_metrics/base_importance_metric.py
+model_compression_toolkit/core/common/pruning/importance_metrics/importance_metric_factory.py
+model_compression_toolkit/core/common/pruning/importance_metrics/lfh_importance_metric.py
+model_compression_toolkit/core/common/pruning/mask/__init__.py
+model_compression_toolkit/core/common/pruning/mask/per_channel_mask.py
+model_compression_toolkit/core/common/pruning/mask/per_simd_group_mask.py
 model_compression_toolkit/core/common/quantization/__init__.py
 model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
 model_compression_toolkit/core/common/quantization/core_config.py
 model_compression_toolkit/core/common/quantization/debug_config.py
 model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
 model_compression_toolkit/core/common/quantization/node_quantization_config.py
-model_compression_toolkit/core/common/quantization/quantization_analyzer.py
 model_compression_toolkit/core/common/quantization/quantization_config.py
 model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
 model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
 model_compression_toolkit/core/common/quantization/quantize_node.py
 model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
-model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
 model_compression_toolkit/core/common/quantization/quantizers/__init__.py
-model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
 model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
 model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
 model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
 model_compression_toolkit/core/common/statistics_correction/__init__.py
 model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
 model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
 model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
@@ -130,58 +152,61 @@
 model_compression_toolkit/core/common/substitutions/weights_activation_split.py
 model_compression_toolkit/core/common/visualization/__init__.py
 model_compression_toolkit/core/common/visualization/final_config_visualizer.py
 model_compression_toolkit/core/common/visualization/nn_visualizer.py
 model_compression_toolkit/core/common/visualization/tensorboard_writer.py
 model_compression_toolkit/core/keras/__init__.py
 model_compression_toolkit/core/keras/constants.py
+model_compression_toolkit/core/keras/custom_layer_validation.py
 model_compression_toolkit/core/keras/default_framework_info.py
 model_compression_toolkit/core/keras/keras_implementation.py
 model_compression_toolkit/core/keras/keras_model_validation.py
 model_compression_toolkit/core/keras/keras_node_prior_info.py
-model_compression_toolkit/core/keras/kpi_data_facade.py
+model_compression_toolkit/core/keras/resource_utilization_data_facade.py
 model_compression_toolkit/core/keras/tf_tensor_numpy.py
 model_compression_toolkit/core/keras/back2framework/__init__.py
 model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
 model_compression_toolkit/core/keras/back2framework/float_model_builder.py
 model_compression_toolkit/core/keras/back2framework/instance_builder.py
 model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
 model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
-model_compression_toolkit/core/keras/back2framework/model_gradients.py
 model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
 model_compression_toolkit/core/keras/graph_substitutions/__init__.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
+model_compression_toolkit/core/keras/graph_substitutions/substitutions/dwconv_to_conv.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
+model_compression_toolkit/core/keras/graph_substitutions/substitutions/matmul_substitution.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
+model_compression_toolkit/core/keras/hessian/__init__.py
+model_compression_toolkit/core/keras/hessian/activation_trace_hessian_calculator_keras.py
+model_compression_toolkit/core/keras/hessian/trace_hessian_calculator_keras.py
+model_compression_toolkit/core/keras/hessian/weights_trace_hessian_calculator_keras.py
 model_compression_toolkit/core/keras/mixed_precision/__init__.py
-model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py
+model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py
+model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py
+model_compression_toolkit/core/keras/pruning/__init__.py
+model_compression_toolkit/core/keras/pruning/pruning_keras_implementation.py
 model_compression_toolkit/core/keras/quantizer/__init__.py
 model_compression_toolkit/core/keras/quantizer/base_quantizer.py
 model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
-model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py
 model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
-model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py
-model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py
-model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py
-model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py
-model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py
 model_compression_toolkit/core/keras/reader/__init__.py
 model_compression_toolkit/core/keras/reader/common.py
 model_compression_toolkit/core/keras/reader/connectivity_handler.py
 model_compression_toolkit/core/keras/reader/node_builder.py
 model_compression_toolkit/core/keras/reader/reader.py
 model_compression_toolkit/core/keras/reader/nested_model/__init__.py
 model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
@@ -190,88 +215,131 @@
 model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
 model_compression_toolkit/core/keras/statistics_correction/__init__.py
 model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
 model_compression_toolkit/core/keras/visualization/__init__.py
 model_compression_toolkit/core/pytorch/__init__.py
 model_compression_toolkit/core/pytorch/constants.py
 model_compression_toolkit/core/pytorch/default_framework_info.py
-model_compression_toolkit/core/pytorch/kpi_data_facade.py
+model_compression_toolkit/core/pytorch/pytorch_device_config.py
 model_compression_toolkit/core/pytorch/pytorch_implementation.py
 model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
+model_compression_toolkit/core/pytorch/resource_utilization_data_facade.py
 model_compression_toolkit/core/pytorch/utils.py
 model_compression_toolkit/core/pytorch/back2framework/__init__.py
 model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
 model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
-model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
 model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
 model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
+model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_batch_norm.py
+model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_layer_norm.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
+model_compression_toolkit/core/pytorch/hessian/__init__.py
+model_compression_toolkit/core/pytorch/hessian/activation_trace_hessian_calculator_pytorch.py
+model_compression_toolkit/core/pytorch/hessian/trace_hessian_calculator_pytorch.py
+model_compression_toolkit/core/pytorch/hessian/weights_trace_hessian_calculator_pytorch.py
 model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
-model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py
-model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py
+model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py
+model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py
+model_compression_toolkit/core/pytorch/pruning/__init__.py
+model_compression_toolkit/core/pytorch/pruning/pruning_pytorch_implementation.py
 model_compression_toolkit/core/pytorch/quantizer/__init__.py
 model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
 model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
 model_compression_toolkit/core/pytorch/reader/__init__.py
 model_compression_toolkit/core/pytorch/reader/graph_builders.py
 model_compression_toolkit/core/pytorch/reader/node_holders.py
 model_compression_toolkit/core/pytorch/reader/reader.py
 model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
 model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
+model_compression_toolkit/data_generation/__init__.py
+model_compression_toolkit/data_generation/common/__init__.py
+model_compression_toolkit/data_generation/common/constants.py
+model_compression_toolkit/data_generation/common/data_generation.py
+model_compression_toolkit/data_generation/common/data_generation_config.py
+model_compression_toolkit/data_generation/common/enums.py
+model_compression_toolkit/data_generation/common/image_pipeline.py
+model_compression_toolkit/data_generation/common/model_info_exctractors.py
+model_compression_toolkit/data_generation/common/optimization_utils.py
+model_compression_toolkit/data_generation/keras/__init__.py
+model_compression_toolkit/data_generation/keras/constants.py
+model_compression_toolkit/data_generation/keras/image_pipeline.py
+model_compression_toolkit/data_generation/keras/keras_data_generation.py
+model_compression_toolkit/data_generation/keras/model_info_exctractors.py
+model_compression_toolkit/data_generation/keras/optimization_utils.py
+model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py
+model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py
+model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py
+model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py
+model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py
+model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py
+model_compression_toolkit/data_generation/pytorch/__init__.py
+model_compression_toolkit/data_generation/pytorch/constants.py
+model_compression_toolkit/data_generation/pytorch/image_pipeline.py
+model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py
+model_compression_toolkit/data_generation/pytorch/optimization_utils.py
+model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py
+model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py
 model_compression_toolkit/exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
+model_compression_toolkit/exporter/model_exporter/fw_agonstic/quantization_format.py
 model_compression_toolkit/exporter/model_exporter/keras/__init__.py
 model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
+model_compression_toolkit/exporter/model_exporter/keras/mctq_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
 model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
 model_compression_toolkit/exporter/model_wrapper/__init__.py
+model_compression_toolkit/exporter/model_wrapper/fw_agnostic/__init__.py
+model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py
 model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
-model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
-model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
 model_compression_toolkit/gptq/__init__.py
 model_compression_toolkit/gptq/runner.py
 model_compression_toolkit/gptq/common/__init__.py
 model_compression_toolkit/gptq/common/gptq_config.py
 model_compression_toolkit/gptq/common/gptq_constants.py
 model_compression_toolkit/gptq/common/gptq_framework_implementation.py
 model_compression_toolkit/gptq/common/gptq_graph.py
@@ -306,17 +374,19 @@
 model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
-model_compression_toolkit/legacy/__init__.py
-model_compression_toolkit/legacy/keras_quantization_facade.py
-model_compression_toolkit/legacy/pytorch_quantization_facade.py
+model_compression_toolkit/pruning/__init__.py
+model_compression_toolkit/pruning/keras/__init__.py
+model_compression_toolkit/pruning/keras/pruning_facade.py
+model_compression_toolkit/pruning/pytorch/__init__.py
+model_compression_toolkit/pruning/pytorch/pruning_facade.py
 model_compression_toolkit/ptq/__init__.py
 model_compression_toolkit/ptq/runner.py
 model_compression_toolkit/ptq/keras/__init__.py
 model_compression_toolkit/ptq/keras/quantization_facade.py
 model_compression_toolkit/ptq/pytorch/__init__.py
 model_compression_toolkit/ptq/pytorch/quantization_facade.py
 model_compression_toolkit/qat/__init__.py
@@ -324,84 +394,66 @@
 model_compression_toolkit/qat/common/qat_config.py
 model_compression_toolkit/qat/keras/__init__.py
 model_compression_toolkit/qat/keras/quantization_facade.py
 model_compression_toolkit/qat/keras/quantizer/__init__.py
 model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
 model_compression_toolkit/qat/keras/quantizer/quant_utils.py
 model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
+model_compression_toolkit/qat/keras/quantizer/lsq/__init__.py
+model_compression_toolkit/qat/keras/quantizer/lsq/symmetric_lsq.py
+model_compression_toolkit/qat/keras/quantizer/lsq/uniform_lsq.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
 model_compression_toolkit/qat/pytorch/__init__.py
 model_compression_toolkit/qat/pytorch/quantization_facade.py
 model_compression_toolkit/qat/pytorch/quantizer/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
 model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
 model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/__init__.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/symmetric_lsq.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/uniform_lsq.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
 model_compression_toolkit/target_platform_capabilities/__init__.py
 model_compression_toolkit/target_platform_capabilities/constants.py
 model_compression_toolkit/target_platform_capabilities/immutable.py
 model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
 model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
 model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
 model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
 model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
-model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
 model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
 model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/target_platform_capabilities.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tp_model.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_keras.py
-model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
 model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
@@ -420,10 +472,11 @@
 model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
 model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
 model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
 model_compression_toolkit/trainable_infrastructure/keras/__init__.py
 model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
 model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
 model_compression_toolkit/trainable_infrastructure/keras/load_model.py
+model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py
 model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
 model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
 model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
```

### Comparing `model_compression_toolkit-1.9.1/setup.py` & `model_compression_toolkit-2.0.0/setup.py`

 * *Files identical despite different names*

