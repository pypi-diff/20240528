# Comparing `tmp/text_fabric-9.4.4-py3-none-any.whl.zip` & `tmp/text_fabric-9.5.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,15 +1,15 @@
-Zip file size: 7312749 bytes, number of entries: 196
+Zip file size: 7314580 bytes, number of entries: 196
 -rw-r--r--  2.0 unx       38 b- defN 21-Feb-02 10:17 tf/__init__.py
 -rw-r--r--  2.0 unx     1652 b- defN 22-Feb-02 10:33 tf/app.py
 -rw-r--r--  2.0 unx       45 b- defN 21-Feb-03 12:48 tf/cheatsheet.py
 -rw-r--r--  2.0 unx     3773 b- defN 21-Dec-11 12:29 tf/clean.py
--rw-r--r--  2.0 unx     7392 b- defN 21-Dec-19 15:49 tf/fabric.py
+-rw-r--r--  2.0 unx     7384 b- defN 22-Jun-13 15:12 tf/fabric.py
 -rw-r--r--  2.0 unx     2034 b- defN 21-Dec-11 12:36 tf/lib.py
--rw-r--r--  2.0 unx     7069 b- defN 22-May-16 14:44 tf/parameters.py
+-rw-r--r--  2.0 unx     7069 b- defN 22-Jun-14 12:26 tf/parameters.py
 -rw-r--r--  2.0 unx       49 b- defN 21-Feb-02 10:28 tf/about/__init__.py
 -rw-r--r--  2.0 unx       43 b- defN 21-Feb-02 08:21 tf/about/apps.py
 -rw-r--r--  2.0 unx       49 b- defN 21-Feb-02 08:21 tf/about/background.py
 -rw-r--r--  2.0 unx       46 b- defN 21-Feb-02 08:21 tf/about/browser.py
 -rw-r--r--  2.0 unx       51 b- defN 21-Apr-30 11:06 tf/about/clientmanual.py
 -rw-r--r--  2.0 unx       43 b- defN 21-Feb-02 08:23 tf/about/code.py
 -rw-r--r--  2.0 unx       46 b- defN 21-Feb-02 08:23 tf/about/corpora.py
@@ -29,30 +29,30 @@
 -rw-r--r--  2.0 unx       42 b- defN 22-Feb-02 08:14 tf/about/use.py
 -rw-r--r--  2.0 unx       46 b- defN 22-Feb-02 08:20 tf/about/usefunc.py
 -rw-r--r--  2.0 unx       46 b- defN 21-Jul-29 06:03 tf/about/volumes.py
 -rw-r--r--  2.0 unx     1702 b- defN 21-Feb-02 08:52 tf/advanced/__init__.py
 -rw-r--r--  2.0 unx    16592 b- defN 22-Feb-02 10:23 tf/advanced/app.py
 -rw-r--r--  2.0 unx     2545 b- defN 22-May-12 10:32 tf/advanced/condense.py
 -rw-r--r--  2.0 unx     9658 b- defN 22-Jan-24 19:10 tf/advanced/data.py
--rw-r--r--  2.0 unx    29859 b- defN 22-May-12 10:32 tf/advanced/display.py
+-rw-r--r--  2.0 unx    29871 b- defN 22-Jun-13 14:28 tf/advanced/display.py
 -rw-r--r--  2.0 unx     5363 b- defN 22-Jan-05 13:33 tf/advanced/find.py
--rw-r--r--  2.0 unx    15956 b- defN 21-Dec-11 12:27 tf/advanced/helpers.py
+-rw-r--r--  2.0 unx    15956 b- defN 22-May-17 08:31 tf/advanced/helpers.py
 -rw-r--r--  2.0 unx     4471 b- defN 22-Apr-29 09:14 tf/advanced/highlight.py
 -rw-r--r--  2.0 unx    19819 b- defN 22-May-04 08:28 tf/advanced/links.py
--rw-r--r--  2.0 unx    20509 b- defN 22-Jan-12 12:49 tf/advanced/options.py
--rw-r--r--  2.0 unx    15313 b- defN 21-Aug-26 07:59 tf/advanced/render.py
--rw-r--r--  2.0 unx    39950 b- defN 22-Feb-01 12:23 tf/advanced/repo.py
+-rw-r--r--  2.0 unx    20509 b- defN 22-May-17 08:31 tf/advanced/options.py
+-rw-r--r--  2.0 unx    15313 b- defN 22-May-17 08:32 tf/advanced/render.py
+-rw-r--r--  2.0 unx    39352 b- defN 22-Jun-13 14:48 tf/advanced/repo.py
 -rw-r--r--  2.0 unx     7363 b- defN 22-Apr-29 09:17 tf/advanced/search.py
 -rw-r--r--  2.0 unx     5464 b- defN 20-Jun-10 19:56 tf/advanced/sections.py
--rw-r--r--  2.0 unx    49028 b- defN 22-May-03 13:34 tf/advanced/settings.py
+-rw-r--r--  2.0 unx    48973 b- defN 22-Jun-13 14:55 tf/advanced/settings.py
 -rw-r--r--  2.0 unx     8432 b- defN 22-Mar-21 09:52 tf/advanced/tables.py
--rw-r--r--  2.0 unx     2828 b- defN 22-Apr-28 13:58 tf/advanced/text.py
+-rw-r--r--  2.0 unx     3397 b- defN 22-Jun-13 08:31 tf/advanced/text.py
 -rw-r--r--  2.0 unx    19582 b- defN 22-Jan-12 12:49 tf/advanced/unravel.py
 -rw-r--r--  2.0 unx     1436 b- defN 21-Jul-29 18:23 tf/advanced/volumes.py
--rw-r--r--  2.0 unx     5842 b- defN 22-Jan-06 14:48 tf/advanced/zipdata.py
+-rw-r--r--  2.0 unx     5743 b- defN 22-May-18 08:30 tf/advanced/zipdata.py
 -rw-r--r--  2.0 unx      944 b- defN 21-May-04 05:24 tf/client/__init__.py
 -rw-r--r--  2.0 unx       36 b- defN 21-Apr-29 14:51 tf/client/make/__init__.py
 -rw-r--r--  2.0 unx   215294 b- defN 22-Jan-05 13:09 tf/client/make/build-notebook.ipynb
 -rw-r--r--  2.0 unx     5418 b- defN 22-Jan-05 13:09 tf/client/make/build-notebook.py
 -rw-r--r--  2.0 unx    43932 b- defN 22-May-16 13:14 tf/client/make/build.py
 -rw-r--r--  2.0 unx     3419 b- defN 22-Jan-05 13:33 tf/client/make/config.yaml
 -rw-r--r--  2.0 unx     4378 b- defN 21-Dec-24 09:06 tf/client/make/gh.py
@@ -80,45 +80,45 @@
 -rw-r--r--  2.0 unx     6148 b- defN 21-Apr-29 05:38 tf/client/static/jslib/.DS_Store
 -rw-r--r--  2.0 unx    89501 b- defN 21-Apr-14 13:40 tf/client/static/jslib/jquery.js
 -rw-r--r--  2.0 unx     6148 b- defN 21-Apr-29 05:37 tf/client/static/png/.DS_Store
 -rw-r--r--  2.0 unx    13001 b- defN 13-Oct-28 21:55 tf/client/static/png/github.png
 -rwxr-xr-x  2.0 unx    43381 b- defN 21-Apr-14 07:05 tf/client/static/png/maker.png
 -rw-r--r--  2.0 unx   149670 b- defN 21-Apr-14 07:06 tf/client/static/png/tf.png
 -rw-r--r--  2.0 unx      413 b- defN 21-Mar-11 13:26 tf/convert/__init__.py
--rw-r--r--  2.0 unx    27615 b- defN 21-Dec-12 10:59 tf/convert/mql.py
--rw-r--r--  2.0 unx    22143 b- defN 21-Dec-23 11:11 tf/convert/recorder.py
+-rw-r--r--  2.0 unx    27621 b- defN 22-May-18 08:41 tf/convert/mql.py
+-rw-r--r--  2.0 unx    23813 b- defN 22-Jun-13 14:30 tf/convert/recorder.py
 -rw-r--r--  2.0 unx     9848 b- defN 21-Dec-12 11:04 tf/convert/tf.py
--rw-r--r--  2.0 unx    50211 b- defN 21-Dec-24 12:22 tf/convert/walker.py
+-rw-r--r--  2.0 unx    51165 b- defN 22-Jun-08 08:56 tf/convert/walker.py
 -rw-r--r--  2.0 unx      515 b- defN 21-Mar-11 13:26 tf/core/__init__.py
 -rw-r--r--  2.0 unx    16108 b- defN 21-Dec-15 09:33 tf/core/api.py
 -rw-r--r--  2.0 unx     1530 b- defN 21-Jul-22 07:01 tf/core/computed.py
 -rw-r--r--  2.0 unx    24844 b- defN 22-Apr-28 13:13 tf/core/data.py
 -rw-r--r--  2.0 unx     9793 b- defN 21-Jul-28 10:32 tf/core/edgefeature.py
--rw-r--r--  2.0 unx    42143 b- defN 22-Feb-04 15:26 tf/core/fabric.py
--rw-r--r--  2.0 unx    12218 b- defN 22-Jan-12 12:49 tf/core/helpers.py
+-rw-r--r--  2.0 unx    42149 b- defN 22-Jun-13 08:19 tf/core/fabric.py
+-rw-r--r--  2.0 unx    13915 b- defN 22-Jun-13 08:16 tf/core/helpers.py
 -rw-r--r--  2.0 unx     9691 b- defN 22-May-11 14:25 tf/core/locality.py
 -rw-r--r--  2.0 unx     3573 b- defN 21-Mar-11 13:26 tf/core/nodefeature.py
--rw-r--r--  2.0 unx     7893 b- defN 21-Mar-11 13:26 tf/core/nodes.py
+-rw-r--r--  2.0 unx     9471 b- defN 22-May-17 15:55 tf/core/nodes.py
 -rw-r--r--  2.0 unx     1588 b- defN 21-Mar-11 13:26 tf/core/oslotsfeature.py
 -rw-r--r--  2.0 unx     3221 b- defN 21-Mar-11 13:26 tf/core/otypefeature.py
 -rw-r--r--  2.0 unx    24169 b- defN 22-May-12 10:48 tf/core/prepare.py
--rw-r--r--  2.0 unx    44463 b- defN 21-Jul-22 07:18 tf/core/text.py
+-rw-r--r--  2.0 unx    45235 b- defN 22-Jun-13 08:29 tf/core/text.py
 -rw-r--r--  2.0 unx     7753 b- defN 21-Mar-22 13:25 tf/core/timestamp.py
--rw-r--r--  2.0 unx      543 b- defN 21-Jul-22 09:29 tf/dataset/__init__.py
--rw-r--r--  2.0 unx    34376 b- defN 22-Jan-12 12:53 tf/dataset/modify.py
--rw-r--r--  2.0 unx    24837 b- defN 21-Dec-16 10:22 tf/dataset/nodemaps.py
+-rw-r--r--  2.0 unx      542 b- defN 22-Jun-13 14:33 tf/dataset/__init__.py
+-rw-r--r--  2.0 unx    34377 b- defN 22-Jun-13 08:16 tf/dataset/modify.py
+-rw-r--r--  2.0 unx    24838 b- defN 22-Jun-13 14:31 tf/dataset/nodemaps.py
 -rw-r--r--  2.0 unx      173 b- defN 21-Mar-11 13:26 tf/search/__init__.py
 -rw-r--r--  2.0 unx     7230 b- defN 22-Jan-12 12:53 tf/search/graph.py
 -rw-r--r--  2.0 unx    53131 b- defN 22-Apr-28 13:26 tf/search/relations.py
 -rw-r--r--  2.0 unx    19745 b- defN 22-Apr-29 08:53 tf/search/search.py
 -rw-r--r--  2.0 unx     6482 b- defN 22-Apr-29 09:14 tf/search/searchexe.py
 -rw-r--r--  2.0 unx    18880 b- defN 22-Apr-28 13:07 tf/search/semantics.py
 -rw-r--r--  2.0 unx    14005 b- defN 22-Apr-28 14:53 tf/search/spin.py
 -rw-r--r--  2.0 unx    28344 b- defN 22-Apr-28 13:07 tf/search/stitch.py
--rw-r--r--  2.0 unx    22260 b- defN 21-Mar-11 13:26 tf/search/syntax.py
+-rw-r--r--  2.0 unx    22424 b- defN 22-May-31 19:36 tf/search/syntax.py
 -rw-r--r--  2.0 unx       39 b- defN 21-Feb-02 10:26 tf/server/__init__.py
 -rw-r--r--  2.0 unx     4032 b- defN 22-Jan-05 11:57 tf/server/command.py
 -rw-r--r--  2.0 unx    22264 b- defN 22-May-03 06:03 tf/server/kernel.py
 -rw-r--r--  2.0 unx      990 b- defN 21-Mar-11 13:26 tf/server/monitor.py
 -rw-r--r--  2.0 unx    11568 b- defN 22-Mar-21 09:52 tf/server/serve.py
 -rw-r--r--  2.0 unx     4974 b- defN 21-Feb-03 09:12 tf/server/servelib.py
 -rw-r--r--  2.0 unx    12199 b- defN 22-Feb-02 11:10 tf/server/start.py
@@ -185,14 +185,14 @@
 -rw-r--r--  2.0 unx      462 b- defN 21-Mar-17 15:48 tf/writing/__init__.py
 -rw-r--r--  2.0 unx       47 b- defN 21-Feb-02 10:24 tf/writing/arabic.py
 -rw-r--r--  2.0 unx       46 b- defN 21-Feb-02 10:24 tf/writing/greek.py
 -rw-r--r--  2.0 unx       47 b- defN 21-Feb-02 10:24 tf/writing/hebrew.py
 -rw-r--r--  2.0 unx       51 b- defN 21-Apr-21 10:17 tf/writing/neoaramaic.py
 -rw-r--r--  2.0 unx       47 b- defN 21-Feb-02 10:24 tf/writing/syriac.py
 -rw-r--r--  2.0 unx    34866 b- defN 21-Feb-02 10:02 tf/writing/transcription.py
--rw-r--r--  2.0 unx     1068 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     1931 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/WHEEL
--rw-r--r--  2.0 unx      142 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        3 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    16251 b- defN 22-May-16 14:44 text_fabric-9.4.4.dist-info/RECORD
-196 files, 14438704 bytes uncompressed, 7287621 bytes compressed:  49.5%
+-rw-r--r--  2.0 unx     1068 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1931 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx      142 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        3 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    16251 b- defN 22-Jun-14 12:27 text_fabric-9.5.2.dist-info/RECORD
+196 files, 14445373 bytes uncompressed, 7289452 bytes compressed:  49.5%
```

## zipnote {}

```diff
@@ -564,26 +564,26 @@
 
 Filename: tf/writing/syriac.py
 Comment: 
 
 Filename: tf/writing/transcription.py
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/LICENSE
+Filename: text_fabric-9.5.2.dist-info/LICENSE
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/METADATA
+Filename: text_fabric-9.5.2.dist-info/METADATA
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/WHEEL
+Filename: text_fabric-9.5.2.dist-info/WHEEL
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/entry_points.txt
+Filename: text_fabric-9.5.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/top_level.txt
+Filename: text_fabric-9.5.2.dist-info/top_level.txt
 Comment: 
 
-Filename: text_fabric-9.4.4.dist-info/RECORD
+Filename: text_fabric-9.5.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tf/fabric.py

```diff
@@ -2,15 +2,15 @@
 # Fabric
 
 The main class that works the core API is `tf.fabric.Fabric`.
 
 It is responsible for feature data loading and saving.
 
 !!! note "Tutorial"
-    The tutorials for specific [annotated corpora](https://github.com/annotation)
+    The tutorials for specific annotated corpora (see `tf.about.corpora`)
     put the Text-Fabric API on show for vastly different corpora.
 
 !!! note "Generic API versus apps"
     This is the API of Text-Fabric in general.
     Text-Fabric has no baked in knowledge of particular corpora.
 
     However, Text-Fabric comes with several *apps* that make working
```

## tf/parameters.py

```diff
@@ -4,15 +4,15 @@
 Fixed values for the whole program.
 """
 
 import sys
 from zipfile import ZIP_DEFLATED
 
 
-VERSION = '9.4.4'
+VERSION = '9.5.2'
 """Program version.
 
 This value is under control of the update process, as run by
 `build.py` in the top-level directory of the repo.
 """
 
 NAME = "Text-Fabric"
```

## tf/advanced/display.py

```diff
@@ -268,17 +268,17 @@
 
 
 def export(app, tuples, toDir=None, toFile="results.tsv", **options):
     """Exports an iterable of tuples of nodes to an Excel friendly `.tsv` file.
 
     !!! hint "Examples"
         See for detailed examples the
-        [exportExcel (bhsa)](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/bhsa/exportExcel.ipynb)
+        [exportExcel (etcbc/bhsa)](https://nbviewer.jupyter.org/github/etcbc/bhsa/blob/master/tutorial/exportExcel.ipynb)
         and
-        [exportExcel (oldbabylonian)](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/oldbabylonian/exportExcel.ipynb)
+        [exportExcel (Nino-cunei/oldbabylonian)](https://nbviewer.jupyter.org/github/Nino-cunei/oldbabylonian/blob/master/tutorial/exportExcel.ipynb)
         notebooks.
 
     Parameters
     ----------
     tuples: iterable of tuples of integer
         The integers are the nodes, together they form a table.
         The table maybe uniform or not uniform,
@@ -372,15 +372,15 @@
     fmt = dContext.fmt
     condenseType = dContext.condenseType
     tupleFeatures = dContext.tupleFeatures
 
     if toDir is None:
         toDir = expanduser(DOWNLOADS)
     else:
-        toDir = normpath(toDir)
+        toDir = expanduser(toDir)
     if not os.path.exists(toDir):
         os.makedirs(toDir, exist_ok=True)
     toPath = f"{toDir}/{toFile}"
 
     resultsX = getRowsX(app, tuples, tupleFeatures, condenseType, fmt=fmt)
 
     with open(toPath, "w", encoding="utf_16_le") as fh:
```

## tf/advanced/repo.py

```diff
@@ -61,15 +61,15 @@
 will be taken and where it will be placed.
 That will be either your `~/github` or your `~/text-fabric-data` directories.
 
 You can override the hard-coded `~/github` and `~/text-fabric-data` directories
 by passing *source* and *dest* respectively.
 
 See the
-[repo](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/banks/repo.ipynb)
+[repo](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/tutorial/repo.ipynb)
 notebook for an exhaustive demo of all the checkout options.
 
 ## other parameters
 
 *withPaths=False* will loose the directory structure of files that are being
 downloaded.
 
@@ -223,45 +223,31 @@
 
 If you want to load a new corpus after having passed the rate limit, and not
 wanting to wait an hour, you could directly clone the repos from GitHub:
 
 Open your terminal, and go to (or create) directory `~/github` (in your
 home directory).
 
-Inside that directory, go to or create directory `annotation`.
+Inside that directory, go to or create directory `org`
 Go to that directory.
 
 Then do
 
 ``` sh
-git clone https://github.com/annotation/app-corpus
+git clone https://github.com/org/repo
 ```
 
-(replacing `corpus` with the name of your corpus).
+(replacing `org` and `repo` with the values that apply to your corpus).
 
-This will fetch the Text-Fabric *app* for that corpus.
+This will fetch the Text-Fabric *data*, *app*, and *tutorials* for that corpus.
 
-Now the corpus data itself:
-
-Find out where on GitHub it is (organization/corpus), e.g.
-`Nino-cunei/oldbabylonian` or `etcbc/bhsa`.
-
-Under your local `~/github`, find or create directory `organization`.
-Then go to that directory and say:
-
-``` sh
-git clone https://github.com/organization/corpus
-```
-
-(replacing `organization` with the name of the organization where the corpus resides
-and `corpus` with the name of your corpus).
 Now you have all data you need on your system.
 
 If you want to see by example how to use this data, have a look at
-[repo](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/banks/repo.ipynb),
+[repo](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/tutorial/repo.ipynb),
 especially when it discusses `clone`.
 
 In order to run Text-Fabric without further access to GitHub, say
 
 ``` sh
 text-fabric corpus:clone checkout=clone
 ```
@@ -375,15 +361,15 @@
     URL_TFDOC,
     GH_BASE,
     EXPRESS_BASE,
     EXPRESS_SYNC,
     EXPRESS_SYNC_LEGACY,
     DOWNLOADS,
 )
-from ..core.helpers import console, htmlEsc, expanduser
+from ..core.helpers import console, htmlEsc, expanduser, initTree
 from .helpers import dh
 from .zipdata import zipData
 
 
 class Repo:
     def __init__(
         self,
@@ -1021,18 +1007,15 @@
 
         self.log(f"\tsaving {label}")
 
         cwd = os.getcwd()
         destZip = self.dirPathLocal
         try:
             z = ZipFile(zf)
-            if not self.keep:
-                if os.path.exists(destZip):
-                    rmtree(destZip)
-            os.makedirs(destZip, exist_ok=True)
+            initTree(destZip, fresh=not self.keep)
             os.chdir(destZip)
             if self.withPaths:
                 z.extractall()
                 if os.path.exists("__MACOSX"):
                     rmtree("__MACOSX")
             else:
                 for zInfo in z.infolist():
@@ -1053,18 +1036,15 @@
     def downloadDir(self, commit, exclude=None, showErrors=False):
         g = self.repoOnline
         if not g:
             return None
 
         destDir = f"{self.dirPathLocal}"
         destSave = f"{self.dirPathSaveLocal}"
-        if not self.keep:
-            if os.path.exists(destDir):
-                rmtree(destDir)
-        os.makedirs(destDir, exist_ok=True)
+        initTree(destDir, fresh=not self.keep)
 
         excludeRe = re.compile(exclude) if exclude else None
 
         good = True
 
         def _downloadDir(subPath, level=0):
             nonlocal good
```

## tf/advanced/settings.py

```diff
@@ -2,16 +2,16 @@
 # App settings
 
 Developers can create TF-apps by specifying a `config.yaml` with settings.
 These settings will be read, checked, and transformed into configuration data
 that is read by the app, see `tf.advanced.settings.showContext`
 
 See for examples:
-* [bhsa](https://github.com/annotation/app-bhsa/blob/master/code/config.yaml).
-* [uruk](https://github.com/annotation/app-uruk/blob/master/code/config.yaml).
+* [bhsa](https://github.com/etcbc/bhsa/blob/master/app/config.yaml).
+* [uruk](https://github.com/Nino-cunei/uruk/blob/master/app/config.yaml).
 
 # Config specs
 
 Here is a specification of all settings you can configure for an app.
 
 Each section below corresponds to a main key in the `config.yaml` of an app.
 
@@ -454,15 +454,15 @@
 If present, it is a dictionary that specifies offsets between page numbers as derived
 from section headings and page numbers as needed in the query string for te url
 of the online resource (see `webUrl`).
 
 Suppose we need to offset sections of level 2 depending on the section of level 1
 they are in.
 For example, in the
-[missieven corpus](https://github.com/annotation/app-missieven/blob/master/code/config.yaml)
+[missieven corpus](https://github.com/clariah/wp6-missieven/blob/master/app/config.yaml)
 we have section levels 1=volume, 2=page, 3=line.
 In each volume, the logical page 1 must be translated into a higher
 number, depending on the number of preface pages in that volume.
 
 The value of this parameter is a dict of dicts.
 
 The first level of keys specifies the section level of the sections that needs offsets.
@@ -649,15 +649,15 @@
 
 Conditions that will exclude nodes of this type from the display.
 
 All nodes that satisfy at least one of these conditions will be left out.
 
 !!! hint
     Use this if you want to exclude particular nodes of some type, e.g. in
-    [dss](https://github.com/annotation/app-dss/blob/master/code/config.yaml).
+    [dss](https://github.com/etcbc/dss/blob/master/app/config.yaml).
     where we want to prevent line terminator signs.
 
 The value is a dictionary of feature name - value pairs.
 
 ```
 exclude:
     type: term
@@ -713,15 +713,15 @@
 
 The app needs to define a function
 
 ```
 getGraphics(isPretty, node, nodeType, isOuter) => HTML code for sourcing the graphics
 ```
 
-See [uruk](https://github.com/annotation/app-uruk/blob/master/code/app.py).
+See [uruk](https://github.com/Nino-cunei/uruk/blob/master/app/app.py).
 
 Default
 :   boolean `null`
 
 ---
 
 ### `hidden`
@@ -820,25 +820,25 @@
     (then each child is NOT stretched horizontally).
 
 Default
 :   boolean `true`
 
 !!! hint
     For some types in
-    [uruk](https://github.com/annotation/app-uruk/blob/master/code/config.yaml)
+    [uruk](https://github.com/Nino-cunei/uruk/blob/master/app/config.yaml)
     it is needed to deviate from the default.
 
 ---
 
 ### `style`
 
 Formatting key for plain and pretty displays.
 
 The label or template may need a special style to format it with.
-You pass specify:
+You can specify:
 
 `normal`
 :   normal non-corpus text
 
 `source`
 :   source text of the corpus (before conversion)
 
@@ -890,15 +890,15 @@
 
 Whether this type should be formatted as a verse
 
 The default is:
 `true` for the lowest section type, if there are section types in `otext.tf`.
 
 But more types can be declared as verselike, e.g. `halfverse` in the
-[bhsa](https://github.com/annotation/app-bhsa/blob/master/code/config.yaml).
+[bhsa](https://github.com/etcbc/bhsa/blob/master/app/config.yaml).
 
 ---
 
 ### `wrap`
 
 Pretty: whether the child displays may be wrapped.
 
@@ -907,15 +907,15 @@
 
     *   `true` if the children form a row, such rows may be wrapped
     *   `false` if the children form a column;
         such columns may not be wrapped (into several columns)
 
 !!! hint
     For some types in
-    [uruk](https://github.com/annotation/app-uruk/blob/master/code/config.yaml)
+    [uruk](https://github.com/Nino-cunei/uruk/blob/master/app/config.yaml)
     it is needed to deviate from the default.
 
 ---
 
 ## `writing`
 
 Code for triggering special fonts, see `tf.writing`.
```

## tf/advanced/text.py

```diff
@@ -1,14 +1,14 @@
 import types
 from textwrap import dedent
 from unicodedata import normalize
 
 from ..core.text import DEFAULT_FORMAT
 from ..core.helpers import htmlEsc
-from .helpers import dh
+from .helpers import dh, dm
 
 
 def textApi(app):
     api = app.api
     T = api.T
     F = api.F
     fOtype = F.otype.v
@@ -29,14 +29,39 @@
         formats[fmt] = descendType
         xdTypes[fmt] = descendType
         func = getattr(app, f"fmt_{method}", rescue)
         xFormats[fmt] = func
 
     aContext.allowedValues["textFormat"] = T.formats
     app.specialCharacters = types.MethodType(specialCharacters, app)
+    app.showFormats = types.MethodType(showFormats, app)
+
+
+def showFormats(app):
+    api = app.api
+    T = api.T
+    tFormats = T._tformats
+    xFormats = T._xformats
+
+    md = dedent("""\
+    format | level | template
+    --- | --- | ---
+    """)
+
+    for (fmt, level) in T.formats.items():
+        tpl = (
+            f"`{tFormats[fmt]}`"
+            if fmt in tFormats
+            else f"*function* `{xFormats[fmt].__name__}`"
+            if fmt in xFormats
+            else "*unknown*"
+        )
+        md += f"""`{fmt}` | **{level}** | {tpl}\n"""
+
+    dm(md)
 
 
 def specialCharacters(app, fmt=None, _browse=False):
     """Generate a widget for hard to type characters.
 
     For each text format it is known which characters may occur in the text.
     Some of those characters may be hard to type because they do not belong
```

## tf/advanced/zipdata.py

```diff
@@ -1,14 +1,13 @@
 import os
 import sys
-from shutil import rmtree
 from zipfile import ZipFile
 
 from ..parameters import ZIP_OPTIONS, TEMP_DIR, RELATIVE, GH_BASE, DOWNLOADS
-from ..core.helpers import console, splitModRef, normpath, expanduser
+from ..core.helpers import console, splitModRef, normpath, expanduser, initTree
 
 GH = expanduser(GH_BASE)
 DW = expanduser(DOWNLOADS)
 
 HELP = """
 USAGE
 
@@ -47,18 +46,15 @@
     console(f"Create release data for {org}/{repo}/{relative}")
     sourceBase = normpath(f"{source}/{org}")
     destBase = normpath(f"{dest}/{org}-release")
     sourceDir = f"{sourceBase}/{repo}/{relative}"
     destDir = f"{destBase}/{repo}"
     dataFiles = {}
 
-    if not keep:
-        if os.path.exists(destDir):
-            rmtree(destDir)
-    os.makedirs(destDir, exist_ok=True)
+    initTree(destDir, fresh=not keep)
     relativeDest = relative.replace("/", "-")
 
     if tf:
         if not os.path.exists(sourceDir):
             return
         with os.scandir(sourceDir) as sd:
             versionEntries = [(sourceDir, e.name) for e in sd if e.is_dir()]
```

## tf/convert/mql.py

```diff
@@ -125,15 +125,15 @@
     cleanName,
     isClean,
     specFromRanges,
     rangesFromList,
     setFromSpec,
     nbytes,
     console,
-    normpath,
+    expanduser,
 )
 
 # If a feature, with type string, has less than ENUM_LIMIT values,
 # an enumeration type for it will be created
 # provided all values of that feature are a valid name for MQL.
 
 ENUM_LIMIT = 1000
@@ -141,15 +141,15 @@
 ONE_ENUM_TYPE = True
 
 
 class MQL(object):
     def __init__(self, mqlDir, mqlName, tfFeatures, tmObj):
         error = tmObj.error
 
-        mqlDir = normpath(mqlDir)
+        mqlDir = expanduser(mqlDir)
         self.mqlDir = mqlDir
         cleanDb = cleanName(mqlName)
         if cleanDb != mqlName:
             error(f'db name "{mqlName}" => "{cleanDb}"')
         self.mqlName = cleanDb
         self.tfFeatures = tfFeatures
         self.tmObj = tmObj
@@ -544,15 +544,15 @@
     Parameters
     ----------
     tmObj: object
         A `tf.core.timestamp.Timestamp` object
     mqlFile, slotType, otype, meta: various
         See `tf.core.fabric.Fabric.importMQL
     """
-    mqlFile = normpath(mqlFile)
+    mqlFile = expanduser(mqlFile)
     error = tmObj.error
 
     if slotType is None:
         error("ERROR: no slotType specified")
         return (False, {}, {}, {})
     (good, objectTypes, tables, edgeF, nodeF) = parseMql(mqlFile, tmObj)
     if not good:
```

## tf/convert/recorder.py

```diff
@@ -42,15 +42,15 @@
 A = use(corpus)
 api = A.api
 ```
 
 or by
 
 ```
-from tf,fabric import Fabric
+from tf.fabric import Fabric
 TF = Fabric(locations, modules)
 api = TF.load(features)
 ```
 
 ```
 from tf.convert.recorder import Recorder
 
@@ -99,26 +99,26 @@
 recorder.
 
 You can write the information of a recorder to disk and read it back later.
 
 And you can generate features from a CSV file using the mapped positions.
 
 To see it in action, see this
-[tutorial](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/bhsa/annotate.ipynb)
+[tutorial](https://nbviewer.jupyter.org/github/etcbc/bhsa/blob/master/tutorial/annotate.ipynb)
 """
 
 
 import os
 from itertools import chain
 
 from ..core.helpers import (
     specFromRangesLogical,
     specFromRanges,
     rangesFromSet,
-    normpath,
+    expanduser,
 )
 
 ZWJ = "\u200d"  # zero width joiner
 
 
 class Recorder:
     def __init__(self, api=None):
@@ -288,50 +288,51 @@
                 nodesByPosByType[nodeType].append(value)
 
         info("done")
         indent(level=False)
         return nodesByPosByType
 
     def iPositions(self, byType=False, logical=True, asEntries=False):
-        """Get the node positions as mapping from nodes.
+        """Get the character positions as mapping from nodes.
 
         Parameters
         ----------
         byType: boolean, optional `False`
             If True, makes a separate node mapping per node type.
             For this it is needed that the Recorder has been
             passed a TF api when it was initialized.
         logical: boolean, optional `True`
             If True, specs are represented as tuples of ranges
-            and a range is represented as a tuples of a begin and end point,
-            or as a tuple of a single point.
+            and a range is represented as a tuple of a begin and end point,
+            or as a single point.
             Points are integers.
-            If False, ranges are represented by strings: , separated ranges,
+            If False, ranges are represented by strings: `,` separated ranges,
             a ranges is b-e or p.
         asEntries: boolean, optional `False`
             If True, do not return the dict, but rather its entries.
 
         Returns
         -------
         list|dict|None
             If `byType`, the result is a dictionary, keyed by node type,
             with values the mapping for nodes of that type.
             Entry `n` in this mapping contains the intervals of all
             character positions in the text where node `n` is active.
 
-            If not `byType` then a single mapping is returned.
+            If not `byType` then a single mapping is returned, where each node
+            is mapped to the intervals where that node is active.
         """
 
         method = specFromRangesLogical if logical else specFromRanges
         posByNode = {}
         for (i, nodeSet) in enumerate(self.nodesByPos):
             for node in nodeSet:
                 posByNode.setdefault(node, set()).add(i)
-        for (n, nodeSet) in posByNode.items():
-            posByNode[n] = method(rangesFromSet(nodeSet))
+        for (n, posSet) in posByNode.items():
+            posByNode[n] = method(rangesFromSet(posSet))
 
         if asEntries:
             posByNode = tuple(posByNode.items())
         if not byType:
             return posByNode
 
         api = self.api
@@ -351,15 +352,15 @@
         F = api.F
         Fotypev = F.otype.v
 
         posByNodeType = {}
         if asEntries:
             for (n, spec) in posByNode:
                 nType = Fotypev(n)
-                posByNodeType.setdefault(nType, []).append(n, spec)
+                posByNodeType.setdefault(nType, []).append((n, spec))
         else:
             for (n, spec) in posByNode.items():
                 nType = Fotypev(n)
                 posByNodeType.setdefault(nType, {})[n] = spec
 
         return posByNodeType
 
@@ -367,15 +368,15 @@
         """Get the first textual position for each node
 
         The position information is a big amount of data, in the general case.
         Under certain assumptions we can economize on this data usage.
 
         Strong assumptions:
 
-        1.  every textual position is covered by exactly one node**
+        1.  every textual position is covered by **exactly one node**
         2.  the nodes are consecutive:
             every next node is equal to the previous node plus 1
         3.  the positions of the nodes are monotonous in the nodes, i.e.
             if node n < m, then the position of n is before the position of m.
 
         Imagine the text partitioned in consecutive non-overlapping chunks, where
         each node corresponds to exactly one chunk, and the order of the nodes
@@ -492,104 +493,144 @@
         if not good:
             return (
                 f"{nonConsecutive} nonConsecutive nodes, "
                 f"of which the first one is {nonConsecutiveFirst}"
             )
         return posList
 
-    def write(self, textPath, posPath=None, byType=False, optimize=True):
+    def write(
+        self, textPath, inverted=False, posPath=None, byType=False, optimize=True
+    ):
         """Write the recorder information to disk.
 
-        The recorded text is written as a lain text file,
+        The recorded text is written as a plain text file,
         and the remembered node positions are written as a tsv file.
 
         You can also let the node positions be written out by node type.
         In that case you can also optimize the file size.
 
         Optimization means that consecutive equal values are prepended
         by the number of repetitions and a `*`.
 
         Parameters
         ----------
         textPath: string
             The file path to which the accumulated text is written.
+        inverted: boolean, optional False
+            If False, the positions are taken as mappings from character
+            positions to nodes. If True, they are a mapping from nodes to
+            character positions.
         posPath: string, optional `None`
             The file path to which the mapped positions are written.
-            If absent, it equals `textPath` with `.pos` appended.
+            If absent, it equals `textPath` with `.pos` appended, or
+            `.ipos` if `inverted` is True.
             The file format is: one line for each character position,
             on each line a tab-separated list of active nodes.
         byType: boolean, optional `False`
             If True, writes separate node mappings per node type.
             For this it is needed that the Recorder has been
             passed a TF api when it was initialized.
             The file names are extended with the node type.
             This extension occurs just before the last `.` of the inferred `posPath`.
         optimize: boolean, optional `True`
-            Optimize file size. Only relevant if `byType` is True.
+            Optimize file size. Only relevant if `byType` is True
+            and `inverted` is False.
+            The format of each line is:
+
+            *rep* `*` *nodes`
+
+            where *rep* is a number that indicates repetition and *nodes*
+            is a tab-separated list of node numbers.
+
+            The meaning is that the following *rep* character positions
+            are associated with these *nodes*.
         """
 
-        textPath = normpath(textPath)
-        posPath = normpath(posPath or f"{textPath}.pos")
+        textPath = expanduser(textPath)
+        posExt = ".ipos" if inverted else ".pos"
+        posPath = expanduser(posPath or f"{textPath}{posExt}")
 
         with open(textPath, "w", encoding="utf8") as fh:
             fh.write(self.text())
 
         if not byType:
             with open(posPath, "w", encoding="utf8") as fh:
-                fh.write(
-                    "\n".join(
-                        "\t".join(str(i) for i in nodes) for nodes in self.nodesByPos
+                if inverted:
+                    fh.write(
+                        "\n".join(
+                            f"{node}\t{intervals}"
+                            for (node, intervals) in self.iPositions(
+                                byType=False, logical=False, asEntries=True
+                            )
+                        )
+                    )
+                else:
+                    fh.write(
+                        "\n".join(
+                            "\t".join(str(i) for i in nodes)
+                            for nodes in self.nodesByPos
+                        )
                     )
-                )
             return
 
-        nodesByPosByType = self.positions(byType=True)
-        if nodesByPosByType is None:
+        mapByType = (
+            self.iPositions(byType=True, logical=False, asEntries=True)
+            if inverted
+            else self.positions(byType=True)
+        )
+        if mapByType is None:
             print("No position files written")
             return
 
         (base, ext) = os.path.splitext(posPath)
 
         # if we reach this, there is a TF api
 
         api = self.api
         info = api.TF.info
         indent = api.TF.indent
 
         indent(level=True, reset=True)
 
-        for (nodeType, nodesByPos) in nodesByPosByType.items():
+        for (nodeType, mapping) in mapByType.items():
             fileName = f"{base}-{nodeType}{ext}"
             info(f"{nodeType:<20} => {fileName}")
             with open(fileName, "w", encoding="utf8") as fh:
-                if not optimize:
+                if inverted:
                     fh.write(
                         "\n".join(
-                            "\t".join(str(i) for i in nodes) for nodes in nodesByPos
+                            f"{node}\t{intervals}" for (node, intervals) in mapping
                         )
                     )
                 else:
-                    repetition = 1
-                    previous = None
-
-                    for nodes in nodesByPos:
-                        if nodes == previous:
-                            repetition += 1
-                            continue
-                        else:
-                            if previous is not None:
-                                prefix = f"{repetition}*" if repetition > 1 else ""
-                                value = "\t".join(str(i) for i in previous)
-                                fh.write(f"{prefix}{value}\n")
-                            repetition = 1
-                            previous = nodes
-                    if previous is not None:
-                        prefix = f"{repetition + 1}*" if repetition else ""
-                        value = "\t".join(str(i) for i in previous)
-                        fh.write(f"{prefix}{value}\n")
+                    if not optimize:
+                        fh.write(
+                            "\n".join(
+                                "\t".join(str(i) for i in nodes) for nodes in mapping
+                            )
+                        )
+                    else:
+                        repetition = 1
+                        previous = None
+
+                        for nodes in mapping:
+                            if nodes == previous:
+                                repetition += 1
+                                continue
+                            else:
+                                if previous is not None:
+                                    prefix = f"{repetition}*" if repetition > 1 else ""
+                                    value = "\t".join(str(i) for i in previous)
+                                    fh.write(f"{prefix}{value}\n")
+                                repetition = 1
+                                previous = nodes
+                        if previous is not None:
+                            prefix = f"{repetition + 1}*" if repetition else ""
+                            value = "\t".join(str(i) for i in previous)
+                            fh.write(f"{prefix}{value}\n")
 
         indent(level=False)
 
     def read(self, textPath, posPath=None):
         """Read recorder information from disk.
 
         Parameters
@@ -599,16 +640,16 @@
         posPath: string, optional `None`
             The file path from which the mapped positions are read.
             If absent, it equals `textPath` with `.pos` appended.
             The file format is: one line for each character position,
             on each line a tab-separated list of active nodes.
         """
 
-        textPath = normpath(textPath)
-        posPath = normpath(posPath or f"{textPath}.pos")
+        textPath = expanduser(textPath)
+        posPath = expanduser(posPath or f"{textPath}.pos")
         self.context = {}
 
         with open(textPath, encoding="utf8") as fh:
             self.material = list(fh)
 
         with open(posPath, encoding="utf8") as fh:
             self.nodesByPos = [
@@ -642,15 +683,15 @@
             Path to the annotation file.
         headers: boolean or iterable, optional `True`
             Indicates whether the annotation file has headers.
             If not True, it may be an iterable of names, which will
             be used as headers.
         """
 
-        featurePath = normpath(featurePath)
+        featurePath = expanduser(featurePath)
         nodesByPos = self.nodesByPos
 
         features = {}
 
         with open(featurePath, encoding="utf8") as fh:
             if headers is True:
                 names = next(fh).rstrip("\n").split("\t")[2:]
```

## tf/convert/walker.py

```diff
@@ -443,14 +443,19 @@
             sLevels = {f: len(sectionInfo[f]) for f in sectionInfo}
             if min(sLevels.values()) != max(sLevels.values()):
                 errors["Inconsistent section info"].append(
                     " but ".join(f'"{f}" has {sLevels[f]} levels' for f in sLevels)
                 )
             self.sectionFeatures = sectionInfo["sectionFeatures"]
             self.sectionTypes = sectionInfo["sectionTypes"]
+            self.featFromSectionType = {
+                typ: feat
+                for (typ, feat) in zip(self.sectionTypes, self.sectionFeatures)
+            }
+            self.sectionSet = set(self.sectionTypes)
 
             structureInfo = {}
             for f in ("structureTypes", "structureFeatures"):
                 if f not in otext:
                     structureInfo[f] = []
                     continue
                 sFields = itemize(otext[f], sep=",")
@@ -467,15 +472,15 @@
                 len(info) == 0 for (s, info) in structureInfo.items()
             ):
                 info("No structure nodes will be set up")
                 self.structureFeatures = []
                 self.structureTypes = []
             self.structureFeatures = structureInfo["structureFeatures"]
             self.structureTypes = structureInfo["structureTypes"]
-            self.featFromType = {
+            self.featFromStructureType = {
                 typ: feat
                 for (typ, feat) in zip(self.structureTypes, self.structureFeatures)
             }
             self.structureSet = set(self.structureTypes)
 
             textFormats = {}
             textFeatures = set()
@@ -718,15 +723,17 @@
         self._checkSecLevel(node, before=True)
 
         if slots:
             maxSlot = curSeq[slotType]
 
             for s in slots:
                 if not 1 <= s <= maxSlot:
-                    errors[f'slot out of range in `cv.node(({nType}, {seq}))`'].append(f"{s}")
+                    errors[f"slot out of range in `cv.node(({nType}, {seq}))`"].append(
+                        f"{s}"
+                    )
                 else:
                     oslots = self.oslots
                     oslots[node].add(s)
 
             self.stats[self.T] += 1
         else:
             curEmbedders.add(node)
@@ -1174,15 +1181,16 @@
         self.totalNodes = totalNodes
 
         if curEmbedders:
             embedCount = collections.Counter()
             for (nType, seq) in curEmbedders:
                 embedCount[nType] += 1
             for (nType, amount) in sorted(
-                embedCount.items(), key=lambda x: (-x[1], x[0]),
+                embedCount.items(),
+                key=lambda x: (-x[1], x[0]),
             ):
                 errors["Unterminated nodes"].append(f"{nType}: {amount} x")
 
         self._showErrors()
 
     def _removeUnlinked(self):
         tmObj = self.TF.tmObj
@@ -1272,15 +1280,15 @@
     def _checkFeatures(self):
         tmObj = self.TF.tmObj
         info = tmObj.info
 
         if not self.good and not self.force:
             return
 
-        info("checking features ... ")
+        info("checking (section) features ... ")
 
         intFeatures = self.intFeatures
         metaData = self.metaData
 
         nodes = self.nodes
         nodeFeatures = self.nodeFeatures
         edgeFeatures = self.edgeFeatures
@@ -1290,55 +1298,71 @@
         for feat in intFeatures:
             if (
                 feat not in WARP
                 and feat not in nodeFeatures
                 and feat not in edgeFeatures
             ):
                 errors["intFeatures"].append(
-                    f'"{feat}" is declared as integer valued, but this feature does not occur'
+                    f'"{feat}" is declared as integer valued, '
+                    "but this feature does not occur"
                 )
         for nType in self.sectionTypes:
             if nType not in nodes:
                 errors["sections"].append(
-                    f'node type "{nType}" is declared as a section type, but this node type does not occur'
+                    f'node type "{nType}" is declared as a section type, '
+                    "but this node type does not occur"
                 )
         for feat in self.sectionFeatures:
             if feat not in nodeFeatures:
                 errors["sections"].append(
-                    f'"{feat}" is declared as a section feature, but this node feature does not occur'
+                    f'"{feat}" is declared as a section feature, '
+                    "but this node feature does not occur"
                 )
         for nType in self.structureTypes:
             if nType not in nodes:
                 errors["structure"].append(
                     f'node type "{nType}" is declared as a structure type,'
                     f" but this node type does not occur"
                 )
         for feat in self.structureFeatures:
             if feat not in nodeFeatures:
                 errors["structure"].append(
-                    f'"{feat}" is declared as a structure feature, but this node feature does not occur'
+                    f'"{feat}" is declared as a structure feature, '
+                    "but this node feature does not occur"
                 )
                 nodeFeatures[feat] = {}
 
+        sectionSet = self.sectionSet
         structureSet = self.structureSet
-        featFromType = self.featFromType
+        featFromSectionType = self.featFromSectionType
+        featFromStructureType = self.featFromStructureType
+
         for nType in nodes:
-            if nType not in structureSet:
-                continue
-            feat = featFromType[nType]
-            for seq in nodes[nType]:
-                if (nType, seq) not in nodeFeatures[feat]:
-                    errors["structure features"].append(
-                        f'"structure element "{nType}" {seq} has no value for "{feat}"'
-                    )
+            if nType in structureSet:
+                feat = featFromStructureType[nType]
+                for seq in nodes[nType]:
+                    if (nType, seq) not in nodeFeatures[feat]:
+                        errors["structure features"].append(
+                            f'"structure element "{nType}" {seq} '
+                            f'has no value for "{feat}"'
+                        )
+            if nType in sectionSet:
+                feat = featFromSectionType[nType]
+                for seq in nodes[nType]:
+                    if (nType, seq) not in nodeFeatures[feat]:
+                        errors["section features"].append(
+                            f'"section element "{nType}" {seq} '
+                            f'has no value for "{feat}"'
+                        )
 
         for feat in self.textFeatures:
             if feat not in nodeFeatures:
                 errors["text formats"].append(
-                    f'"{feat}" is used in a text format, but this node feature does not occur'
+                    f'"{feat}" is used in a text format, '
+                    "but this node feature does not occur"
                 )
 
         for feat in WARP:
             if feat in nodeFeatures or feat in edgeFeatures:
                 errors[feat].append(f'Do not construct the "{feat}" feature yourself')
 
         for feat in sorted(nodeFeatures) + sorted(edgeFeatures):
```

## tf/core/fabric.py

```diff
@@ -385,15 +385,15 @@
                     self._loadFeature(fName, optional=fName in formatFeats)
 
                 dep1Feats = self.dep1Feats
                 if dep1Feats:
                     cformats = self.cformats
                     tFormats = {}
                     tFeats = set()
-                    for (fmt, (tpl, featData)) in cformats.items():
+                    for (fmt, (otpl, tpl, featData)) in cformats.items():
                         feats = set(chain.from_iterable(x[0] for x in featData))
                         tFormats[fmt] = tuple(sorted(feats))
                         tFeats |= feats
                     tFeats = tuple(sorted(tFeats))
                     extraDependencies = [tFormats]
                     for tFeat in tFeats:
                         featData = self.features[tFeat].data
```

## tf/core/helpers.py

```diff
@@ -1,10 +1,11 @@
 import os
 import sys
 import re
+from shutil import rmtree
 
 from ..parameters import OMAP
 
 
 def normpath(path):
     return None if path is None else path.replace("\\", "/")
 
@@ -41,27 +42,90 @@
 
 
 def dirEmpty(target):
     target = normpath(target)
     return not os.path.exists(target) or not os.listdir(target)
 
 
+def clearTree(path):
+    """Remove all files from a directory, recursively, but leave subdirs.
+
+    Reason: we want to inspect output in an editor.
+    But if we remove the directories, the editor looses its current directory
+    all the time.
+
+    Parameters
+    ----------
+    path:
+        The directory in question. A leading `~` will be expanded to the user's
+        home directory.
+    """
+
+    subdirs = []
+    path = expanduser(path)
+
+    with os.scandir(path) as dh:
+        for (i, entry) in enumerate(dh):
+            name = entry.name
+            if name.startswith("."):
+                continue
+            if entry.is_file():
+                os.remove(f"{path}/{name}")
+            elif entry.is_dir():
+                subdirs.append(name)
+
+    for subdir in subdirs:
+        clearTree(f"{path}/{subdir}")
+
+
+def initTree(path, fresh=False, gentle=False):
+    """Make sure a directory exists, optionally clean it.
+
+    Parameters
+    ----------
+    path:
+        The directory in question. A leading `~` will be expanded to the user's
+        home directory.
+
+        If the directory does not exist, it will be created.
+
+    fresh: boolean, optional False
+        If True, existing contents will be removed, more or less gently.
+
+    gentle: boolean, optional False
+        When existing content is removed, only files are recursively removed, not
+        subdirectories.
+    """
+
+    path = expanduser(path)
+    exists = os.path.exists(path)
+    if fresh:
+        if exists:
+            if gentle:
+                clearTree(path)
+            else:
+                rmtree(path)
+
+    if not exists or fresh:
+        os.makedirs(path, exist_ok=True)
+
+
 LETTER = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ")
 VALID = set("_0123456789") | LETTER
 
 WARN32 = """WARNING: you are not running a 64-bit implementation of Python.
 You may run into memory problems if you load a big data set.
 Consider installing a 64-bit Python.
 """
 
 MSG64 = """Running on 64-bit Python"""
 
 SEP_RE = re.compile(r"[\n\t ,]+")
 STRIP_RE = re.compile(r"(?:^[\n\t ,]+)|(?:[\n\t ,]+$)", re.S)
-VAR_RE = re.compile(r"\{([^}]+?)(:[^}]+)?\}")
+VAR_RE = re.compile(r"\{([^}]+?)(:[^}]*)?\}")
 MSG_LINE_RE = re.compile(r"^( *[0-9]+) (.*)$")
 
 QUAD = "  "
 
 
 def isInt(val):
     try:
@@ -336,15 +400,15 @@
             fts = tuple(varText.split("/"))
             features.append((fts, default))
             for ft in fts:
                 featureSet.add(ft)
             return "{}"
 
         rtpl = VAR_RE.sub(varReplace, tpl)
-        return (rtpl, tuple(features))
+        return (tpl, rtpl, tuple(features))
 
     formats = {}
     for (fmt, tpl) in sorted(config.items()):
         if fmt.startswith("fmt:"):
             formats[fmt[4:]] = collectFormat(tpl)
     return (formats, sorted(featureSet))
```

## tf/core/nodes.py

```diff
@@ -216,28 +216,70 @@
         """
 
         api = self.api
 
         Crank = api.C.rank.data
         return sorted(nodeSet, key=lambda n: Crank[n - 1])
 
-    def walk(self):
+    def walk(self, events=False):
         """Generates all nodes in the *canonical order*.
         (`tf.core.nodes`)
 
         By `walk()` you traverse all nodes of your corpus
         in a very natural order. See `tf.core.nodes`.
 
+        The order is much like walking a tree in pre-order: first parents,
+        then children from left to right.
+
+        The thing is: in general the nodes do not form a tree, but a more
+        liberal structure: a graph.
+
+        But even then we can order the nodes in such a way that nodes that embed
+        slots from other nodes come before those other nodes, provided those other
+        nodes start later.
+
+        When we generate those nodes and consume them, we now when each node starts,
+        but we loose track of where exactly they end.
+
+        To remedy that, you can call this function with `events=True`.
+        In that case, a stream of events is generated, where each event has the
+        form `(node, False)` or `(node, True)`, where `False` means: beginning of
+        node and `True` means: end of node.
+
+        In case of slot nodes, only one event per slot is generated: `(node, None)`.
+
         !!! hint "More ways of walking"
             Under `tf.core.nodefeature.NodeFeatures` there is another convenient way
             to walk through subsets of nodes.
 
+        Parameters
+        ----------
+        events: boolean, optional `False`
+            If True, wraps the generated nodes in event tuples as described above.
+
         Returns
         -------
         nodes: int
             One at a time.
         """
 
         api = self.api
 
-        for n in api.C.order.data:
-            yield n
+        if events:
+            C = api.C
+            endSlots = C.boundary.data[1]
+
+            otype = api.F.otype
+            Fotypev = otype.v
+            slotType = otype.slotType
+
+            for n in C.order.data:
+                if Fotypev(n) == slotType:
+                    yield (n, None)
+                    for m in reversed(endSlots[n - 1]):
+                        yield(m, True)
+                else:
+                    yield(n, False)
+
+        else:
+            for n in api.C.order.data:
+                yield n
```

## tf/core/text.py

```diff
@@ -178,24 +178,46 @@
 but if there is also a rare feature `special` that you want to use if it
 is defined for that word, you can make a format
 
 ```
 {special/normal}
 ```
 
-This tries the feature `special` first, and if that is empty, it takes
+This tries the feature `special` first, and if that is undefined, it takes
 `normal`.
 
+!!! caution "undefined versus empty"
+    The criterion to skip the value of feature `special` and use the value
+    of feature `normal` is that `special` either has no value, or its value is
+    `None` (Text-Fabric essentially makes no difference between the two).
+    But if the value of `special` happens to be the empty string, it will be used!
+
+!!! hint "longer chains"
+    You can chain multiple features with `/`, as many as you want:
+
+    ```
+    {veryspecial/special/often/normal}
+    ```
+
 You can also add a fixed default. If you want to display a `.` if
 neither `special` nor `normal` exist, you can say
 
 ```
 {special/normal:.}
 ```
 
+You can also specify the empty string as the default:
+
+```
+{special/normal:}
+```
+
+However, you do not need to do that, because the default is the empty string
+by default!
+
 TF datasets may also define formats of the form
 
 *nodetype*`-default`
 
 where *nodetype* is a valid type of node in the dataset.
 
 These formats will be invoked in cases where no explicit format is specified as
@@ -380,15 +402,17 @@
         (
             self.hdFromNd,
             self.ndFromHd,
             self.hdMult,
             self.hdTop,
             self.hdUp,
             self.hdDown,
-        ) = (structure.data if structure else (None, None, None, None, None, None))
+        ) = (
+            structure.data if structure else (None, None, None, None, None, None)
+        )
         self.headings = (
             ()
             if structure is None
             else tuple(zip(self.structureTypes, self.structureFeats))
         )
         otypeInfo = api.F.otype
         fOtype = otypeInfo.v
@@ -1119,17 +1143,15 @@
                 repf = func
                 if explain:
                     fmtRep += f" (overridden with the explicit func argument {repf})"
             if not repf:
                 repf = rescue
                 good = False
                 if explain:
-                    fmtRep += (
-                        "\n\t\t\twhich is not defined: formatting as node types and numbers"
-                    )
+                    fmtRep += "\n\t\t\twhich is not defined: formatting as node types and numbers"
 
             if explain:
                 error(f"\t\tFORMATTING: {fmtRep}", tm=False)
                 error("\t\tMATERIAL:", tm=False)
             for n in xnodes:
                 rep = repf(n, **kwargs)
                 material.append(rep)
@@ -1155,25 +1177,28 @@
 
     def _compileFormats(self):
         api = self.api
         TF = api.TF
         cformats = TF.cformats
 
         self.formats = {}
+        self._tformats = {}
         self._xformats = {}
         self._xdTypes = {}
-        for (fmt, (rtpl, feats)) in sorted(cformats.items()):
+        for (fmt, (otpl, rtpl, feats)) in sorted(cformats.items()):
             defaultType = self.splitDefaultFormat(fmt)
             if defaultType:
                 self.defaultFormats[defaultType] = fmt
             (descendType, rtpl) = self.splitFormat(rtpl)
+            (dummy, otpl) = self.splitFormat(otpl)
             tpl = rtpl.replace("\\n", "\n").replace("\\t", "\t")
             self._xdTypes[fmt] = descendType
             self._xformats[fmt] = self._compileFormat(tpl, feats)
             self.formats[fmt] = descendType
+            self._tformats[fmt] = otpl
 
     def splitFormat(self, tpl):
         api = self.api
         F = api.F
         slotType = F.otype.slotType
         otypes = set(F.otype.all)
```

## tf/dataset/__init__.py

```diff
@@ -7,14 +7,14 @@
 
 * Modify, see `tf.dataset.modify`,
   (add/merge/delete types and features to/from a single data source)
 * Node maps, see `tf.dataset.nodemaps`,
   (make node mappings between versions of TF data)
 
 See also the
-[dataset chapter](https://nbviewer.jupyter.org/github/annotation/tutorials/blob/master/banks/compose.ipynb)
+[dataset chapter](https://nbviewer.jupyter.org/github/annotation/banks/blob/master/tutorial/compose.ipynb)
 in the Banks tutorial.
 """
 
 
 from .modify import modify
 from .nodemaps import Versions
```

## tf/dataset/modify.py

```diff
@@ -460,15 +460,15 @@
 
         ensureTypes = set()
         ensureFeatures = set()
         for kind in (SE_TP, ST_TP):
             ensureTypes |= set(itemize(combi.get(kind, ""), sep=","))
         for kind in (SE_FT, ST_FT):
             ensureFeatures |= set(itemize(combi.get(kind, ""), sep=","))
-        ensureFeatures |= set(collectFormats(combi)[1])
+        ensureFeatures |= set(collectFormats(combi)[-1])
         return (ensureTypes, ensureFeatures)
 
     def allInt(values):
         return all(isInt(v) for v in values)
 
     def prepare():
         nonlocal api
```

## tf/dataset/nodemaps.py

```diff
@@ -117,15 +117,15 @@
     these are nodes for which no match could be found.
 
 
 An involved example of creating node mappings between versions (not using this code) is
 [versionMappings.ipynb](https://nbviewer.jupyter.org/github/ETCBC/bhsa/blob/master/programs/versionMappings.ipynb).
 
 A simpler example, using this code is
-[map.ipynb](https://nbviewer.jupyter.org/github/Dans-labs/clariah-gm/blob/master/programs/map.ipynb).
+[map.ipynb](https://nbviewer.jupyter.org/github/clariah/wp6-missieven/blob/master/programs/map.ipynb).
 
 ## Usage
 
 ```python
 from tf.dataset import Versions
 
 V = Versions(api, va, vb, slotMap)
```

## tf/search/syntax.py

```diff
@@ -219,16 +219,17 @@
 
         (ES, ET, EA, EI, EP, EK) = (False,) * 6
 
         if UBO:
             ES = lineQuKind not in QINIT
             ET = len(tokens) == 0
             lastAtom = lastAtomToken()
-            EA = len(tokens) and not lastAtomToken
-            EI = len(tokens) and lastAtom["indent"] != lineIndent
+            EA = len(tokens) and not lastAtom
+            # EA = len(tokens) and not lastAtomToken
+            EI = len(tokens) and lastAtom and lastAtom["indent"] != lineIndent
             # EA = (len(tokens) and tokens[-1]['kind'] != 'atom' and 'otype' not in tokens[-1])
             # EI = (len(tokens) and tokens[-1]['indent'] != lineIndent)
 
         if PCO or PCI:
             EP = (lineQuKind == QHAVE and curQuKind != QWHERE) or (
                 lineQuKind == QOR and curQuKind not in {QWITH, QOR}
             )
@@ -265,15 +266,18 @@
                     searchExe.badSyntax.append(
                         (
                             i,
                             "Quantifier: Does not immediately follow an atom at the same level",
                         )
                     )
                     good = False
-                prevAtom = tokens[-1]
+                if not good:
+                    continue
+                # prevAtom = tokens[-1]
+                prevAtom = lastAtomToken()
                 curQu.append((i, lineQuKind, lineIndent))
                 curQuTemplates = [[]]
                 quantifiers = prevAtom.setdefault("quantifiers", [])
                 quantifiers.append((lineQuKind, curQuTemplates, i))
                 continue
             if PBI:
                 # start inner quantifier
```

## Comparing `text_fabric-9.4.4.dist-info/LICENSE` & `text_fabric-9.5.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `text_fabric-9.4.4.dist-info/METADATA` & `text_fabric-9.5.2.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text-fabric
-Version: 9.4.4
+Version: 9.5.2
 Summary: Processor and browser for Text Fabric Data
 Home-page: https://github.com/annotation/text-fabric
 Author: Dirk Roorda
 Author-email: text.annotation@icloud.com
 License: UNKNOWN
 Keywords: text,linguistics,database,graph,hebrew,bible,peshitta,quran,cuneiform,uruk,greek,syriac,akkadian,babylonian
 Platform: UNKNOWN
```

## Comparing `text_fabric-9.4.4.dist-info/RECORD` & `text_fabric-9.5.2.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 tf/__init__.py,sha256=ZqEJwz25p4SfTR3kSpqLZh7Glkxn_gpJPLoU_kWjrMU,38
 tf/app.py,sha256=tmgWx00MuTAuAJZNFKEZHqdCfe2u4f9ASWyxT5uEWnk,1652
 tf/cheatsheet.py,sha256=2TAIa_rsb03pre9zs5be6Baiv2P0hjeSrB1DjjINGno,45
 tf/clean.py,sha256=UiqG73RRVugyzOcNtYNsIEP9Q1BXR7iXGzekjv3dRic,3773
-tf/fabric.py,sha256=7IhTnYOiAtrxwoJryeMKJ7UATznqXawSN7XtUzbP4pw,7392
+tf/fabric.py,sha256=xRwMCKDORInipP6rQ99iJcbNjWnzuavzZsUzpAq4Dgs,7384
 tf/lib.py,sha256=EMeinN1JOKdmsXUZm0iPmkt0dd2slbkOJMxriEFgRpY,2034
-tf/parameters.py,sha256=C2HLvqwTZSet3Zc370yFQHiqTOxkw4bRihE9H01wvEI,7069
+tf/parameters.py,sha256=yfO4gqeGET3GX4uYZ6FOxj-9etnW2jRXmV_pjoHyp9o,7069
 tf/about/__init__.py,sha256=wwRoPvaYuB19FZBK7RFd1zEBzzf0tXzHHSgN_6UrNY8,49
 tf/about/apps.py,sha256=ZIH-j7qCnshsr0CaVaUbQdhayBsyrLH339NvPW4OUYg,43
 tf/about/background.py,sha256=F11RV95BA1ZENd2j2JiRgwu6QJoHgE-mMnCmamTKwJE,49
 tf/about/browser.py,sha256=tXvI_jZGISKLGyTsjZ1XpRHyJEFdnS5t7Aun3DfDXzo,46
 tf/about/clientmanual.py,sha256=s7p2Mn7PAk8CI1mQ7TiE6o3MV5Pb-Swj9M1uA83JrwM,51
 tf/about/code.py,sha256=-SXt2DgiIH7BivSMwkW2R7g-65NJi1aIZkpOheWVG4s,43
 tf/about/corpora.py,sha256=KERpYLoxImPhFsEXr6OZi8ZZhCnzlx9NYVz65W3kuiI,46
@@ -28,30 +28,30 @@
 tf/about/use.py,sha256=87eRIFyiR_ejjEJ4wT3iAYgwn_FWIeASGeXcBeUNF4s,42
 tf/about/usefunc.py,sha256=KhbIeAhz_uTuHvlTDWWl6j6LmGhmC4phRNx6Bd_YIeA,46
 tf/about/volumes.py,sha256=Yw61f2lpXxwKR7pGCj-1WnxFbJj_7Nw6MFcRJrcGTnA,46
 tf/advanced/__init__.py,sha256=Rb-xUmQWHNaq2FqJivj8uoQZ06pKbd-YqMPYQgSt28Y,1702
 tf/advanced/app.py,sha256=Q6SBD0wm3JPnElBA_2Q104gnjGZyxSp9-zylhzVetHU,16592
 tf/advanced/condense.py,sha256=HoD5IRRA2pInpCYL6i0Cx4cwUH3EhkQA_KIlnvHeLxk,2545
 tf/advanced/data.py,sha256=oFO6yE9XJNUNpcj-jabUuhkpcTe4tIVD_xNvAk_Q7xM,9658
-tf/advanced/display.py,sha256=5FnqFaF3B5znCtf5eKKlGs99eCffcVSmdWfrdOZxvs4,29859
+tf/advanced/display.py,sha256=vrzfdgPS0GcTnVbKIDbiKUpMWUs_WeghP_YW1SuVo4M,29871
 tf/advanced/find.py,sha256=1KppAeAraYLbG-Z6DbhQWPM-ptBb9uIHG5IOrDW1uBg,5363
 tf/advanced/helpers.py,sha256=O2JiZTBo7klQbC4s0KBpZpNn77HGnRwmRHxuwiQJYCY,15956
 tf/advanced/highlight.py,sha256=ncEPROSkgIkrktvJhkuEcEO8I6Nw1x-2aZVJy71aTPg,4471
 tf/advanced/links.py,sha256=EN_au1mbm8tBNWNHfEdzjCwP926OUTxffeezGzUJglM,19819
 tf/advanced/options.py,sha256=rarZW_VfK0TVcoJG3cVffpiquSJx0kvmZqVSu_1_MR4,20509
 tf/advanced/render.py,sha256=G_gujua9gEdsfRsjfhS4I4aMHZdWnVxPJkiiEitf1Ng,15313
-tf/advanced/repo.py,sha256=hJ4Exp-3iLaB4gyL1WJhglg32PInPkx2VD363kORXSM,39950
+tf/advanced/repo.py,sha256=UjCkj59N42vBOxjQZjBfbKXR1rWTc8HH4F1axN2R8PI,39352
 tf/advanced/search.py,sha256=i0DeDvVpS2jUCluHF9ayIggQV139xvkWnX1ycUZNskI,7363
 tf/advanced/sections.py,sha256=3K-VnNOY86fdPERQEK61VOvaQS_-bJfDrcqM8wgK0Ug,5464
-tf/advanced/settings.py,sha256=YRFHP5enaEyPEP5SM5dVkcNzZO0M6hY2SjVOU4HbQr8,49028
+tf/advanced/settings.py,sha256=MaX1Bcd0aUe-F3nbAzunqNa0H9FMEo1vsKcdqWAdGr0,48973
 tf/advanced/tables.py,sha256=Uz0Z_pUl7m7E72VVe7XDTxWhbWDYWnzVetIud-rA0MM,8432
-tf/advanced/text.py,sha256=CBeCSJi7X8Z3vtzT0tmxVKLsMmIyRLEjXOlFGv986gY,2828
+tf/advanced/text.py,sha256=-X3v6Doyc95Dsld1LBOofbc5_iI_lF_TbjaaQ_Gc8Bw,3397
 tf/advanced/unravel.py,sha256=cd0tythI5YOt_NgkW0kvbeAkcgl26atN_3zv28yqn8E,19582
 tf/advanced/volumes.py,sha256=LEhyOIUryDVuoQQMl5CNFsHVauEZDHTB9HpP7uMIevM,1436
-tf/advanced/zipdata.py,sha256=rWpNA0kLly3XqcXyGDiljWIHQjqVPb5RsNxz9LHyDW0,5842
+tf/advanced/zipdata.py,sha256=gXSQHto9iA0uZXhKrxYB0LBb6iytIfb2B9O1fkK2fHk,5743
 tf/client/__init__.py,sha256=y1yY8pPqTuEgADJIrH_ek2ymagklirTc6CNNQMmwVTs,944
 tf/client/make/__init__.py,sha256=vhuNgqfKrXfnfthPKiXhXodC7-nqDcfZYy1ghSaccRQ,36
 tf/client/make/build-notebook.ipynb,sha256=zkpF5rkrpQewFDPJLNnp4XH_XsxsfoacV93SYWn0PwE,215294
 tf/client/make/build-notebook.py,sha256=DO9P4pkHP_gYWx_UHK-TYM5S2qHK7aNJFYFr90cKefM,5418
 tf/client/make/build.py,sha256=bcPuGdgoky_z2oYvuN1-TzSPYmDgH1RrgrQ71B5qagY,43932
 tf/client/make/config.yaml,sha256=BfFGHin85KxN9MuldEpPDqXGPaVLHTvDsbyWiBbNeWI,3419
 tf/client/make/gh.py,sha256=xVFWQFeJJjJ_XXCgf2MofySW8QmPORiWl-sNNfFo7G8,4378
@@ -79,45 +79,45 @@
 tf/client/static/jslib/.DS_Store,sha256=1lFlJ5EFymdzGAUAaI30vcaaLHt3F1LwpG7xILf9jsM,6148
 tf/client/static/jslib/jquery.js,sha256=_xUj-3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej_m4,89501
 tf/client/static/png/.DS_Store,sha256=1lFlJ5EFymdzGAUAaI30vcaaLHt3F1LwpG7xILf9jsM,6148
 tf/client/static/png/github.png,sha256=j-jkDSH-4UDV62JQCGP97Q5wiUyUfjDZMNo9GJ9s1hc,13001
 tf/client/static/png/maker.png,sha256=SPTP7ECbCgH-h2cCSSkCmGn0iXN-xe7rWJywY216bvM,43381
 tf/client/static/png/tf.png,sha256=1F1_6erQ8oski7CSox2fs9dxGsCsBAdmBlNrckZpcCI,149670
 tf/convert/__init__.py,sha256=B6diJmlVhcjfeeEioaJhPycNMjgQkvKOFm31peakSGw,413
-tf/convert/mql.py,sha256=gXCp0MCrhNSaADZ29LHjrEaQbkSdiiyfo66bjQVH1Zo,27615
-tf/convert/recorder.py,sha256=GP5JFsdfijpuKsQAQWjnGKgAmmOI0hpSex-TQqPq9pQ,22143
+tf/convert/mql.py,sha256=_WNvS7YSy8jJ2KnUkP4dnMoDTUX_HDwkfA6k-LalUiY,27621
+tf/convert/recorder.py,sha256=rKJzkXn1I3zvHZdIujna3TEVkHjnKJx7l_7vndlxHtY,23813
 tf/convert/tf.py,sha256=Lcrn3EJtVqKKaowFImNcPlTSDkhx1dM4vGTcaB9UyFQ,9848
-tf/convert/walker.py,sha256=z5sm-VzLyd8qMeBxPOIwnGidVeOJSsfcf15aE8DtViM,50211
+tf/convert/walker.py,sha256=Lk6O2AQj-NRAysbXs4-smsFJ-yn42_xxOaWjw7cjk4I,51165
 tf/core/__init__.py,sha256=1BAMi95DO1dVvbOP9v4pE7T63i08MZfOiWaz_DJS5Sc,515
 tf/core/api.py,sha256=CuuPF6D12ETYyIfJxo_ETJBI-yZrEYcHm2bUD_eGYeQ,16108
 tf/core/computed.py,sha256=fwCxny-5b-_laKK7W7dlIEEw5eXI_Oz4XMvNkwxMTW8,1530
 tf/core/data.py,sha256=yo9nu9A2OLVVYMIuOzdZ1IVKFq-X8nLmMQ1ViCfnH88,24844
 tf/core/edgefeature.py,sha256=xTo4U5uKxxymlG2M8BvU5BPBvT06rEu0utRcQaUNOIs,9793
-tf/core/fabric.py,sha256=JXs25SByNPymxA5M_n10n3xTmIkByI0i2SwjRQYLdRI,42143
-tf/core/helpers.py,sha256=yOAryhnDvB2aadUZ653vF4tBt0TLHX26c6GP4RWhcmQ,12218
+tf/core/fabric.py,sha256=BZSSX83_Jj7cTYMQaItCAchtZgWZZ6GFHHrkBlyw8p4,42149
+tf/core/helpers.py,sha256=YZ8XVvepIICKhS11URMX2_i0Ht_ceUUNwkbmlbedCRs,13915
 tf/core/locality.py,sha256=VuyWwhQMN8mqp1BHTrVz-RpXdKpKFTm8fELwbG_owjE,9691
 tf/core/nodefeature.py,sha256=8dPFSPfkQtveFJtjTSo-bhnCdnqYZX_65TPpfRl7ViU,3573
-tf/core/nodes.py,sha256=D60gFnTa-Sn-aC5lFuC_Z_M8EwXC67B_BucpZ4fyUSY,7893
+tf/core/nodes.py,sha256=iKAPKHdlBPjbgLanZ5t0LU1c79BShLIftW0o70u1ld4,9471
 tf/core/oslotsfeature.py,sha256=MytjfFOlnFJUuhVe9cJisqg0Ix2U9CGQn0zu-fiDGIo,1588
 tf/core/otypefeature.py,sha256=v6YYo9QCmT0kDdi65JRcXvNIrT-ETmRhuozZqRGyYXg,3221
 tf/core/prepare.py,sha256=HjuH4pjFAc7agL7FZ4UjD5BGXw_WWUIrAWGYZOsmrwE,24169
-tf/core/text.py,sha256=8AVX-hjDdyLFVlXqDLrBBBYZpEnLa2xwh_U7RF5Qf5c,44463
+tf/core/text.py,sha256=_5FKTrlGn-PqD1UVPTxLBoP3TTRZyUOvSI1qiJ09d4c,45235
 tf/core/timestamp.py,sha256=ULN49eGhGT2Yoi8qqzKgnUUdB3US_IZshARDTjyXM4M,7753
-tf/dataset/__init__.py,sha256=Ky_YfF5VHgmz93NSFxmFT5783CamLqkP2vYDR-q-lvs,543
-tf/dataset/modify.py,sha256=cb7qHw2ptcAYKTJ5i83tBnfxKOCcNxBB9lYMaDB8i1M,34376
-tf/dataset/nodemaps.py,sha256=2kU2NrrvSrjtD-1jQDtfWgoe8cQiHuEMdGkbN1q-Pas,24837
+tf/dataset/__init__.py,sha256=hGSatLe5YYYR34KRHDXvCHhZEOvvZsSJluo5K3rRmOo,542
+tf/dataset/modify.py,sha256=aj6MJOc3orYTQquFRUbnxAxqtvqwCFOyxHMAogneiw8,34377
+tf/dataset/nodemaps.py,sha256=mcykLq_7yPforumaVx-xUn5hklEvxCBeHa-c5oj9eJc,24838
 tf/search/__init__.py,sha256=Dn1ogZZyX9wKL8nq3z1HwCOV4T881r1AlOBEk9NlKvw,173
 tf/search/graph.py,sha256=3IQO38u4wXZuesBxm7QZRUQIMdLeZsc2-440d5YVYWs,7230
 tf/search/relations.py,sha256=51adFTu9CI1NIXfLc2X9ZiD9E5ljxcCP5VM6cs1mF7s,53131
 tf/search/search.py,sha256=nS34vk1uJ81JM974GKOu-_YORoDvUR7yimlzpwzlZQ8,19745
 tf/search/searchexe.py,sha256=XcehqJHvQ9WG1Zq82BFneIxHFFk7XpPUPq0eRR2HD_Y,6482
 tf/search/semantics.py,sha256=AI167at-qxV27eVH8_lCNqmWQxYLt0MogNS2i8yqzuU,18880
 tf/search/spin.py,sha256=kwUKu7faU8hN_69hSGTQcUyl8hlxxGGaXcLWLpiqcuo,14005
 tf/search/stitch.py,sha256=-hB7Bm-x8xsDKIXdCsscuL1gN5LDdCntPEUq5GH-AAg,28344
-tf/search/syntax.py,sha256=sMNycNCWjbZ1kqh7xRg0UiP1zSManUYPn34QGDDLxd0,22260
+tf/search/syntax.py,sha256=apE_jKfmver-JNg9zBY3mIjyWXpdBEd-H0w4kVdkK38,22424
 tf/server/__init__.py,sha256=wPvFcAXOmmD77_5d_Dpwunm_jb3gi5aiV_lFCewvKnI,39
 tf/server/command.py,sha256=89-fW1Qx0i24R7JkBUPzTcxlQ-AjJK4lTJSFKwfzy3s,4032
 tf/server/kernel.py,sha256=DgU4ecbkCCQi-6J-zDBbKVIaqDunXXKDcXw4NNEmMys,22264
 tf/server/monitor.py,sha256=LPSUYF_Cw6N-ooPR4tHvbwv5SR7j0Y0SLvucLIpoQzE,990
 tf/server/serve.py,sha256=SAy2Aw-c8_mDqwQXedWCCycHiRMPqFcALS9OImI1SDg,11568
 tf/server/servelib.py,sha256=u5xHumZoreH2mbKp33pnp-2MPu4IySjVhyaq_Cer08s,4974
 tf/server/start.py,sha256=z0uW3rsuvLLl_r0dVKZSLlaynqNfTr0KZkYrA5jgmXs,12199
@@ -184,13 +184,13 @@
 tf/writing/__init__.py,sha256=SOp4e675eYxoHoRlV6LfL0DUeGcislJkrNN3Aa108UU,462
 tf/writing/arabic.py,sha256=OH-gDTq6u3x4HR9gO23WEnONwAlc4dpY-M6WVs3O8uA,47
 tf/writing/greek.py,sha256=jIE2X0DhnvUpWtVlkWT3OGipwJmraNiE_RFCjmpeN4A,46
 tf/writing/hebrew.py,sha256=9OL6ehQSAZPv6UAxlYhRLNKnfMKKjAkeYbpJuJdfAlU,47
 tf/writing/neoaramaic.py,sha256=mCDCh5jcEOyUK9hJ010wR6yVPqH1BGpWYtlBt-aJ2wI,51
 tf/writing/syriac.py,sha256=lgKif8b-xCTJvG-K42Sl9bpxOa6m343ytBTKKCP3LGg,47
 tf/writing/transcription.py,sha256=4U3zzqy65Pk7ZrIOsXznA-NvQAxu8Cu4IDB0LnGymwk,34866
-text_fabric-9.4.4.dist-info/LICENSE,sha256=laODeDHbqbmdt5HPawJJlbWGVeAoDk0G7h7XVUCiKsc,1068
-text_fabric-9.4.4.dist-info/METADATA,sha256=2KS7z0HQfBYKm9lxUMzMB0i-g1LtCfHjvLpBX5-PG28,1931
-text_fabric-9.4.4.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-text_fabric-9.4.4.dist-info/entry_points.txt,sha256=1HMSjJbqXxuNvM2Gi5EDF5pPhm0k_EfnlPzlDBStYPU,142
-text_fabric-9.4.4.dist-info/top_level.txt,sha256=EPXLV1KzWPAb0jt752aiQBLx1vUBJSt_Pp3aO-sqKmI,3
-text_fabric-9.4.4.dist-info/RECORD,,
+text_fabric-9.5.2.dist-info/LICENSE,sha256=laODeDHbqbmdt5HPawJJlbWGVeAoDk0G7h7XVUCiKsc,1068
+text_fabric-9.5.2.dist-info/METADATA,sha256=ECiIegf4fTpHmeT0zivlcgU2YgVPx4pBx-3jJo3oEEI,1931
+text_fabric-9.5.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+text_fabric-9.5.2.dist-info/entry_points.txt,sha256=1HMSjJbqXxuNvM2Gi5EDF5pPhm0k_EfnlPzlDBStYPU,142
+text_fabric-9.5.2.dist-info/top_level.txt,sha256=EPXLV1KzWPAb0jt752aiQBLx1vUBJSt_Pp3aO-sqKmI,3
+text_fabric-9.5.2.dist-info/RECORD,,
```

