# Comparing `tmp/tptbox-0.1.1-py3-none-any.whl.zip` & `tmp/tptbox-0.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,62 +1,55 @@
-Zip file size: 1141866 bytes, number of entries: 60
--rwxr-xr-x  2.0 unx     1119 b- defN 80-Jan-01 00:00 TPTBox/__init__.py
+Zip file size: 1120052 bytes, number of entries: 53
+-rwxr-xr-x  2.0 unx     1059 b- defN 80-Jan-01 00:00 TPTBox/__init__.py
 -rwxr-xr-x  2.0 unx      619 b- defN 80-Jan-01 00:00 TPTBox/core/__init__.py
--rwxr-xr-x  2.0 unx     3972 b- defN 80-Jan-01 00:00 TPTBox/core/bids_constants.py
--rwxr-xr-x  2.0 unx    55801 b- defN 80-Jan-01 00:00 TPTBox/core/bids_files.py
--rwxr-xr-x  2.0 unx    64166 b- defN 80-Jan-01 00:00 TPTBox/core/nii_wrapper.py
--rwxr-xr-x  2.0 unx     9151 b- defN 80-Jan-01 00:00 TPTBox/core/nii_wrapper_math.py
+-rwxr-xr-x  2.0 unx     4561 b- defN 80-Jan-01 00:00 TPTBox/core/bids_constants.py
+-rwxr-xr-x  2.0 unx    56726 b- defN 80-Jan-01 00:00 TPTBox/core/bids_files.py
+-rwxr-xr-x  2.0 unx    66755 b- defN 80-Jan-01 00:00 TPTBox/core/nii_wrapper.py
+-rwxr-xr-x  2.0 unx     9168 b- defN 80-Jan-01 00:00 TPTBox/core/nii_wrapper_math.py
 -rwxr-xr-x  2.0 unx    37507 b- defN 80-Jan-01 00:00 TPTBox/core/np_utils.py
--rwxr-xr-x  2.0 unx    69550 b- defN 80-Jan-01 00:00 TPTBox/core/poi.py
--rwxr-xr-x  2.0 unx    28761 b- defN 80-Jan-01 00:00 TPTBox/core/poi_abstract.py
+-rwxr-xr-x  2.0 unx    69554 b- defN 80-Jan-01 00:00 TPTBox/core/poi.py
+-rwxr-xr-x  2.0 unx    28790 b- defN 80-Jan-01 00:00 TPTBox/core/poi_abstract.py
 -rwxr-xr-x  2.0 unx     2907 b- defN 80-Jan-01 00:00 TPTBox/core/poi_global.py
 -rwxr-xr-x  2.0 unx    18197 b- defN 80-Jan-01 00:00 TPTBox/core/sitk_utils.py
--rwxr-xr-x  2.0 unx     7861 b- defN 80-Jan-01 00:00 TPTBox/core/vert_constants.py
--rwxr-xr-x  2.0 unx    47636 b- defN 80-Jan-01 00:00 TPTBox/core/vertebra_pois_non_centroids.py
--rwxr-xr-x  2.0 unx      207 b- defN 80-Jan-01 00:00 TPTBox/docker/__init__.py
--rwxr-xr-x  2.0 unx    36551 b- defN 80-Jan-01 00:00 TPTBox/docker/docker.py
--rwxr-xr-x  2.0 unx     1392 b- defN 80-Jan-01 00:00 TPTBox/docker/docker_run.py
+-rwxr-xr-x  2.0 unx     7885 b- defN 80-Jan-01 00:00 TPTBox/core/vert_constants.py
+-rwxr-xr-x  2.0 unx    47849 b- defN 80-Jan-01 00:00 TPTBox/core/vertebra_pois_non_centroids.py
 -rwxr-xr-x  2.0 unx      165 b- defN 80-Jan-01 00:00 TPTBox/logger/__init__.py
 -rwxr-xr-x  2.0 unx     5063 b- defN 80-Jan-01 00:00 TPTBox/logger/log_constants.py
--rwxr-xr-x  2.0 unx    15482 b- defN 80-Jan-01 00:00 TPTBox/logger/log_file.py
--rw-r--r--  2.0 unx     4836 b- defN 80-Jan-01 00:00 TPTBox/mesh3D/mesh.py
--rwxr-xr-x  2.0 unx     4633 b- defN 80-Jan-01 00:00 TPTBox/mesh3D/mesh_colors.py
+-rwxr-xr-x  2.0 unx    18159 b- defN 80-Jan-01 00:00 TPTBox/logger/log_file.py
+-rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 TPTBox/mesh3D/__init__.py
+-rw-r--r--  2.0 unx     4802 b- defN 80-Jan-01 00:00 TPTBox/mesh3D/mesh.py
+-rwxr-xr-x  2.0 unx     4598 b- defN 80-Jan-01 00:00 TPTBox/mesh3D/mesh_colors.py
 -rwxr-xr-x  2.0 unx      418 b- defN 80-Jan-01 00:00 TPTBox/registration/__init__.py
--rwxr-xr-x  2.0 unx    13413 b- defN 80-Jan-01 00:00 TPTBox/registration/_deepali/test.py
--rwxr-xr-x  2.0 unx     3715 b- defN 80-Jan-01 00:00 TPTBox/registration/ridged_intensity/register.py
+-rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 TPTBox/registration/ridged_intensity/__init__.py
+-rwxr-xr-x  2.0 unx     4105 b- defN 80-Jan-01 00:00 TPTBox/registration/ridged_intensity/register.py
 -rwxr-xr-x  2.0 unx      144 b- defN 80-Jan-01 00:00 TPTBox/registration/ridged_points/__init__.py
 -rwxr-xr-x  2.0 unx    11219 b- defN 80-Jan-01 00:00 TPTBox/registration/ridged_points/point_registration.py
--rwxr-xr-x  2.0 unx     7039 b- defN 80-Jan-01 00:00 TPTBox/registration/script_ax2sag.py
--rwxr-xr-x  2.0 unx    13839 b- defN 80-Jan-01 00:00 TPTBox/registration/script_ax2sag_v2.py
--rwxr-xr-x  2.0 unx     8053 b- defN 80-Jan-01 00:00 TPTBox/spine/POI_plotter.py
--rw-r--r--  2.0 unx      835 b- defN 80-Jan-01 00:00 TPTBox/spine/mesh3D/vert_mesh_colors.py
+-rwxr-xr-x  2.0 unx     7161 b- defN 80-Jan-01 00:00 TPTBox/registration/script_ax2sag.py
+-rw-r--r--  2.0 unx      851 b- defN 80-Jan-01 00:00 TPTBox/spine/mesh3D/vert_mesh_colors.py
 -rw-r--r--  2.0 unx       44 b- defN 80-Jan-01 00:00 TPTBox/spine/segmentation/__init__.py
 -rw-r--r--  2.0 unx     2125 b- defN 80-Jan-01 00:00 TPTBox/spine/segmentation/spineps.py
 -rwxr-xr-x  2.0 unx      221 b- defN 80-Jan-01 00:00 TPTBox/spine/snapshot2D/__init__.py
--rwxr-xr-x  2.0 unx    33768 b- defN 80-Jan-01 00:00 TPTBox/spine/snapshot2D/snapshot_modular.py
--rwxr-xr-x  2.0 unx    15619 b- defN 80-Jan-01 00:00 TPTBox/spine/snapshot2D/snapshot_templates.py
+-rwxr-xr-x  2.0 unx    32664 b- defN 80-Jan-01 00:00 TPTBox/spine/snapshot2D/snapshot_modular.py
+-rwxr-xr-x  2.0 unx    15647 b- defN 80-Jan-01 00:00 TPTBox/spine/snapshot2D/snapshot_templates.py
 -rwxr-xr-x  2.0 unx     2156 b- defN 80-Jan-01 00:00 TPTBox/spine/spinal_cord_segmentation/__count_segmented.py
 -rwxr-xr-x  2.0 unx        1 b- defN 80-Jan-01 00:00 TPTBox/spine/spinal_cord_segmentation/__init__.py
 -rwxr-xr-x  2.0 unx    20055 b- defN 80-Jan-01 00:00 TPTBox/spine/spinal_cord_segmentation/seg_spinalcordtoolbox.py
 -rw-r--r--  2.0 unx     2256 b- defN 80-Jan-01 00:00 TPTBox/stitching/README.md
 -rwxr-xr-x  2.0 unx       86 b- defN 80-Jan-01 00:00 TPTBox/stitching/__init__.py
--rwxr-xr-x  2.0 unx    11308 b- defN 80-Jan-01 00:00 TPTBox/stitching/__stitching_reg.py
--rwxr-xr-x  2.0 unx    10048 b- defN 80-Jan-01 00:00 TPTBox/stitching/__stitching_vertical.py
 -rw-r--r--  2.0 unx   326043 b- defN 80-Jan-01 00:00 TPTBox/stitching/stitching.jpg
 -rwxr-xr-x  2.0 unx    22293 b- defN 80-Jan-01 00:00 TPTBox/stitching/stitching.py
--rwxr-xr-x  2.0 unx     4427 b- defN 80-Jan-01 00:00 TPTBox/stitching/stitching_tools.py
+-rwxr-xr-x  2.0 unx     4469 b- defN 80-Jan-01 00:00 TPTBox/stitching/stitching_tools.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 TPTBox/tests/__init__.py
 -rw-r--r--  2.0 unx   357914 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_ct/sub-ct_label-22_ct.nii.gz
 -rw-r--r--  2.0 unx    14259 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_ct/sub-ct_seg-subreg_label-22_msk.nii.gz
 -rw-r--r--  2.0 unx    12844 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_ct/sub-ct_seg-vert_label-22_msk.nii.gz
 -rw-r--r--  2.0 unx   259999 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_mri/sub-mri_label-6_T2w.nii.gz
 -rw-r--r--  2.0 unx    12846 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_mri/sub-mri_seg-subreg_label-6_msk.nii.gz
 -rw-r--r--  2.0 unx    12144 b- defN 80-Jan-01 00:00 TPTBox/tests/sample_mri/sub-mri_seg-vert_label-6_msk.nii.gz
--rwxr-xr-x  2.0 unx     1756 b- defN 80-Jan-01 00:00 TPTBox/tests/speedtest.py
--rw-r--r--  2.0 unx     2871 b- defN 80-Jan-01 00:00 TPTBox/tests/speedtest_cc3d.py
--rw-r--r--  2.0 unx     1045 b- defN 80-Jan-01 00:00 TPTBox/tests/speedtest_morphological.py
+-rwxr-xr-x  2.0 unx     1742 b- defN 80-Jan-01 00:00 TPTBox/tests/speedtest.py
+-rw-r--r--  2.0 unx     2887 b- defN 80-Jan-01 00:00 TPTBox/tests/speedtest_cc3d.py
 -rw-r--r--  2.0 unx    11959 b- defN 80-Jan-01 00:00 TPTBox/tests/test_utils.py
--rw-r--r--  2.0 unx    34523 b- defN 80-Jan-01 00:00 tptbox-0.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     4337 b- defN 80-Jan-01 00:00 tptbox-0.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 tptbox-0.1.1.dist-info/WHEEL
-?rw-r--r--  2.0 unx     5464 b- defN 16-Jan-01 00:00 tptbox-0.1.1.dist-info/RECORD
-60 files, 1656750 bytes uncompressed, 1133110 bytes compressed:  31.6%
+-rw-r--r--  2.0 unx    34523 b- defN 80-Jan-01 00:00 tptbox-0.1.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8944 b- defN 80-Jan-01 00:00 tptbox-0.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 tptbox-0.1.2.dist-info/WHEEL
+?rw-r--r--  2.0 unx     4837 b- defN 16-Jan-01 00:00 tptbox-0.1.2.dist-info/RECORD
+53 files, 1571308 bytes uncompressed, 1112264 bytes compressed:  29.2%
```

## zipnote {}

```diff
@@ -33,62 +33,50 @@
 
 Filename: TPTBox/core/vert_constants.py
 Comment: 
 
 Filename: TPTBox/core/vertebra_pois_non_centroids.py
 Comment: 
 
-Filename: TPTBox/docker/__init__.py
-Comment: 
-
-Filename: TPTBox/docker/docker.py
-Comment: 
-
-Filename: TPTBox/docker/docker_run.py
-Comment: 
-
 Filename: TPTBox/logger/__init__.py
 Comment: 
 
 Filename: TPTBox/logger/log_constants.py
 Comment: 
 
 Filename: TPTBox/logger/log_file.py
 Comment: 
 
+Filename: TPTBox/mesh3D/__init__.py
+Comment: 
+
 Filename: TPTBox/mesh3D/mesh.py
 Comment: 
 
 Filename: TPTBox/mesh3D/mesh_colors.py
 Comment: 
 
 Filename: TPTBox/registration/__init__.py
 Comment: 
 
-Filename: TPTBox/registration/_deepali/test.py
+Filename: TPTBox/registration/ridged_intensity/__init__.py
 Comment: 
 
 Filename: TPTBox/registration/ridged_intensity/register.py
 Comment: 
 
 Filename: TPTBox/registration/ridged_points/__init__.py
 Comment: 
 
 Filename: TPTBox/registration/ridged_points/point_registration.py
 Comment: 
 
 Filename: TPTBox/registration/script_ax2sag.py
 Comment: 
 
-Filename: TPTBox/registration/script_ax2sag_v2.py
-Comment: 
-
-Filename: TPTBox/spine/POI_plotter.py
-Comment: 
-
 Filename: TPTBox/spine/mesh3D/vert_mesh_colors.py
 Comment: 
 
 Filename: TPTBox/spine/segmentation/__init__.py
 Comment: 
 
 Filename: TPTBox/spine/segmentation/spineps.py
@@ -114,20 +102,14 @@
 
 Filename: TPTBox/stitching/README.md
 Comment: 
 
 Filename: TPTBox/stitching/__init__.py
 Comment: 
 
-Filename: TPTBox/stitching/__stitching_reg.py
-Comment: 
-
-Filename: TPTBox/stitching/__stitching_vertical.py
-Comment: 
-
 Filename: TPTBox/stitching/stitching.jpg
 Comment: 
 
 Filename: TPTBox/stitching/stitching.py
 Comment: 
 
 Filename: TPTBox/stitching/stitching_tools.py
@@ -156,26 +138,23 @@
 
 Filename: TPTBox/tests/speedtest.py
 Comment: 
 
 Filename: TPTBox/tests/speedtest_cc3d.py
 Comment: 
 
-Filename: TPTBox/tests/speedtest_morphological.py
-Comment: 
-
 Filename: TPTBox/tests/test_utils.py
 Comment: 
 
-Filename: tptbox-0.1.1.dist-info/LICENSE
+Filename: tptbox-0.1.2.dist-info/LICENSE
 Comment: 
 
-Filename: tptbox-0.1.1.dist-info/METADATA
+Filename: tptbox-0.1.2.dist-info/METADATA
 Comment: 
 
-Filename: tptbox-0.1.1.dist-info/WHEEL
+Filename: tptbox-0.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: tptbox-0.1.1.dist-info/RECORD
+Filename: tptbox-0.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## TPTBox/__init__.py

```diff
@@ -30,15 +30,12 @@
     calc_poi_labeled_buffered,
 )
 from TPTBox.core.poi import load_poi
 from TPTBox.core.poi import load_poi as load_centroids
 from TPTBox.core.poi_global import POI_Global
 from TPTBox.core.vert_constants import Location, Zooms, v_idx2name, v_idx_order, v_name2idx
 
-# segmentation
-from TPTBox.docker.docker import run_docker
-
 # Logger
 from TPTBox.logger import Log_Type, Logger, Logger_Interface, Print_Logger, String_Logger
 from TPTBox.logger.log_file import No_Logger
 
 Centroids = POI
```

## TPTBox/core/bids_constants.py

```diff
@@ -1,34 +1,42 @@
 ### Legal formats ###
+# https://flexikon.doccheck.com/de/MRT-Sequenz
 formats = [
     "ct",
     "dixon",
     "snp",
     "log",
     "msk",
     "ctd",
     "T2c",
     "T1c",
     "pd",
-    "mevibe",
-    "vibe",
+    "mevibe",  # Multi Echo Volume Interpolated Breathhold Examination
+    "vibe",  # Volume Interpolated Breathhold Examination
     "T1w",
-    "T2w",
+    "T2w",  # TSE/FSE
     "T2star",
-    "TIRM",
-    "STIR",
+    "TIRM",  # (Turbo-Inversion Recovery-Magnitude)
+    "STIR",  # (Short-Tau Inversion Recovery)
     "model",
     "poi",
     "uncertainty",
     "angles",
     "subvar",
     "T2haste",
     "logit",
     "flair",
-    "DWI",
+    "DWI",  # Diffusion weighted imaging
+    "IR",  # Inversion recovery
+    "SE",  # Spine ech
+    "SE-SAT",  # Spin-Echo fat saturation, Synonyme: FAT SAT, FRF-SE, SPIR, CHESS, SPECIAL)
+    "GRE",  # Gradient-Echo
+    "PWI",  # Perfusion weighted imaging
+    "DTI",  # Diffusion tensor imaging
+    "radiomics",
 ]
 formats_relaxed = [*formats, "t2", "t1", "t2c", "t1c", "cta", "mr", "snapshot", "t1dixon"]
 # Recommended writing style: T1c, T2c; This list is not official and can be extended.
 
 
 # Actual official final folder
 # func (task based and resting state functional MRI)
@@ -94,19 +102,20 @@
     "Run/ID": "run",
     # Single class segmentation
     "Label": "label",
     # Others (never used)
     "Split": "split",
     "Density": "den",
     "Description": "desc",
+    "nameconflict": "nameconflict",
 }
 entities_keys = {v: k for k, v in entities.items()}
 
 entity_alphanumeric = [entities[i] for i in ["Task", "Reconstruction Direction"]]
-entity_decimal = [entities[i] for i in ["Run/ID", "Echo", "Flip Angle", "Inversion Time"]]
+entity_decimal = [entities[i] for i in ["Run/ID", "Echo", "Flip Angle", "Inversion Time", "nameconflict"]]
 entity_format = [entities[i] for i in ["Corresponding Modality"]]
 entity_on_off = [entities[i] for i in ["Magnetization Transfer"]]
 entity_left_right = [entities[i] for i in ["Hemisphere"]]
 entity_parts = [entities[i] for i in ["Part"]]
 
 
 parents = ["sourcedata", "rawdata", "derivatives"]
```

## TPTBox/core/bids_files.py

```diff
@@ -21,17 +21,18 @@
     entity_left_right,
     entity_on_off,
     entity_parts,
     file_types,
     formats,
     formats_relaxed,
     sequence_naming_keys,
-    sequence_splitting_keys,
 )
 
+# ,
+
 file = Path(__file__).resolve()
 sys.path.append(str(file.parents[1]))
 
 
 ### TODO Not implemented ###
 # Subject/Session/Sequence LVL Meta data
 # Recursive Meta data
@@ -141,27 +142,38 @@
                 print(f'[!] "{s}" is not a valid key/value pair. Expected "KEY-VALUE" in {name}')
     return bids_format, dic, bids_key, file_type
 
 
 class BIDS_Global_info:
     def __init__(
         self,
-        datasets: Sequence[Path] | Sequence[str],
-        parents: Sequence[str] = ["rawdata", "derivatives"],
+        datasets: Sequence[Path] | Sequence[str] | str | Path,
+        parents: Sequence[str] | str = ["rawdata", "derivatives"],
         additional_key: Sequence[str] = ["sequ", "seg", "ovl"],
         verbose: bool = True,
         file_name_manipulation: typing.Callable[[str], str] | None = None,
+        sequence_splitting_keys: list[str] | None = None,
     ):
         """This Objects creates a datastructures reflecting BIDS-folders.
 
         Args:
             datasets (typing.List[str]): List of dataset paths
             parents (typing.List[str]): List of parents (like ["rawdata","sourcedata","derivatives"])
             additional_key (list, optional): Additional keys that are not in the default BIDS but should not raise a warning. Defaults to ["sequ", "seg", "ovl"].
         """
+        if sequence_splitting_keys is None:
+            from TPTBox.core.bids_constants import sequence_splitting_keys
+
+            self.sequence_splitting_keys = sequence_splitting_keys
+
+        self.sequence_splitting_keys = sequence_splitting_keys
+        if isinstance(datasets, Path | str):
+            datasets = [datasets]  # type: ignore
+        if isinstance(parents, str):
+            parents = [parents]
         assert isinstance(datasets, Sequence), "datasets is not a list"
         assert isinstance(parents, Sequence), "parents is not a list"
         self.__bids_list: dict = {}
 
         self.file_name_manipulation = file_name_manipulation
         # Validate
         for ds in datasets:
@@ -194,23 +206,14 @@
         self.entities_keys = entities_keys
 
     def search_folder(self, path: Path, ds) -> None:
         for w in path.rglob("*"):
             if w.is_file():
                 self.add_file_2_subject(w, ds)
 
-    @classmethod
-    def add_splitting_key(cls, key):
-        sequence_splitting_keys.append(key)
-
-    @classmethod
-    def remove_splitting_key(cls, key):
-        if key in sequence_splitting_keys:
-            sequence_splitting_keys.remove(key)
-
     def add_file_2_subject(self, bids: BIDS_FILE | Path, ds=None) -> None:
         if isinstance(bids, Path) and "DS_Store" in bids.name:
             return
         if ds is None:
             if isinstance(bids, BIDS_FILE):
                 ds = bids.dataset
             else:
@@ -222,23 +225,18 @@
                 print("[!] skip file with out a type declaration:", bids.name)
                 # raise e
                 return
 
             if bids_key in self._global_bids_list:
                 self._global_bids_list[bids_key].add_file(bids)
                 return
-            bids = BIDS_FILE(
-                bids,
-                ds,
-                verbose=self.verbose,
-                file_name_manipulation=self.file_name_manipulation,
-            )
+            bids = BIDS_FILE(bids, ds, verbose=self.verbose, file_name_manipulation=self.file_name_manipulation)
         subject = bids.info.get("sub", "unsorted")
         if subject not in self.subjects:
-            self.subjects[subject] = Subject_Container(subject)
+            self.subjects[subject] = Subject_Container(subject, self.sequence_splitting_keys)
         print("Found:", subject, "  total=", len(self.subjects), "    ", end="\r")
         self.subjects[subject].add(bids)
 
     def enumerate_subjects(self, sort=False) -> list[tuple[str, Subject_Container]]:
         # TODO Enumerate should put out numbers...
         if sort:
             return sorted(self.subjects.items())
@@ -257,21 +255,22 @@
 
     @property
     def _global_bids_list(self):
         return self.__bids_list
 
 
 class Subject_Container:
-    def __init__(self, name) -> None:
+    def __init__(self, name, sequence_splitting_keys: list[str]) -> None:
         self.name = name
         self.sequences: dict[str, list[BIDS_FILE]] = {}
+        self.sequence_splitting_keys = sequence_splitting_keys.copy()
 
     def get_sequence_name(self, bids: BIDS_FILE):
         key_values = []
-        for key in sequence_splitting_keys:
+        for key in self.sequence_splitting_keys:
             key_values.append(bids.info[key]) if key in bids.info else None
         key = str.join("_", key_values)
         # sequence_names are only unique in the same session
         # ses = bids.info.get("ses", None)
         append_id = ""
         # idx = 1
         # This code fixes that in different sessions/same patient the filename can reuse splitting-keys
@@ -285,15 +284,16 @@
         #    else:
         #        break
         return key + append_id
 
     def add(self, bids: BIDS_FILE) -> None:
         sequ = self.get_sequence_name(bids)
         self.sequences.setdefault(sequ, [])
-        self.sequences[sequ].append(bids)
+        if bids not in self.sequences[sequ]:
+            self.sequences[sequ].append(bids)
         bids.set_subject(self)
 
     def new_query(self, flatten=False) -> Searchquery:
         """Make a new search_query
 
         Args:
             flatten (bool, optional): If you look for single file set flatten to True,
@@ -349,15 +349,15 @@
                     key = out
 
             if key in out_dict:
                 s1 = out_dict[key]
                 s1.append(s)
             else:
                 out_dict[key] = [s]
-        return BIDS_Family(out_dict)
+        return BIDS_Family(out_dict, self.sequence_splitting_keys)
 
 
 class BIDS_FILE:
     def __init__(
         self,
         file: Path | str,
         dataset: Path | str,
@@ -420,27 +420,37 @@
 
     def exists(self):
         if "nii.gz" in self.file:
             return self.file["nii.gz"].exists()
         else:
             return self.file[next(iter(self.file.keys()))].exists()
 
+    def unlink(self, missing_ok=True):
+        for f in self.file.values():
+            f.unlink(missing_ok=missing_ok)
+
     def __lt__(self, other):
         return self.BIDS_key < other.BIDS_key
 
     def __len__(self):
         return 1
 
     def __getitem__(self, key):
         assert key == 0, key
         return self
 
     def __iter__(self):
         return iter((self,))
 
+    def __eq__(self, key):
+        if hasattr(key, "BIDS_key"):
+            return self.BIDS_key == key.BIDS_key
+        else:
+            return False
+
     def set_subject(self, sub: Subject_Container):
         self.subject = sub
 
     def set(self, key, value):
         validate_entities(key, value, f"..._{key}-{value}_...", self.verbose)
         self.info[key] = value
 
@@ -531,15 +541,15 @@
         info: dict | None = None,
         from_info=False,
         auto_add_run_id=False,
         additional_folder: str | None = None,
         dataset_path: str | None = None,
         make_parent=True,
         non_strict_mode: bool = False,
-    ):
+    ) -> Path:
         """
         Changes part of the path to generate new flies. The new parent will be derivatives as a default.
         Examples:
         subreg_path = ct_bids.get_changed_path(file_type="nii.gz",parent = "derivatives",info={"seg": "subreg"}, format="cdt")
 
         Args:
             file_type (str | None, optional): Override the file type, like nii.gz to json Defaults to "nii.gz".
@@ -1174,15 +1184,21 @@
         )
         if sort:
             l = sorted(l)  # type: ignore
         return l
 
 
 class BIDS_Family:
-    def __init__(self, family_data: dict[str, list[BIDS_FILE]]):
+    def __init__(self, family_data: dict[str, list[BIDS_FILE]], sequence_splitting_keys: list[str]):
+        k = []
+        for x in family_data.values():
+            for y in x:
+                assert y.BIDS_key not in k, family_data
+                k.append(y.BIDS_key)
+        self.sequence_splitting_keys = sequence_splitting_keys.copy()
         self.data_dict = family_data.copy()
         self.family_id = self.get_identifier()
 
     def __getitem__(self, item: str) -> list[BIDS_FILE]:
         return self.data_dict[item]
 
     def __setitem__(self, key, value):
@@ -1222,15 +1238,15 @@
         first_e = self.data_dict[next(iter(self.data_dict.keys()))][0]
         if "sub" not in first_e.info:
             print(f"family_id, no sub-key, got {first_e.info} and data_dict {list(self.data_dict.keys())}")
             identifier = "sub-404"
         else:
             identifier = "sub-" + first_e.info["sub"]
         for s in first_e.info.keys():
-            if s in sequence_splitting_keys:
+            if s in self.sequence_splitting_keys:
                 identifier += "_" + s + "-" + first_e.info[s]
         return identifier
 
     def items(self):
         return self.data_dict.items()
 
     def keys(self):
```

## TPTBox/core/nii_wrapper.py

```diff
@@ -1,14 +1,14 @@
 import traceback
 import warnings
 from collections.abc import Sequence
 from enum import Enum
 from math import ceil, floor
 from pathlib import Path
-from typing import TYPE_CHECKING, Literal, TypeVar, Union
+from typing import TYPE_CHECKING, Any, Literal, TypeVar, Union
 
 import nibabel as nib
 import nibabel.orientations as nio
 import nibabel.processing as nip
 import numpy as np
 from nibabel import Nifti1Header, Nifti1Image  # type: ignore
 from typing_extensions import Self
@@ -302,14 +302,17 @@
         self._unpack()
         return self._arr.copy() #type: ignore
     def _extract_affine(self, rm_key=()):
         out =  {"zoom":self.zoom,"origin": self.origin, "shape": self.shape, "rotation": self.rotation, "orientation":self.orientation}
         for k in rm_key:
             out.pop(k)
         return out
+    def get_empty_POI(self):
+        from TPTBox import POI
+        return POI({},orientation=self.orientation,zoom = self.zoom,shape=self.shape,rotation=self.rotation,origin=self.origin)
     def get_array(self) -> np.ndarray:
         if self.seg:
             return self.get_seg_array()
         self._unpack()
         return self._arr.copy()
     def set_array(self,arr:np.ndarray, inplace=False,verbose:vc.logging=False)-> Self:  # noqa: ARG002
         """Creates a NII where the array is replaces with the input array.
@@ -530,15 +533,15 @@
         return self.apply_crop(*args,**qargs)
 
     def apply_crop_slice_(self,*args,**qargs):
         import warnings
         warnings.warn("apply_crop_slice_ id deprecated use apply_crop_ instead",stacklevel=5) #TODO remove in version 1.0
         return self.apply_crop_(*args,**qargs)
 
-    def apply_crop(self,ex_slice:tuple[slice,slice,slice]|tuple[slice,...] , inplace=False):
+    def apply_crop(self,ex_slice:tuple[slice,slice,slice]|Sequence[slice] , inplace=False):
         """
         The apply_crop_slice function applies a given slice to reduce the Nifti image volume. If a list of slices is provided, it computes the minimum volume of all slices and applies it.
 
         Args:
             ex_slice (tuple[slice,slice,slice] | list[tuple[slice,slice,slice]]): A tuple or a list of tuples, where each tuple represents a slice for each axis (x, y, z).
             inplace (bool, optional): If True, it applies the slice to the original image and returns it. If False, it returns a new NII object with the sliced image.
         Returns:
@@ -546,15 +549,15 @@
         """        ''''''
         nii = self.nii.slicer[ex_slice] if ex_slice is not None else self.nii_abstract
         if inplace:
             self.nii = nii
             return self
         return self.copy(nii)
 
-    def apply_crop_(self,ex_slice:tuple[slice,slice,slice]|tuple[slice,...]):
+    def apply_crop_(self,ex_slice:tuple[slice,slice,slice]|Sequence[slice]):
         return self.apply_crop(ex_slice=ex_slice,inplace=True)
 
     def pad_to(self,target_shape:list[int]|tuple[int,int,int] | Self, mode="constant",crop=False,inplace = False):
         if isinstance(target_shape, NII):
             target_shape = target_shape.shape
         padding = []
         crop = []
@@ -605,29 +608,29 @@
         return self.rescale_and_reorient(axcodes_to=axcodes_to,voxel_spacing=voxel_spacing,c_val=c_val,mode=mode,verbose=verbose,inplace=True)
 
     def reorient_same_as(self, img_as: Nifti1Image | Self, verbose:vc.logging=False, inplace=False) -> Self:
         axcodes_to: Ax_Codes = nio.ornt2axcodes(nio.io_orientation(img_as.affine)) # type: ignore
         return self.reorient(axcodes_to=axcodes_to, verbose=verbose, inplace=inplace)
     def reorient_same_as_(self, img_as: Nifti1Image | Self, verbose:vc.logging=False) -> Self:
         return self.reorient_same_as(img_as=img_as,verbose=verbose,inplace=True)
-    def rescale(self, voxel_spacing=(1, 1, 1), c_val:float|None=None, verbose:vc.logging=False, inplace=False,mode='constant',aline_corners:bool=False):
+    def rescale(self, voxel_spacing=(1, 1, 1), c_val:float|None=None, verbose:vc.logging=False, inplace=False,mode='constant',align_corners:bool=False):
         """
         Rescales the NIfTI image to a new voxel spacing.
 
         Args:
             voxel_spacing (tuple[float, float, float]): The desired voxel spacing in millimeters (x, y, z). -1 is keep the voxel spacing.
                 Defaults to (1, 1, 1).
             c_val (float | None, optional): The padding value. Defaults to None, meaning that the padding value will be
                 inferred from the image data.
             verbose (bool, optional): Whether to print a message indicating that the image has been resampled. Defaults to
                 False.
             inplace (bool, optional): Whether to modify the current object or return a new one. Defaults to False.
             mode (str, optional): One of the supported modes by scipy.ndimage.interpolation (e.g., "constant", "nearest",
                 "reflect", "wrap"). See the documentation for more details. Defaults to "constant".
-            aline_corners (bool|default): If True or not set and seg==True. Aline corners for scaling. This prevents segmentation mask to shift in a direction.
+            align_corners (bool|default): If True or not set and seg==True. Aline corners for scaling. This prevents segmentation mask to shift in a direction.
         Returns:
             NII: A new NII object with the resampled image data.
         """
         if voxel_spacing in ((-1, -1, -1), self.zoom):
             log.print(f"Image already resampled to voxel size {self.zoom}",verbose=verbose)
             return self.copy() if inplace else self
 
@@ -641,25 +644,25 @@
         if voxel_spacing == self.zoom:
             return self.copy() if inplace else self
 
         # Calculate new shape
         new_shp = tuple(np.rint([shp[i] * zms[i] / voxel_spacing[i] for i in range(len(voxel_spacing))]).astype(int))
         new_aff = nib.affines.rescale_affine(aff, shp, voxel_spacing, new_shp)  # type: ignore
         new_aff[:3, 3] = nib.affines.apply_affine(aff, [0, 0, 0])# type: ignore
-        new_img = _resample_from_to(self, (new_shp, new_aff,voxel_spacing), order=order, mode=mode,aline_corners=aline_corners)
+        new_img = _resample_from_to(self, (new_shp, new_aff,voxel_spacing), order=order, mode=mode,align_corners=align_corners)
         log.print(f"Image resampled from {zms} to voxel size {voxel_spacing}",verbose=verbose)
         if inplace:
             self.nii = new_img
             return self
         return NII(new_img, self.seg,self.c_val)
 
     def rescale_(self, voxel_spacing=(1, 1, 1), c_val:float|None=None, verbose:vc.logging=False,mode='constant'):
         return self.rescale( voxel_spacing=voxel_spacing, c_val=c_val, verbose=verbose,mode=mode, inplace=True)
 
-    def resample_from_to(self, to_vox_map:Image_Reference|Proxy, mode='constant', c_val=None, inplace = False,verbose:vc.logging=True,aline_corners:bool=False):
+    def resample_from_to(self, to_vox_map:Image_Reference|Proxy, mode='constant', c_val=None, inplace = False,verbose:vc.logging=True,align_corners:bool=False):
         """self will be resampled in coordinate of given other image. Adheres to global space not to local pixel space
         Args:
             to_vox_map (Image_Reference|Proxy): If object, has attributes shape giving input voxel shape, and affine giving mapping of input voxels to output space. If length 2 sequence, elements are (shape, affine) with same meaning as above. The affine is a (4, 4) array-like.\n
             mode (str, optional): Points outside the boundaries of the input are filled according to the given mode ('constant', 'nearest', 'reflect' or 'wrap').Defaults to 'constant'.\n
             cval (float, optional): Value used for points outside the boundaries of the input if mode='constant'. Defaults to 0.0.\n
             aline_corners (bool|default): If True or not set and seg==True. Aline corners for scaling. This prevents segmentation mask to shift in a direction.
             inplace (bool, optional): Defaults to False.
@@ -668,22 +671,22 @@
             NII:
         """        ''''''
         c_val = self.get_c_val(c_val)
 
         mapping = to_nii_optional(to_vox_map,seg=self.seg,default=to_vox_map)
         assert mapping is not None
         log.print(f"resample_from_to: {self} to {mapping}",verbose=verbose)
-        nii = _resample_from_to(self, mapping,order=0 if self.seg else 3, mode=mode,aline_corners=aline_corners)
+        nii = _resample_from_to(self, mapping,order=0 if self.seg else 3, mode=mode,align_corners=align_corners)
         if inplace:
             self.nii = nii
             return self
         else:
             return NII(nii,self.seg,self.c_val)
-    def resample_from_to_(self, to_vox_map:Image_Reference|Proxy, mode='constant', c_val:float|None=None,verbose:logging=True):
-        return self.resample_from_to(to_vox_map,mode=mode,c_val=c_val,inplace=True,verbose=verbose)
+    def resample_from_to_(self, to_vox_map:Image_Reference|Proxy, mode='constant', c_val:float|None=None,verbose:logging=True,aline_corners=False):
+        return self.resample_from_to(to_vox_map,mode=mode,c_val=c_val,inplace=True,verbose=verbose,align_corners=aline_corners)
 
     def n4_bias_field_correction(
         self,
         threshold = 60,
         mask=None,
         shrink_factor=4,
         convergence=None,
@@ -901,15 +904,15 @@
         return NII(msk_e,seg=True,c_val=0)
 
     def dilate_msk_(self, mm:int = 5, labels: list[int] | None = None, connectivity: int=3, mask: Self | None = None, verbose:logging=True):
         return self.dilate_msk(mm=mm, labels=labels, connectivity=connectivity, mask=mask, inplace=True, verbose=verbose)
 
 
     def fill_holes(self, labels: int | list[int] | None = None, verbose:logging=True, inplace=False):
-        """Fills holes in segmentations
+        """Fills holes in segmentation
 
         Args:
             labels (int | list[int] | None, optional): Labels that the hole-filling should be applied to. If none, applies on all labels found in arr. Defaults to None.
             verbose: whether to print which labels have been filled
             inplace (bool): Whether to modify the current NIfTI image object in place or create a new object with the mapped labels.
                 Default is False.
 
@@ -1096,14 +1099,56 @@
                 out.set_data_dtype(np.int32)
         log.print(f"Save {file} as {out.get_data_dtype()}",verbose=verbose,ltype=vc.log_file.Log_Type.SAVE)
         nib.save(out, file) #type: ignore
     def __str__(self) -> str:
         return f"shp={self.shape}; ori={self.orientation}, zoom={tuple(np.around(self.zoom, decimals=2))}, seg={self.seg}" # type: ignore
     def __repr__(self)-> str:
         return self.__str__()
+    def __array__(self,dtype=None):
+            self._unpack()
+            if dtype is None:
+                return self._arr
+            else:
+                return self._arr.astype(dtype, copy=False)
+    def __array_wrap__(self, array):
+        if array.shape != self.shape:
+            raise SyntaxError(f"Function call induce a shape change of nii image. Before {self.shape} after {array.shape}.")
+        return self.set_array(array)
+    def __getitem__(self, key)-> Any:
+        if isinstance(key,Sequence):
+            if all(isinstance(k, slice) for k in key):
+                #if all(k.step is not None and k.step == 1 for k in key):
+                #    raise NotImplementedError(f"Slicing is not implemented. Attemted {key}")
+                if len(key)!= len(self.shape):
+                    raise ValueError(f"Number slices must have exact number of slices like in dimension. Attemted: {key} - Shape {self.shape}")
+                return self.apply_crop(key) # type: ignore
+            elif  all(isinstance(k, int) for k in key):
+                if len(key)!= len(self.shape):
+                    raise ValueError(f"Number ints must have exact number of slices like in dimension. Attemted: {key} - Shape {self.shape}")
+                self._unpack()
+                return self._arr.__getitem__(key)
+        else:
+            raise TypeError("Invalid argument type.")
+    def __setitem__(self, key,value):
+        self._arr[key] = value
+        #if isinstance(key,Sequence):
+        #    if all(isinstance(k, slice) for k in key):
+        #        #if all(k.step is not None and k.step == 1 for k in key):
+        #        #    raise NotImplementedError(f"Slicing is not implemented. Attemted {key}")
+        #        if len(key)!= len(self.shape):
+        #            raise ValueError(f"Number slices must have exact number of slices like in dimension. Attemted: {key} - Shape {self.shape}")
+        #        return self.apply_crop(key)
+        #    elif  all(isinstance(k, int) for k in key):
+        #        if len(key)!= len(self.shape):
+        #            raise ValueError(f"Number ints must have exact number of slices like in dimension. Attemted: {key} - Shape {self.shape}")
+        #        self._unpack()
+        #        self._arr[key] = value
+        #else:
+        #    raise TypeError("Invalid argument type.")
+
     @classmethod
     def suppress_dtype_change_printout_in_set_array(cls, value=True):
         global suppress_dtype_change_printout_in_set_array  # noqa: PLW0603
         suppress_dtype_change_printout_in_set_array = value
     def is_intersecting_vertical(self, b: Self, min_overlap=40) -> bool:
         '''
         Test if the image intersect in global space.
@@ -1159,15 +1204,15 @@
             assert label != 0, 'Zero label does not make sens. This is the background'
             seg_arr[seg_arr != label] = 0
             seg_arr[seg_arr == label] = 1
         if keep_label:
             seg_arr = seg_arr * self.get_seg_array()
         return self.set_array(seg_arr,inplace=inplace)
     def extract_label_(self,label:int|vc.Location|list[int]|list[vc.Location], keep_label=False):
-        self.extract_label(label,keep_label,inplace=True)
+        return self.extract_label(label,keep_label,inplace=True)
     def remove_labels(self,*label:int | list[int], inplace=False, verbose:logging=True):
         '''If this NII is a segmentation you can single out one label.'''
         assert label != 0, 'Zero label does not make sens.  This is the background'
         seg_arr = self.get_seg_array()
         for l in label:
             if isinstance(l, list):
                 for g in l:
@@ -1316,15 +1361,15 @@
 
 
 def _resample_from_to(
     from_img:NII,
     to_img:NII|tuple[SHAPE,AFFINE,Zooms],
     order=3,
     mode="constant",
-    aline_corners:bool|Sentinel=Sentinel()  # noqa: B008
+    align_corners:bool|Sentinel=Sentinel()  # noqa: B008
 ):
     import numpy.linalg as npl
     import scipy.ndimage as scipy_img
     from nibabel.affines import AffineError, to_matvec
     from nibabel.imageclasses import spatial_axes_first
 
     # This check requires `shape` attribute of image
@@ -1337,15 +1382,15 @@
         assert to_img.zoom is not None
         to_shape:SHAPE = to_img.shape
         to_affine:AFFINE = to_img.affine
         zoom_to = np.array(to_img.zoom)
     from_n_dim = len(from_img.shape)
     if from_n_dim < 3:
         raise AffineError("from_img must be at least 3D")
-    if (isinstance(aline_corners,Sentinel) and order == 0) or aline_corners:
+    if (isinstance(align_corners,Sentinel) and order == 0) or align_corners:
         # https://discuss.pytorch.org/t/what-we-should-use-align-corners-false/22663/6
         # https://discuss.pytorch.org/uploads/default/original/2X/6/6a242715685b8192f07c93a57a1d053b8add97bf.png
         # Simulate align_corner=True, by manipulating the affine
         # Updated to matrix:
         # make the output by one voxel larger
         # z_new = z * num_pixel/(num_pixel+1)
         to_affine_new = to_affine.copy()
```

## TPTBox/core/nii_wrapper_math.py

```diff
@@ -147,19 +147,19 @@
     def normalize_mri(self,min_out = 0, max_out = 1, quantile = 0.99, inplace=False):
         a  = self.clamp(min=0,inplace=inplace)
         return a.normalize(min_out = min_out, max_out = max_out,quantile = quantile,clamp_lower=0,inplace=inplace)
     def normalize_ct(self,min_out = 0, max_out = 1,inplace=False):
         arr = self.clamp(min=-1024,max=1024,inplace=inplace)
         return arr.normalize(min_out = min_out, max_out = max_out, inplace=inplace)
 
-    def sum(self,axis = None,keepdims=False,where = np._NoValue)->float:  # type: ignore
+    def sum(self,axis = None,keepdims=False,where = np._NoValue, **qargs)->float:  # type: ignore
         if hasattr(where,"get_array"):
             where=where.get_array().astype(bool)
 
-        return np.sum(self.get_array(),axis=axis,keepdims=keepdims,where=where)
+        return np.sum(self.get_array(),axis=axis,keepdims=keepdims,where=where,**qargs)
     def threshold(self,threshold=0.5, inplace=False):
         arr = self.get_array()
         arr2 = arr.copy()
         arr[arr2>=threshold] = 1
         arr[arr2<=threshold] = 0
         nii = self.set_array(arr,inplace,verbose=False)
         nii.seg =True
```

## TPTBox/core/poi.py

```diff
@@ -1549,15 +1549,15 @@
         - NaN values in the binary mask are ignored.
     """
     if isinstance(subreg_id, Location):
         subreg_id = subreg_id.value
     assert vert_id == -1 or subreg_id == -1, "first or second dimension must be fixed."
     msk_nii = to_nii(msk, seg=True)
     msk_data = msk_nii.get_seg_array()
-    axc: Ax_Codes = nio.aff2axcodes(msk.affine)  # type: ignore
+    axc: Ax_Codes = nio.aff2axcodes(msk_nii.affine)  # type: ignore
     if extend_to is None:
         ctd_list = POI_Descriptor()
     else:
         if not inplace:
             extend_to = extend_to.copy()
         ctd_list = extend_to.centroids
         extend_to.assert_affine(msk_nii)
```

## TPTBox/core/poi_abstract.py

```diff
@@ -290,22 +290,20 @@
     def apply_all(self, fun: Callable[[float, float, float], Coordinate], inplace=False):
         ctd = self._get_centroids()._apply_all(fun, inplace)
         if inplace:
             return self
         return self.copy(ctd)
 
     @property
-    def is_global(self) -> bool:
-        ...
+    def is_global(self) -> bool: ...
 
     def clone(self, **qargs):
         return self.copy(**qargs)
 
-    def copy(self, centroids: POI_Descriptor | None = None, **qargs) -> Self:
-        ...
+    def copy(self, centroids: POI_Descriptor | None = None, **qargs) -> Self: ...
 
     def map_labels(
         self,
         label_map_full: dict[tuple[int, int], tuple[int, int]] | None = None,
         label_map_region: MAPPING = None,
         label_map_subregion: MAPPING = None,
         verbose: logging = False,
@@ -394,14 +392,15 @@
         )
 
     def sort(self, inplace=True) -> Self:
         """Sort vertebra dictionary by sorting_list"""
         poi = self.centroids.sort(inplace=inplace)
 
         if inplace:
+            self.centroids = poi
             return self
         return self.copy(centroids=poi)
 
     def fit_spline(
         self, smoothness: int = 10, samples_per_poi=20, location: int | Location = Location.Vertebra_Corpus, vertebra=False
     ) -> tuple[np.ndarray, np.ndarray]:
         """
@@ -418,29 +417,28 @@
             tuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays:
                 - spline_points: A 2D NumPy array representing the points on the interpolated spline curve.
                 - spline_1st_derivative: A 2D NumPy array representing the first derivative of the spline curve.
                 shape: first dimension to select a cord, second dimension to select all X/Y/Z
         """
         if isinstance(location, Location):
             location = location.value
-        if location not in self.keys_subregion() or isinstance(location, (list, tuple)):
+        if location not in self.keys_subregion() and not isinstance(location, Sequence):
             raise ValueError(f"The location {location} is not computed in this POI class")
         # Extract subregion based on the provided location
         poi = self.extract_subregion(*location) if isinstance(location, Sequence) else self.extract_subregion(location)
-
         # If vertebra sorting is requested, perform it
         if vertebra:
             from TPTBox.core.poi import POI, VertebraCentroids
 
             if isinstance(poi, (VertebraCentroids, POI)):
                 poi = VertebraCentroids.from_pois(poi).sort()  # Set sorting
             else:
                 raise ValueError("global POI does not supprot Vertebra sorting")
         else:
-            poi = poi.sort()
+            poi = poi.sort(inplace=False)
 
         # Convert centroids to NumPy array for processing
         centroids_coords = np.asarray(list(poi.values()))
 
         # Calculate the number of sample points to generate
         num_sample_pts = len(centroids_coords) * samples_per_poi
```

## TPTBox/core/vert_constants.py

```diff
@@ -36,14 +36,15 @@
 
 class Sentinel:
     pass
 
 
 class Location(Enum):
     Unknown = 0
+    # S1 = 26  # SACRUM
     # Vertebral subregions
     Vertebra_Full = 40
     Arcus_Vertebrae = 41
     Spinosus_Process = 42
     Costal_Process_Left = 43
     Costal_Process_Right = 44
     Superior_Articular_Left = 45
```

## TPTBox/core/vertebra_pois_non_centroids.py

```diff
@@ -182,15 +182,15 @@
         plane_coords = plane_coords.astype(int)
         # create_subregion
         # 1 where the selected subreg is, else 0
         select = subreg_iso.get_array() * 0
         select[plane_coords[:, :, 0], plane_coords[:, :, 1], plane_coords[:, :, 2]] = 1
         out += target_labels * select * reg_label
         if fill_back is not None:
-            fill_back[select == 1] = reg_label
+            fill_back[np.logical_and(select == 1, fill_back == 0)] = reg_label
     if fill_back is not None and fill_back_nii is not None:
         subreg_sar = subreg_iso.set_array(fill_back).reorient(("S", "A", "R"))
         fill_back = subreg_sar.get_array()
         x_slice = np.ones_like(fill_back[0]) * np.max(fill_back) + 1
         for i in range(fill_back.shape[0]):
             curr_slice = fill_back[i]
             cond = np.where(curr_slice != 0)
@@ -626,15 +626,15 @@
                 arr_poi = arcus.copy() * 0
                 arr_poi[loc102[0], loc102[1], loc102[2]] = 1
                 loc102 = np.concatenate(np.where(arr_poi[plane_coords[:, :, 0], plane_coords[:, :, 1], plane_coords[:, :, 2]]))
                 loc125 = get_nearest_neighbor(loc102, plane_arcus, 1)  # 41
                 cords = plane_coords[loc125[0], loc125[1], :]
                 poi[vert_id, out_id] = tuple(x + y.start for x, y in zip(cords, bb, strict=False))
             except Exception:
-                print(vert_id, out_id, "missed its target. Skipped")
+                print(vert_id, out_id, "missed its target. Skipped", loc102.sum(), plane_arcus.sum(), np.unique(plane_arcus))
 
 
 def _compute_vert_corners_in_reference_frame(poi: POI, vert_id: int, plane_coords: np.ndarray, subregion: np.ndarray):
     to_reference_frame, _ = get_vert_direction_matrix(poi, vert_id)
     # plane_coords x,y,3
     pc = (
         plane_coords[subregion[plane_coords[:, :, 0], plane_coords[:, :, 1], plane_coords[:, :, 2]] != 0].swapaxes(-1, 0).reshape((3, -1))
@@ -839,15 +839,15 @@
             else:
                 raise NotImplementedError(location.value)
 
 
 def calc_center_spinal_cord(
     poi: POI,
     subreg: NII,
-    spline_subreg_point_id: Location = Location.Vertebra_Corpus,
+    spline_subreg_point_id: Location | list[Location] = Location.Vertebra_Corpus,
     source_subreg_point_id: Location = Location.Vertebra_Corpus,
     subreg_id=Location.Spinal_Canal,
     intersection_target: list[Location] | None = None,
     _fill_inplace: NII | None = None,
     add_dense=False,
 ) -> POI:
     """
@@ -889,18 +889,19 @@
     from TPTBox import calc_centroids
 
     if intersection_target is None:
         intersection_target = [Location.Spinal_Cord, Location.Spinal_Canal]
     assert _fill_inplace is None or subreg == _fill_inplace
     poi_iso = poi.rescale()
     if add_dense and (2, Location.Dens_axis) in poi_iso:
-        poi_iso[1, spline_subreg_point_id] = poi_iso[2, Location.Dens_axis]
-        poi_iso[1, source_subreg_point_id] = poi_iso[2, Location.Dens_axis]
+        sx = spline_subreg_point_id[0] if isinstance(spline_subreg_point_id, Sequence) else spline_subreg_point_id
+        poi_iso[1, sx] = poi_iso[2, Location.Dens_axis]
+        poi_iso[1, sx] = poi_iso[2, Location.Dens_axis]
     subreg_iso = subreg.rescale()
-    body_spline, body_spline_der = poi_iso.fit_spline(location=spline_subreg_point_id, vertebra=True)
+    body_spline, body_spline_der = poi_iso.fit_spline(location=spline_subreg_point_id, vertebra=False)
     target_labels = subreg_iso.extract_label(intersection_target).get_array()
     out = target_labels * 0
     fill_back = out.copy() if _fill_inplace is not None else None
     for reg_label, _, cords in poi_iso.extract_subregion(source_subreg_point_id).items():
         # calculate_normal_vector
         distances = np.sqrt(np.sum((body_spline - np.array(cords)) ** 2, -1))
         normal_vector = body_spline_der[np.argmin(distances)]
@@ -930,16 +931,15 @@
         # create_subregion
         # 1 where the selected subreg is, else 0
         select = subreg_iso.get_array() * 0
         select[plane_coords[:, :, 0], plane_coords[:, :, 1], plane_coords[:, :, 2]] = 1
         out += target_labels * select * reg_label
 
         if fill_back is not None:
-            fill_back[select == 1] = reg_label
-
+            fill_back[np.logical_and(select == 1, fill_back == 0)] = reg_label
     if fill_back is not None:
         assert _fill_inplace is not None
         subreg_iso = subreg_iso.set_array(fill_back).reorient(("S", "A", "R"))
         fill_back = subreg_iso.get_array()
         x_slice = np.ones_like(fill_back[0]) * np.max(fill_back) + 1
         for i in range(fill_back.shape[0]):
             curr_slice = fill_back[i]
```

## TPTBox/logger/log_file.py

```diff
@@ -3,14 +3,16 @@
 import time
 import traceback
 import warnings
 import weakref
 from pathlib import Path
 from typing import TYPE_CHECKING, Protocol
 
+import numpy as np
+
 from TPTBox.logger.log_constants import (
     Log_Type,
     _clean_all_color_from_text,
     _convert_seconds,
     color_log_text,
     datatype_to_string,
     format_time_short,
@@ -112,25 +114,21 @@
             _type_: _description_
         """
         indent: str = self._prefix_indentation_level()
         if self.prefix is not None:
             return indent + f"[{self.prefix}]"
         return indent + type2bcolors[ltype][1]
 
-    def _log(self, text: str, end: str = "\n", ltype=Log_Type.TEXT):
-        ...
+    def _log(self, text: str, end: str = "\n", ltype=Log_Type.TEXT): ...
 
-    def close(self):
-        ...
+    def close(self): ...
 
-    def flush(self):
-        ...
+    def flush(self): ...
 
-    def flush_sub_logger(self, sublogger: String_Logger, closed=False):
-        ...
+    def flush_sub_logger(self, sublogger: String_Logger, closed=False): ...
 
     def add_sub_logger(self, name: str, default_verbose: bool = False) -> String_Logger | No_Logger:
         """Creates a sub-logger that only logs to string. Will be appended in this loggers log file as sub-logger.
         Args:
             name: name of the sub-logger
             default_verbose: default_verbose attribute for the sub-logger
 
@@ -150,14 +148,68 @@
             return sub_logger
         else:
             return self  # type: ignore
 
     def print_error(self, **args):
         self.print(traceback.format_exc(), ltype=Log_Type.FAIL, **args)
 
+    logging_state = None
+
+    def log_statistic(self, key, value, key2=None, verbose=True, round_print: int | None = 5):
+        if self.logging_state is None:
+            self.logging_state = {}
+        if key not in self.logging_state:
+            self.logging_state[key] = {}
+        if key2 is None:
+            key2 = len(self.logging_state[key])
+
+        self.logging_state[key][key2] = value
+        if round_print is not None and isinstance(value, (float, np.floating)):
+            value = np.round(value, decimals=round_print)
+        self.on_neutral(f"{key2:10}: {key:17} = {value}", verbose=verbose)
+
+    def _print_by_logger(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.LOG, verbose=verbose, **qargs)
+
+    def print_statistic(self):
+        if self.logging_state is None:
+            self._print_by_logger("??? No Accumulated Statistics ???")
+            return
+        self._print_by_logger("############## Accumulated Statistics ##############")
+        self._print_by_logger(f"{'key':17} {'mean':8}{'std':9} {'median':8} {'  count':7}")
+        for k, v in self.logging_state.items():
+            values = np.array(list(v.values()))
+            mean = f"{np.mean(values):.3}"
+            std = f"{np.std(values):.3}"
+            median = f"{np.median(values):.3}"
+            count = len(values)
+            self._print_by_logger(f"{k:17} {mean:8}±{std:8} {median:8} {count:7}")
+        self._print_by_logger("####################################################")
+
+    def on_fail(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.FAIL, verbose=verbose, **qargs)
+
+    def on_save(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.SAVE, verbose=verbose, **qargs)
+
+    def on_debug(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.STRANGE, verbose=verbose, **qargs)
+
+    def on_ok(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.OK, verbose=verbose, **qargs)
+
+    def on_neutral(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.NEUTRAL, verbose=verbose, **qargs)
+
+    def on_warning(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.WARNING, verbose=verbose, **qargs)
+
+    def on_text(self, *text, end="\n", verbose: bool | None = None, **qargs):
+        self.print(*text, end=end, ltype=Log_Type.TEXT, verbose=verbose, **qargs)
+
 
 class Logger(Logger_Interface):
     """
     Defines a logger object, that automatically creates a logs folder and file in it. Logs logger.print() calls to this file.
     """
 
     def __init__(
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## TPTBox/mesh3D/mesh.py

```diff
@@ -39,16 +39,15 @@
             self.mesh.export_obj(filepath)
         else:
             raise NotImplementedError(f"save with mode {mode}")
         log.print(f"Saved mesh: {filepath}", Log_Type.SAVE, verbose=verbose)
 
     @classmethod
     def load(cls, filepath: str | Path):
-        filepath = str(filepath)
-        assert os.path.exists(filepath), f"loading mesh from {filepath}, filepath does not exist"
+        assert Path(filepath).exists(), f"loading mesh from {filepath}, filepath does not exist"
         reader = pv.get_reader(str(filepath))
         mesh = reader.read()
         return Mesh3D(mesh)
 
     def show(self):
         pv.start_xvfb()
         pl = pv.Plotter()
```

## TPTBox/mesh3D/mesh_colors.py

```diff
@@ -104,16 +104,14 @@
     ITK_48 = RGB_Color.init_list([112, 128, 144])
     ITK_49 = RGB_Color.init_list([34, 139, 34])
     ITK_50 = RGB_Color.init_list([248, 248, 255])
 
 
 _color_dict = {v: getattr(Mesh_Color_List, v) for v in vars(Mesh_Color_List) if not callable(v) and not v.startswith("__")}
 
-_color_mapping_by_label: dict[int, RGB_Color] = {
-    i: _color_dict[f"ITK_{i}"] if f"ITK_{i}" in _color_dict else Mesh_Color_List.BLACK for i in range(1, 150)
-}
+_color_mapping_by_label: dict[int, RGB_Color] = {i: _color_dict.get(f"ITK_{i}", Mesh_Color_List.BLACK) for i in range(1, 150)}
 
 _color_map_in_row = np.array([v.rgb for v in _color_mapping_by_label.values()])
 
 
 def get_color_by_label(label: int):
     return _color_mapping_by_label[label]
```

## TPTBox/registration/ridged_intensity/register.py

```diff
@@ -1,27 +1,35 @@
 import os
 import sys
 from pathlib import Path
 from typing import Literal
 
 import nibabel as nib
-import nipy.algorithms.registration as nipy_reg
-import numpy as np
-from nipy.algorithms.registration.affine import Affine
 
+from TPTBox import Print_Logger
+
+try:
+    import nipy.algorithms.registration as nipy_reg
+    import numpy as np
+    from nipy.algorithms.registration.affine import Affine
+except ModuleNotFoundError:
+    err = Print_Logger()
+    err.on_fail("This subscript needs nipy as an additonal package")
+    err.on_fail("Please install: pip install nipy")
+    sys.exit()
 from TPTBox import NII, Ax_Codes
 
 Similarity_Measures = Literal["slr", "mi", "pmi", "dpmi", "cc", "cr", "crl1"]
 Affine_Transforms = Literal["affine", "affine2d", "similarity", "similarity2d", "rigid", "rigid2d"]
 
 
 class HiddenPrints:
     def __enter__(self):
         self._original_stdout = sys.stdout
-        sys.stdout = open(os.devnull, "w")
+        sys.stdout = open(os.devnull, "w")  # noqa: SIM115
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         sys.stdout.close()
         sys.stdout = self._original_stdout
 
 
 def registrate_ants(moving: NII, fixed: NII, type_of_transform="DenseRigid", verbose=False, **qargs):
@@ -31,48 +39,52 @@
 
     warped_moving = mytx["warpedmovout"]
     print(mytx)
     return NII(ants.to_nibabel(warped_moving)), mytx["fwdtransforms"]
 
 
 def registrate_nipy(
-    moving: NII, fixed: NII, similarity: Similarity_Measures = "cc", optimizer: Affine_Transforms = "rigid", other_moving: list[NII] = None
+    moving: NII,
+    fixed: NII,
+    similarity: Similarity_Measures = "cc",
+    optimizer: Affine_Transforms = "rigid",
+    other_moving: list[NII] | None = None,
 ):
     if other_moving is None:
         other_moving = []
     hist_reg = nipy_reg.HistogramRegistration(fixed.nii, moving.nii, similarity=similarity)
     with HiddenPrints():
-        T: Affine = hist_reg.optimize(optimizer, iterations=100)
-    aligned_img = apply_registration_nipy(moving, fixed, T)
-    out_arr = [apply_registration_nipy(i, fixed, T) for i in other_moving]
+        transform: Affine = hist_reg.optimize(optimizer, iterations=100)
+    aligned_img = apply_registration_nipy(moving, fixed, transform)
+    out_arr = [apply_registration_nipy(i, fixed, transform) for i in other_moving]
     for out, other in zip(out_arr, other_moving, strict=False):
         out.seg = other.seg
-    return aligned_img, T, out_arr
+    return aligned_img, transform, out_arr
 
 
-def only_change_affine(nii: NII, T: Affine):
+def only_change_affine(nii: NII, transform: Affine):
     aff = nii.affine
-    Tv = T.as_affine()
-    Tv = np.dot(Tv, aff)
-    return NII(nib.nifti1.Nifti1Image(nii.get_array(), Tv), nii.seg)
+    t_affine = transform.as_affine()
+    t_affine = np.dot(t_affine, aff)
+    return NII(nib.nifti1.Nifti1Image(nii.get_array(), t_affine), nii.seg)
 
 
-def apply_registration_nipy(moving: NII, fixed: NII, T: Affine):
-    aligned_img = nipy_reg.resample(moving.nii, T, fixed.nii, interp_order=0 if moving.seg else 3)
+def apply_registration_nipy(moving: NII, fixed: NII, transform: Affine):
+    aligned_img = nipy_reg.resample(moving.nii, transform, fixed.nii, interp_order=0 if moving.seg else 3)
     aligned_img = fixed.set_array(aligned_img.get_data())
     aligned_img.seg = moving.seg
     return aligned_img
 
 
 def register_native_res(
     moving: NII,
     fixed: NII,
     similarity: Similarity_Measures = "cc",
     optimizer: Affine_Transforms = "rigid",
-    other_moving: list[NII] = None,
+    other_moving: list[NII] | None = None,
 ) -> tuple[NII, NII, Affine, list[NII]]:
     """register an image to an other, with its native resolution of moving. Uses Global coordinates.
 
     Args:
         moving (NII): _description_
         fixed (NII): _description_
         similarity (Similarity_Measures, optional): _description_. Defaults to "cc".
@@ -81,16 +93,16 @@
     Returns:
         (NII,NII): _description_
     """
     if other_moving is None:
         other_moving = []
     fixed_m_res = fixed.copy()
     fixed_m_res.resample_from_to_(moving)
-    aligned_img, T, out_arr = registrate_nipy(moving, fixed_m_res, similarity, optimizer, other_moving)
-    return aligned_img, fixed_m_res, T, out_arr
+    aligned_img, transform, out_arr = registrate_nipy(moving, fixed_m_res, similarity, optimizer, other_moving)
+    return aligned_img, fixed_m_res, transform, out_arr
 
 
 def crop_shared_(a: NII, b: NII):
     crop = a.compute_crop()
     crop = b.compute_crop(other_crop=crop)
     print(crop)
     a.apply_crop_(crop)
```

## TPTBox/registration/script_ax2sag.py

```diff
@@ -1,36 +1,35 @@
 """
 This script assumes that there are aligned Sagittal data and poorly aligned axial data.
 """
+
 import pickle
 from pathlib import Path
 
 import nibabel as nib
 
 from TPTBox import NII, BIDS_Global_info, No_Logger, to_nii
 from TPTBox.core.bids_files import Subject_Container
 from TPTBox.registration.ridged_intensity.register import only_change_affine, registrate_nipy
 from TPTBox.stitching import stitching
 
 
 def register_ax_and_stich_both(sub: Subject_Container, out_folder, buffer_path, verbose_stitching=True):
-    if Path(buffer_path).exists():
-        registration_buffer = pickle.load(open(buffer_path, "rb"))
-    else:
-        registration_buffer = {}
+    with open(buffer_path, "rb") as f:
+        registration_buffer = pickle.load(f) if Path(buffer_path).exists() else {}
     new_reg_buffer = {}
     q = sub.new_query(flatten=True)
     q.filter_non_existence("seg")
     q.filter_non_existence("lesions")
     sag_q = q.copy()
     sag_q.filter("acq", "sag")
-    sag_q.filter("chunk", lambda x: True)
+    sag_q.filter("chunk", lambda _: True)
     q.filter("acq", "ax")
     sag_files = list(sag_q.loop_list())
-    sessions = list(set([s.get("ses") for s in sag_files]))
+    sessions = list({s.get("ses") for s in sag_files})
     for session in sessions:
         try:
             sag_q2 = sag_q.copy()
             sag_q2.filter("ses", str(session))
             sag_files = list(sag_q2.loop_list())
             ## MAKE Sagital Stich
             out_sag = sag_files[0].get_changed_path(parent=out_folder, info={"sequ": "stitched", "chunk": None})
@@ -58,83 +57,84 @@
             # out_ax2 = ax_files[0].get_changed_path(parent=out_folder, info={"sequ": "stitchedxxx", "chunk": None})
             if not out_ax.exists():
                 reg_ax_files = []
                 stitched_sag = to_nii(out_sag)
                 for ax_f in ax_files:
                     ax_nii = to_nii(ax_f)
                     if ax_f.file["nii.gz"] in registration_buffer:
-                        T = registration_buffer[ax_f.file["nii.gz"]]
+                        transform = registration_buffer[ax_f.file["nii.gz"]]
                     else:
                         a = ax_nii.reorient().rescale() / ax_nii.max()
                         b = stitched_sag.resample_from_to(a) / stitched_sag.max()
                         if b.sum() == 0:
                             b = stitched_sag.resample_from_to(a)  # prevent error, when we do not intersect
-                        aligned_img, T, out_arr = registrate_nipy(a, b, similarity="cc", optimizer="rigid")
-                    new_reg_buffer[ax_f.file["nii.gz"]] = T
+                        aligned_img, transform, out_arr = registrate_nipy(a, b, similarity="cc", optimizer="rigid")
+                    new_reg_buffer[ax_f.file["nii.gz"]] = transform
 
-                    aligned_img = only_change_affine(ax_nii, T)
+                    aligned_img = only_change_affine(ax_nii, transform)
                     reg_ax_files.append(aligned_img)
 
                 stitching(*reg_ax_files, out=out_ax, bias_field=True, verbose_stitching=verbose_stitching)
             ## MAKE SPINAL-CORD
             if not out_ax_cord.exists():
                 reg_ax_files = []
                 for ax_f in ax_files:
                     seg = ax_f.get_sequence_files(key_addendum=["seg"])["T2w_seg-manual"][0]
                     ax_nii = to_nii(seg, seg=True)
                     ax_nii.seg = True
                     if ax_f.file["nii.gz"] in new_reg_buffer:
-                        T = new_reg_buffer[ax_f.file["nii.gz"]]
-                        aligned_img = only_change_affine(ax_nii, T)
+                        transform = new_reg_buffer[ax_f.file["nii.gz"]]
+                        aligned_img = only_change_affine(ax_nii, transform)
                     else:
-                        assert False, out_ax
+                        raise FileNotFoundError(out_ax)  # noqa: TRY301
                         aff = to_nii(ax_f).affine
                         aligned_img = NII(nib.nifti1.Nifti1Image(ax_nii.get_array(), aff), ax_nii.seg)
 
                     reg_ax_files.append(aligned_img)
 
                 stitching(*reg_ax_files, out=out_ax_cord)
             ## MAKE MS STICH
             if not out_ax_ms.exists():
                 reg_ax_files = []
                 for ax_f in ax_files:
                     seg = ax_f.get_sequence_files(key_addendum=["lesions"])["T2w_lesions-manual"][0]
                     ax_nii = to_nii(seg, seg=True)
                     ax_nii.seg = True
                     if ax_f.file["nii.gz"] in new_reg_buffer:
-                        T = new_reg_buffer[ax_f.file["nii.gz"]]
-                        aligned_img = only_change_affine(ax_nii, T)
+                        transform = new_reg_buffer[ax_f.file["nii.gz"]]
+                        aligned_img = only_change_affine(ax_nii, transform)
                     else:
-                        assert False, out_ax
+                        raise FileNotFoundError(out_ax)  # noqa: TRY301
                         aff = to_nii(ax_f).affine
                         aligned_img = NII(nib.nifti1.Nifti1Image(ax_nii.get_array(), aff), ax_nii.seg)
 
                     reg_ax_files.append(aligned_img)
 
                 stitching(*reg_ax_files, out=out_ax_ms)
 
         except Exception:
             # raise e
             No_Logger().print_error()
             try:
                 print(ax_files)
-            except:
+            except Exception:
                 pass
 
     return new_reg_buffer
 
 
 def save_registration_buffer(buffers: dict | list[dict], registration_buffer, buffer_path):
     if not isinstance(buffers, list):
         buffers = [buffers]
     assert buffers is not None
     for new_buffer in buffers:
         for key, value in new_buffer.items():
             registration_buffer[key] = value
-    pickle.dump(registration_buffer, open(buffer_path, "wb"))
+    with open(buffer_path, "wb") as x:
+        pickle.dump(registration_buffer, x)
 
 
 def run(root="/media/data/robert/datasets/dataset-McGinnes/", out_folder="rawdata_new", n_jobs=1, chunks=16):
     bgi = BIDS_Global_info([root], parents=["rawdata"])
     registration_buffer = {}
     buffer_path = Path(root, "registration_affines.pkl")
     if n_jobs == 1:
```

## TPTBox/spine/mesh3D/vert_mesh_colors.py

```diff
@@ -1,8 +1,8 @@
-import numpy as np
+import numpy as np  # noqa: INP001
 
 from TPTBox import v_idx2name
 from TPTBox.mesh3D.mesh_colors import Mesh_Color_List, _color_dict
 
 snap3d_vert_subregion_color_list = (
     Mesh_Color_List.DARKGRAY,
     Mesh_Color_List.BEIGE,
```

## TPTBox/spine/snapshot2D/snapshot_modular.py

```diff
@@ -1,14 +1,10 @@
 import copy
 import sys
 import warnings
-from pathlib import Path
-
-file = Path(__file__).resolve()
-sys.path.append(str(file.parents[1]))
 from dataclasses import dataclass, field
 from enum import Enum, auto
 from pathlib import Path
 from typing import Literal, Optional
 
 import matplotlib.pyplot as plt
 import numpy as np
@@ -150,28 +146,27 @@
     Returns:
         x_ctd: ctd x values sorted (nparray), y_cord: interpolated y coords (nparray), z_cord: interpolated z coords (nparray)
     """
     assert ctd_list is not None
 
     if 26 in ctd_list.centroids.keys_region() and cor_savgol_filter:
         warnings.warn(
-            "Sacrum centroid present with cor_savgol_filter might overshadow the sacrum in coronal view",
-            UserWarning,
+            "Sacrum centroid present with cor_savgol_filter might overshadow the sacrum in coronal view", UserWarning, stacklevel=4
         )
     # Sagittal and coronal projections of a curved plane defined by centroids
     # Note: Will assume IPL orientation!
     # if x-direction (=S/I) is not fully incremental, a straight, not an interpolated plane will be returned
     order = v_idx_order
     order += [i for i in range(256) if i not in v_idx_order]
-    ctd_list.sorting_list = v_idx_order
+    # ctd_list.sorting_list = v_idx_order
     ctd_list.round_(3)
 
     ctd_list.sort()
     if curve_location == Location.Vertebra_Corpus:
-        l = [v for k1, k2, v in ctd_list.items() if k2 == 0 or k2 == 50 or k2 == 40]
+        l = [v for k1, k2, v in ctd_list.items() if k2 in (0, 50, 40)]
     else:
         l = [v for k1, k2, v in ctd_list.items() if k2 == curve_location.value]
 
     # TODO make the selected subregion flexible.
     if len(l) <= 3:
         l = list(ctd_list.values())
     if len(l) <= 3:
@@ -180,29 +175,26 @@
     x_cur = l[0][0] - 1
     throw_idx = []
     for idx, c in enumerate(l.copy()):
         if c[0] > x_cur:
             x_cur = c[0]
         else:
             throw_idx.append(idx)
-    if len(l) - len(throw_idx) >= 3:
-        l = [i for idx, i in enumerate(l) if idx not in throw_idx]
-    else:
-        l = sorted(l, key=lambda x: x[0])
+    l = [i for idx, i in enumerate(l) if idx not in throw_idx] if len(l) - len(throw_idx) >= 3 else sorted(l, key=lambda x: x[0])
 
     ctd_arr = np.transpose(np.asarray(l))
     shp = img_data.shape
     x_ctd = np.rint(ctd_arr[0]).astype(int)
     y_ctd = np.rint(ctd_arr[1]).astype(int)
     z_ctd = np.rint(ctd_arr[2]).astype(int)
     # axl_plane = np.zeros((shp[1], shp[2]))
     try:
         f_sag = interp1d(x_ctd, y_ctd, kind="quadratic")
         f_cor = interp1d(x_ctd, z_ctd, kind="quadratic")
-    except:
+    except Exception:
         f_sag = interp1d(x_ctd, y_ctd, kind="linear")
         f_cor = interp1d(x_ctd, z_ctd, kind="linear")
         # print("quadratic", l, x_ctd, len(l), len(throw_idx))
         # exit()
     window_size = int((max(x_ctd) - min(x_ctd)) / 2)
     poly_order = 3
     if window_size % 2 == 0:
@@ -246,15 +238,15 @@
     ctd_list,
     thick_t: tuple[int, int] = (100, 300),
 ):
     shp = img_data.shape
     cor_plane = np.zeros((shp[0], shp[2]))
     sag_plane = np.zeros((shp[0], shp[1]))
     y_zoom = zms[1]  # 0.9 = 1px = 0.9 mm # 10cm = 112px
-    thick = thick_t + ()
+    thick = (*thick_t,)
 
     for x in range(shp[0] - 1):
         if x < min(x_ctd):  # higher
             y_ref = y_cord[0]
         elif x >= max(x_ctd):  # lower than sacrum
             y_ref = y_cord[-1]
         else:
@@ -269,35 +261,35 @@
         y_range_high = int(min(y_ref + thick[0], y_post_rel_to_border))  # sagittal right
         cor_cut = img_data[x, y_range_low:y_range_high, :]
 
         plane_bool = np.zeros_like(cor_cut).astype(bool)
         plane_bool[cor_cut > 0] = True
         sag = np.nansum(img_data[x, :, :], 1, where=img_data[x, :, :] > 0)
         sag_plane[x, :] = div0(sag, np.count_nonzero(img_data[x, :, :], 1), fill=0)
-        cor = np.nansum(cor_cut, 0, where=plane_bool == True)
+        cor = np.nansum(cor_cut, 0, where=plane_bool)
         cor_plane[x, :] = div0(cor, np.count_nonzero(plane_bool, 0), fill=0)
     return sag_plane, cor_plane, curve_projection_axial_fallback(img_data, x_ctd)
 
 
 def curve_projected_mip(
     img_data: np.ndarray,
     zms: tuple[float, float, float],
     x_ctd,
     y_cord,
-    ctd_list,
+    ctd_list,  # noqa: ARG001
     thick_t: tuple[int, int] = (100, 300),
     make_colored_depth: bool = False,
 ):
     shp = img_data.shape
     cor_plane = np.zeros((shp[0], shp[2]))
     cor_depth_plane = np.zeros((shp[0], shp[2]))
     sag_plane = np.zeros((shp[0], shp[1]))
     sag_depth_plane = np.zeros((shp[0], shp[1]))
     y_zoom = zms[1]  # 0.9 = 1px = 0.9 mm # 10cm = 112px
-    thick = thick_t + ()
+    thick = (*thick_t,)
 
     for x in range(shp[0] - 1):
         if x < min(x_ctd):  # higher
             y_ref = y_cord[0]
         elif x >= max(x_ctd):  # lower than sacrum
             y_ref = y_cord[-1]
         else:
@@ -307,69 +299,55 @@
         #    thick = (100, 50)
 
         # TODO set y_zoom for broken sample, see if it works
         try:
             thicke = [int(i // y_zoom) + int(i % y_zoom > 0) for i in thick]
         except Exception:
             print("thick infinity bug", y_zoom, thick_t, thick)
-            thicke = thick_t + ()
+            thicke = (*thick_t,)
         thick = thicke
         y_post_rel_to_border = y_ref + int(0.4 * (shp[1] - 1 - y_ref))  # one-third distance to border
         y_range_low = int(max(0, y_ref - thick[1]))  # sagittal left
         y_range_high = int(min(y_ref + thick[0], y_post_rel_to_border))  # sagittal right
         # print("range", y_range_low, y_range_high)
         cor_cut = img_data[x, y_range_low:y_range_high, :]
-        cut_shp = cor_cut.shape
 
         cor_plane[x, :] = np.max(cor_cut, axis=0)  # arr[x, mip_i, :]
         cor_depth_plane[x, :] = np.argmax(cor_cut, axis=0)
-        cor_max_depth = cut_shp[0]
         sag_plane[x, :] = np.max(img_data[x, :, :], axis=1)  # img_data[x, :, z_ref]
         sag_depth_plane[x, :] = np.argmax(img_data[x, :, :], axis=1)
-        sag_max_depth = shp[1]
 
     if make_colored_depth:
         cor_depth_plane = normalize_image(cor_depth_plane)
         sag_depth_plane = normalize_image(sag_depth_plane)
 
         cor_plane = normalize_image(cor_plane)
         sag_plane = normalize_image(sag_plane)
 
         cor_m_plane = np.sqrt(cor_plane) * cor_depth_plane  # sqrt
         sag_m_plane = np.sqrt(sag_plane) * sag_depth_plane
         cor_m_plane = normalize_image(cor_m_plane)
         sag_m_plane = normalize_image(sag_m_plane)
 
-        # print("cor shape", cor_plane.shape)
         # convert to color image
         cmap = plt.get_cmap("inferno")
-        # cmap2 = plt.get_cmap("viridis")
         cor_plane = cmap(cor_m_plane)[..., :3]
-        # cor_plane_c = cmap2(cor_plane)[..., :3]
-        # cor_depth_c = cmap(cor_depth_plane)[..., :3]
-        # cor_plane = (cor_depth_c + cor_plane_c) / 2
-
         sag_plane = cmap(sag_m_plane)[..., :3]
 
-        # cor_r = cor_plane * 2
-        # cor_b = cor_depth_plane
-        # cor_g = cor_m_plane
-        # cor_plane = np.stack([cor_r, cor_g, cor_b], axis=-1)
-
     return sag_plane, cor_plane, curve_projection_axial_fallback(img_data, x_ctd)
 
 
-def normalize_image(img, range: tuple[float, float] | None = None):
-    if range is None:
-        min = np.min(img)
-        max = np.max(img)
+def normalize_image(img, v_range: tuple[float, float] | None = None):
+    if v_range is None:
+        min_v = np.min(img)
+        max_v = np.max(img)
     else:
-        min = range[0]
-        max = range[1]
-    return (img - min) / (max - min)
+        min_v = v_range[0]
+        max_v = v_range[1]
+    return (img - min_v) / (max_v - min_v)
 
 
 def curve_projection_axial_fallback(img_data, x_ctd):
     # Axial
     center = x_ctd[len(x_ctd) // 2]
     center_up = x_ctd[max(0, len(x_ctd) // 2 - 1)]
     center_down = x_ctd[min(len(x_ctd) - 1, len(x_ctd) // 2 + 1)]
@@ -385,31 +363,31 @@
     except Exception as e:
         print(e)
         axl_plane = np.zeros((1, 1))
     return axl_plane
 
 
 def make_isotropic2d(arr2d: np.ndarray, zms2d, msk=False) -> np.ndarray:
-    if arr2d.dtype == np.float64 or arr2d.dtype == np.float16 or arr2d.dtype == np.float32:
+    if arr2d.dtype in (np.float64, np.float16, np.float32):
         arr2d = arr2d.astype(int)
-    xs = [x for x in range(arr2d.shape[0])]
-    ys = [y for y in range(arr2d.shape[1])]
+    xs = list(range(arr2d.shape[0]))
+    ys = list(range(arr2d.shape[1]))
     if msk:
         interpolator = RegularGridInterpolator((xs, ys), arr2d, method="nearest")
     else:
         interpolator = RegularGridInterpolator((xs, ys), arr2d, method="linear")
     new_shp = tuple(np.rint(np.multiply(arr2d.shape, zms2d)).astype(int))
     x_mm = np.linspace(0, arr2d.shape[0] - 1, num=new_shp[0])
     y_mm = np.linspace(0, arr2d.shape[1] - 1, num=new_shp[1])
     xx, yy = np.meshgrid(x_mm, y_mm)
     pts = np.vstack([xx.ravel(), yy.ravel()]).T
     try:
         lt = interpolator(pts)
     except Exception:
-        raise ValueError(f"Needs to be casted into a other type: arr2d {arr2d.dtype}")
+        raise ValueError(f"Needs to be casted into a other type: arr2d {arr2d.dtype}")  # noqa: B904
     img = np.reshape(lt, new_shp, order="F")
     return img
 
 
 def make_isotropic2dpluscolor(arr3d, zms2d, msk=False):
     # print(arr3d.shape)
     if arr3d.ndim == 2:
@@ -454,29 +432,17 @@
     cmap: ListedColormap = cm_itk,
     curve_location: Location = Location.Vertebra_Corpus,
 ):
     # requires v_dict = dictionary of mask labels
     for k1, k2, v in ctd.items():
         # print(k, v, (v[1] * zms[1], v[0] * zms[0]), zms)
         try:
-            axs.add_patch(
-                Circle(
-                    (v[1] * zms[1], v[0] * zms[0]),
-                    2,
-                    color=cmap((k1 - 1) % LABEL_MAX % cmap.N),
-                )
-            )
-            if not hide_centroid_labels:
-                if k2 == curve_location.value and k1 in poi_labelmap:
-                    axs.text(
-                        4,
-                        v[0] * zms[0],
-                        poi_labelmap[k1],
-                        fontdict={"color": cmap(k1 - 1), "weight": "bold"},
-                    )
+            axs.add_patch(Circle((v[1] * zms[1], v[0] * zms[0]), 2, color=cmap((k1 - 1) % LABEL_MAX % cmap.N)))
+            if not hide_centroid_labels and k2 == curve_location.value and k1 in poi_labelmap:
+                axs.text(4, v[0] * zms[0], poi_labelmap[k1], fontdict={"color": cmap(k1 - 1), "weight": "bold"})
         except Exception as e:
             print(e)
 
 
 def plot_cor_centroids(
     axs,
     ctd: POI,
@@ -485,29 +451,17 @@
     hide_centroid_labels: bool,
     cmap: ListedColormap = cm_itk,
     curve_location: Location = Location.Vertebra_Corpus,
 ):
     # requires v_dict = dictionary of mask labels
     for k1, k2, v in ctd.items():
         try:
-            axs.add_patch(
-                Circle(
-                    (v[2] * zms[2], v[0] * zms[0]),
-                    2,
-                    color=cmap((k1 - 1) % LABEL_MAX % cmap.N),
-                )
-            )
-            if not hide_centroid_labels:
-                if k2 == curve_location.value and k1 in poi_labelmap:
-                    axs.text(
-                        4,
-                        v[0] * zms[0],
-                        poi_labelmap[k1],
-                        fontdict={"color": cmap(k1 - 1), "weight": "bold"},
-                    )
+            axs.add_patch(Circle((v[2] * zms[2], v[0] * zms[0]), 2, color=cmap((k1 - 1) % LABEL_MAX % cmap.N)))
+            if not hide_centroid_labels and k2 == curve_location.value and k1 in poi_labelmap:
+                axs.text(4, v[0] * zms[0], poi_labelmap[k1], fontdict={"color": cmap(k1 - 1), "weight": "bold"})
         except Exception:
             pass
 
 
 def make_2d_slice(
     img: Image_Reference,
     ctd: POI,
@@ -516,16 +470,16 @@
     visualization_type: Visualization_Type,
     ctd_fallback: POI,
     cor_savgol_filter: bool = False,
     to_ax=("I", "P", "L"),
     curve_location: Location = Location.Vertebra_Corpus,
     rescale_to_iso: bool = True,
 ):
-    img = to_nii(img)
-    img_reo = img.reorient_(to_ax)
+    img_nii = to_nii(img)
+    img_reo = img_nii.reorient_(to_ax)
     ctd_reo = ctd.reorient_centroids_to(img_reo)
     img_data = img_reo.get_array()
 
     if visualization_type in [
         visualization_type.Slice,
         visualization_type.Maximum_Intensity,
         visualization_type.Maximum_Intensity_Colored_Depth,
@@ -569,15 +523,15 @@
             cor = make_isotropic2d(cor, (zms[0], zms[2]), msk=msk)
             axl = make_isotropic2d(axl, (zms[1], zms[2]), msk=msk)
         elif sag.ndim == 3:  # color also encoded
             sag = make_isotropic2dpluscolor(sag, (zms[0], zms[1]), msk=msk)
             cor = make_isotropic2dpluscolor(cor, (zms[0], zms[2]), msk=msk)
             axl = make_isotropic2dpluscolor(axl, (zms[1], zms[1]), msk=msk)
         else:
-            assert False, f"make_2d_slice: expected sag to be ndim 2 or 3, but got shape {sag.shape}"
+            raise ValueError(f"make_2d_slice: expected sag to be ndim 2 or 3, but got shape {sag.shape}")
 
     sag = sag.astype(float)
     cor = cor.astype(float)
     axl = axl.astype(float)
 
     if msk:
         sag[sag == 0] = np.nan
@@ -644,15 +598,15 @@
     ctd = load_centroids(ctd_bids)
     if len(ctd) > 2:  # handle case if empty centroid file given
         return ctd
     print("[!][snp] To few centroids", ctd)
     return None
 
 
-def create_snapshot(
+def create_snapshot(  # noqa: C901
     snp_path: str | Path | list[str | Path] | list[str] | list[Path],
     frames: list[Snapshot_Frame],
     crop=False,
     check=False,
     to_ax=("I", "P", "L"),
     dpi=96,
     verbose: bool = False,
@@ -675,15 +629,15 @@
 
     img_list = []
     frame_list = []
     frames = [f for f in frames if f is not None]
     for frame in frames:
         # PRE-PROCESSING
         img = to_nii(frame.image)
-        assert img != None
+        assert img is not None
         seg = to_nii_optional(frame.segmentation, seg=True)  # can be None
         ctd = copy.deepcopy(to_cdt(frame.centroids))
         if (crop or frame.crop_msk) and seg is not None:  # crop to segmentation
             ex_slice = seg.compute_crop()
             img = img.copy().apply_crop_(ex_slice)
             seg = seg.copy().apply_crop_(ex_slice)
             ctd = ctd.apply_crop(ex_slice).filter_points_inside_shape() if ctd is not None else None
@@ -716,15 +670,15 @@
             img_data[img_data < frame.image_threshold] = 0  # type: ignore
         if frame.denoise_threshold is not None:
             import torch
             from torch.nn.functional import avg_pool3d
 
             try:
                 t_arr = torch.from_numpy(img_data.copy()).unsqueeze(0).to(torch.float32)
-            except:
+            except Exception:
                 # can't handel uint16
                 t_arr = torch.from_numpy(img_data.astype(np.int32).copy()).unsqueeze(0).to(torch.float32)
 
             img_data_smoothed = (
                 avg_pool3d(
                     t_arr,
                     kernel_size=(3, 3, 3),
@@ -759,17 +713,17 @@
                 ctd_tmp = calc_centroids(seg)  # TODO BUFFER
             elif ctd is None:
                 ctd_tmp = calc_centroids(seg)  # TODO BUFFER
                 if frame.force_show_cdt:
                     ctd = ctd_tmp
             else:
                 ctd_tmp = ctd
-        except Exception as e:
+        except Exception:
             print("did not manage to calc ctd_tmp\n", frame)
-            raise e
+            raise
         try:
             sag_img, cor_img, axl_img = make_2d_slice(
                 img,
                 ctd_tmp,
                 zms,
                 msk=False,
                 visualization_type=frame.visualization_type,
@@ -790,17 +744,17 @@
                     to_ax=to_ax,
                     cor_savgol_filter=frame.cor_savgol_filter,
                     curve_location=frame.curve_location,
                     rescale_to_iso=frame.rescale_to_iso,
                 )
             else:
                 sag_seg, cor_seg, axl_seg = (None, None, None)
-        except Exception as e:
+        except Exception:
             print(frame)
-            raise e
+            raise
         # Conversion to 2D image done, now normalization
         try:
             max_sag = np.percentile(sag_img[sag_img != 0], 99)
         except Exception:
             max_sag = 1
         try:
             max_cor = np.percentile(cor_img[cor_img != 0], 99)
```

## TPTBox/spine/snapshot2D/snapshot_templates.py

```diff
@@ -1,10 +1,11 @@
 from pathlib import Path
 
 import numpy as np
+
 from TPTBox import BIDS_FILE, NII, POI, Image_Reference, POI_Reference
 from TPTBox.spine.snapshot2D.snapshot_modular import Snapshot_Frame, Visualization_Type, create_snapshot
 
 
 ##########################
 # Snapshot_Frame Templates
 ##########################
@@ -102,15 +103,15 @@
             crop_msk=True,
             cor_savgol_filter=False,
             # cmap="viridis",#ListedColormap(cmap),
         )
     ]
     if add_ctd is not None:
         for c in add_ctd:
-            frames.append(
+            frames.append(  # noqa: PERF401
                 Snapshot_Frame(
                     image=ct_ref,
                     segmentation=vert_msk,
                     centroids=c,
                     mode="CT",
                     sagittal=False,
                     coronal=True,
@@ -381,15 +382,15 @@
     #    "ITL_S": 142,
     #    "ITL_D": 144,
     # }
     poi_all = POI.load(subreg_ctd)
     sinister = {}
     median = {}
     dorsal = {}
-    all = {}
+    all_ = {}
     pll = {}
     fl = {}
     # isl = {}
     # itl = {}
     from TPTBox.core.vert_constants import Location, conversion_poi2text
 
     for k, k2, v in poi_all.items():
@@ -397,15 +398,15 @@
         if conversion_poi2text[s].endswith("_D"):
             dorsal[k] = v
         elif conversion_poi2text[s].endswith("_S"):
             sinister[k] = v
         else:
             median[k] = v
         if "ALL" in conversion_poi2text[s]:
-            all[k] = v
+            all_[k] = v
         elif "PLL" in conversion_poi2text[s]:
             pll[k] = v
         elif "FL" in conversion_poi2text[s]:
             fl[k] = v
 
     frames = [
         Snapshot_Frame(
@@ -428,15 +429,15 @@
             centroids=poi_all.copy(sinister),
             mode="CT",
             curve_location=Location.Ligament_Attachment_Point_Anterior_Longitudinal_Superior_Left,
         ),
         Snapshot_Frame(
             image=ct_nii,
             segmentation=vert_msk,
-            centroids=poi_all.copy(all),
+            centroids=poi_all.copy(all_),
             mode="CT",
             coronal=True,
             curve_location=Location.Ligament_Attachment_Point_Anterior_Longitudinal_Superior_Median,
         ),
         Snapshot_Frame(
             image=ct_nii,
             segmentation=vert_msk,
@@ -481,33 +482,33 @@
     # f = "/media/data/robert/datasets/dataset-poi/derivatives/WS_31/ses-20221024/sub-WS_31_ses-20221024_test-robert_seg-subreg_msk.nii.gz"
     # g = f.replace("nii.gz", "json")
     #
     # poi_snapshot(ct_file, f, g)
     from pathlib import Path
 
     def make_snap(file):
-        id = file.stem
-        if id != "63":
+        idx = file.stem
+        if idx != "63":
             return
         # print(id)
         try:
-            base_path = f"/media/data/robert/datasets/dataset-poi/derivatives/WS_{id}"
+            base_path = f"/media/data/robert/datasets/dataset-poi/derivatives/WS_{idx}"
 
             ct_file = BIDS_FILE(
                 next(Path(base_path).glob("ses-*/sub-WS_*_ses-*_seq-*_space-aligASL_ct.nii.gz")),
                 "/media/data/robert/datasets/dataset-poi/",
                 verbose=False,
             )
             f = next(Path(base_path).glob("ses-*/sub-WS_*_ses-*_seq-*_seg-subreg_space-aligASL_msk.nii.gz"))
-            g = next(Path(base_path).glob("ses-*/sub-WS_*_ses-*_seq-*_seg-subreg_space-aligASL_msk.nii.gz"))
+            # g = next(Path(base_path).glob("ses-*/sub-WS_*_ses-*_seq-*_seg-subreg_space-aligASL_msk.nii.gz"))
             poi_snapshot(
                 ct_file,
                 f,
                 file,
-                out_path=f"/media/data/robert/datasets/dataset-poi/snapshot/{id}.png",
+                out_path=f"/media/data/robert/datasets/dataset-poi/snapshot/{idx}.png",
             )
         except StopIteration:
-            print(id, "stopIteration")
+            print(idx, "stopIteration")
 
     from joblib import Parallel, delayed
 
     out = Parallel(n_jobs=16)(delayed(make_snap)(file) for file in Path("/media/data/robert/datasets/dataset-poi/poi/").iterdir())
```

## TPTBox/stitching/stitching_tools.py

```diff
@@ -40,17 +40,17 @@
     if chunk_info not in cut:
         logger.print("chunk_info must be in [HWS, BWS, LWS]")
     ori = nii.orientation
     return nii.reorient_().apply_crop_(cut[chunk_info]).reorient_(ori)
 
 
 def GNC_stitch_T2w(
-    HWS: Image_Reference,
-    BWS: Image_Reference,
-    LWS: Image_Reference,
+    HWS: Image_Reference,  # noqa: N803
+    BWS: Image_Reference,  # noqa: N803
+    LWS: Image_Reference,  # noqa: N803
     n4_after_stitch: bool = False,
     # cut={
     #    "HWS": (slice(None), slice(0, 400), slice(None)),
     #    "BWS": (slice(None), slice(80, 400), slice(None)),
     #    "LWS": (slice(None), slice(48, 448), slice(None)),
     # },
 ):
```

## TPTBox/tests/speedtest.py

```diff
@@ -2,27 +2,27 @@
 from copy import deepcopy
 from time import perf_counter
 
 import numpy as np
 from tqdm import tqdm
 
 
-def speed_test_input(input, functions: list[Callable], assert_equal_function: Callable | None = None, *args, **kwargs):
+def speed_test_input(inp, functions: list[Callable], assert_equal_function: Callable | None = None, *args, **kwargs):
     time_measures = {}
     outs = {}
     for f in functions:
         start = perf_counter()
-        input_copy = deepcopy(input)
+        input_copy = deepcopy(inp)
         out = f(input_copy, *args, **kwargs)
         time = perf_counter() - start
         outs[f.__name__] = out
         time_measures[f.__name__] = time
 
     if assert_equal_function is not None:
-        for oname, o in outs.items():
+        for o in outs.values():
             assertion = assert_equal_function(o, outs[functions[0].__name__])
             assert assertion, f"speed_test: nonequal results given the assert_equal_function, got {o, outs[functions[0].__name__]}"
     return time_measures
 
 
 def speed_test(
     get_input_func: Callable, functions: list[Callable], repeats: int = 20, assert_equal_function: Callable | None = None, *args, **kwargs
@@ -33,17 +33,17 @@
     first_input = get_input_func()
     for f in functions:
         input_copy = deepcopy(first_input)
         out = f(input_copy, *args, **kwargs)
         print(f.__name__, out)
 
     time_sums = {}
-    for i in tqdm(range(repeats)):
-        input = get_input_func()
-        time_measures = speed_test_input(input, functions=functions, assert_equal_function=assert_equal_function, *args, **kwargs)
+    for _ in tqdm(range(repeats)):
+        inp = get_input_func()
+        time_measures = speed_test_input(inp, *args, functions=functions, assert_equal_function=assert_equal_function, **kwargs)
         for k, v in time_measures.items():
             if k not in time_sums:
                 time_sums[k] = 0
             time_sums[k] += v
 
     for k, v in time_sums.items():
         print(k, "\t", round(v / repeats, ndigits=6), "+-", round(np.std(v), ndigits=6))
```

## TPTBox/tests/speedtest_cc3d.py

```diff
@@ -90,11 +90,11 @@
     print(np_unique(arr))
     print(np.unique(arr))
 
     speed_test(
         repeats=50,
         get_input_func=get_nii_array,
         functions=[np_unique, np.unique],
-        assert_equal_function=lambda x, y: True,  # np.all([x[i] == y[i] for i in range(len(x))]),
+        assert_equal_function=lambda x, y: True,  # np.all([x[i] == y[i] for i in range(len(x))]),  # noqa: ARG005
         # np.all([x[i] == y[i] for i in range(len(x))])
     )
     # print(time_measures)
```

## Comparing `tptbox-0.1.1.dist-info/LICENSE` & `tptbox-0.1.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tptbox-0.1.1.dist-info/RECORD` & `tptbox-0.1.2.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,60 +1,53 @@
-TPTBox/__init__.py,sha256=dLdT0MXmNt3zIUfknLbkamVa-x3vxLjUc2w4_IViJ90,1119
+TPTBox/__init__.py,sha256=bparkIqaW1Znn-hLRk9j-SCaJl3uMN9n4RTXPg1a-kg,1059
 TPTBox/core/__init__.py,sha256=iMKh_SsaaHczAmq_4Cy17ZWZ7oIpW1rdj0WeGfLOQ4g,619
-TPTBox/core/bids_constants.py,sha256=G18_6qk0F_eHv-fan30KmD9sWONvfeMWOgaVANaTd4I,3972
-TPTBox/core/bids_files.py,sha256=zWUe0OwfYee_M3z1V22OTKAicp8Jd4otvMEaEpeqSp4,55801
-TPTBox/core/nii_wrapper.py,sha256=mzw3mKDhW2vX1lGsUnR4_Tn0DtIsBlfb3GCgALWQSUo,64166
-TPTBox/core/nii_wrapper_math.py,sha256=Qb50sYDVuMPWui31C_hjZkbRrlZwUwpgVtyoyAGcoP8,9151
+TPTBox/core/bids_constants.py,sha256=-HW3StaFNh8zg8WVmf_wALkzvae7-pkGkzZ89s3rqn0,4561
+TPTBox/core/bids_files.py,sha256=h0iVchjKLqwgDRmyRfZdN_33Rnm2WUPoKw4nqhUF_j4,56726
+TPTBox/core/nii_wrapper.py,sha256=PqPZShWayCcZubBdhNoHvH5sxWFA03HminB5Q3sZTBo,66755
+TPTBox/core/nii_wrapper_math.py,sha256=s7dAg3EJUsFzbFyF97JbPoAlpq5BzGnOqBVpqfH5Ijc,9168
 TPTBox/core/np_utils.py,sha256=baioyRE36MGwsYb0Zx0WQQrRdxV6Uf3-wW6cr6vaVsQ,37507
-TPTBox/core/poi.py,sha256=WM8A_ksGBjubPqIi1w0OxeStmCkyQoYKvItL-3MHpmo,69550
-TPTBox/core/poi_abstract.py,sha256=skEEGqK4iYbpGKLYFvphvixU1409IdxHhQQxJa5mpms,28761
+TPTBox/core/poi.py,sha256=ASg-KUFRm_RfhIGz9JRHICv8N4vZcwFUAGtUGC8IBQI,69554
+TPTBox/core/poi_abstract.py,sha256=txDnlSnPAL-JdHIZsDGUi7Rph8OVzJ-cwaNQ8WCTnQ0,28790
 TPTBox/core/poi_global.py,sha256=rU08hHG0YpUTGYxj1X2d2idyp44LRjylcNU85zHNEmI,2907
 TPTBox/core/sitk_utils.py,sha256=WPsD8mW8Gjvrc0aFUByagiv8PSHVTmhZ-F_KoIHfisc,18197
-TPTBox/core/vert_constants.py,sha256=3NJskOIzrkdssBwha-_abBSir2rQjtSVzHigmWSa4QY,7861
-TPTBox/core/vertebra_pois_non_centroids.py,sha256=SExcejChqwtGcDDqZhGLbCkOkVJpvUOWabS0hmyX7YY,47636
-TPTBox/docker/__init__.py,sha256=ln-Xk0IkOyoYkfti6nZruiwhH7_bQmprrpLY5dUXCnE,207
-TPTBox/docker/docker.py,sha256=9f_zgvnHZTpJmjUfKWbl3mtyeRN5J63jyotNgLNbuKA,36551
-TPTBox/docker/docker_run.py,sha256=UQv-qpYFDnUU4VojxxozaKzQWovfEFbFGi1hNPbutCg,1392
+TPTBox/core/vert_constants.py,sha256=_V1RelUmvVzJCfAszWdpzQw76h9R1j5rZre3XRgdF5g,7885
+TPTBox/core/vertebra_pois_non_centroids.py,sha256=WJbdVZlbG7kOxhVvqSRouRG6oCBpw7SfASlKEnuBPLo,47849
 TPTBox/logger/__init__.py,sha256=JFjqzAZ6Tjq40hXMqXtHwSut-o3dBcYBtqhcQamMXZI,165
 TPTBox/logger/log_constants.py,sha256=W2A4KjOXV8m3Hi0gxZJN_nvEL1DRBiv-ljpAmY_NT5I,5063
-TPTBox/logger/log_file.py,sha256=1AeIHvgVvPxpF8Rn6iy0r6hSr5MmHPjTYVxbTLTe_QM,15482
-TPTBox/mesh3D/mesh.py,sha256=-p-piOao9RPFbbdn46UTYhtCnUqW2PeFCGsNilDG_wY,4836
-TPTBox/mesh3D/mesh_colors.py,sha256=PCO7pUf8tglbJ_l8sDx9CuhzpVFs2mQn-VcdJcwo3lk,4633
+TPTBox/logger/log_file.py,sha256=m8DbRpdtwO_sfCkPHT77J_2MUN9_N3VRVs4kSUyTmPE,18159
+TPTBox/mesh3D/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+TPTBox/mesh3D/mesh.py,sha256=jjs11uxXzIZMQZ_HhwPUkWqwBjz1sg1K9gGwUilkTKw,4802
+TPTBox/mesh3D/mesh_colors.py,sha256=EYgf4qZ2Z0MDInxmMhVuXk_-miJOKlOLfcYbISs0WOc,4598
 TPTBox/registration/__init__.py,sha256=rP0Cc84NU66ZDIHvsvTn1ygPCsMLL6o7njpPoZxA_R8,418
-TPTBox/registration/_deepali/test.py,sha256=o9jGhDJLHIK00DN_DpJDKVMFobJxbWEJppRS1EGE0xQ,13413
-TPTBox/registration/ridged_intensity/register.py,sha256=hE29j2moTI23pyZYbmZOWDzpmwzt6XW1rdOZREugJLM,3715
+TPTBox/registration/ridged_intensity/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+TPTBox/registration/ridged_intensity/register.py,sha256=eNnWrxjO-x1ryqMl9U_kjUG_Pa7aRlfIOdWzh7-l6Fo,4105
 TPTBox/registration/ridged_points/__init__.py,sha256=4H0bpP143SWiLRVX8lPpFqVgG4p6T6TZrlbiWGRyyS0,144
 TPTBox/registration/ridged_points/point_registration.py,sha256=0xof8xDxnuMnTpDXBg6knLVTYAYXd1jR0NOOTPN-mbw,11219
-TPTBox/registration/script_ax2sag.py,sha256=nXAKJHKA006Yau9G1qiVRJh3kPVIvdgUhBVnhm72S-o,7039
-TPTBox/registration/script_ax2sag_v2.py,sha256=tHBOdRV-_-Hca_j9gyn-U3xuW_hLXURVgNE4omzgaYw,13839
-TPTBox/spine/POI_plotter.py,sha256=5ICeKRfz-i4TBkRkY6GsfBZld48B35W81WdrDuunUrM,8053
-TPTBox/spine/mesh3D/vert_mesh_colors.py,sha256=MUK_xLz0rPA3ArkuvwNI9TFVrrzyJTDea8l4p-0h-uA,835
+TPTBox/registration/script_ax2sag.py,sha256=bakeKy6M7fFi6cjB8Fn9QF-pDPojzcwL2fnQ5UtXKUE,7161
+TPTBox/spine/mesh3D/vert_mesh_colors.py,sha256=WOCOxgZJ_zVD2IVNCbK8C5NXp10gc5iu-SRCFCm4Wjw,851
 TPTBox/spine/segmentation/__init__.py,sha256=zbNZqStPri6m1aBA68dg2Qytu7y7iQX-sL1moB8XDAQ,44
 TPTBox/spine/segmentation/spineps.py,sha256=oWx3D9c-2s5htSa1pIvQ4zg2bqF2VSXVUbbFQ0_PPE0,2125
 TPTBox/spine/snapshot2D/__init__.py,sha256=0Tdrhrs3Rb4fALXnbCdn3NdfpRXKYIIXfi9m0wtnY68,221
-TPTBox/spine/snapshot2D/snapshot_modular.py,sha256=c9yMs8iAGCo0ZAfgV0Lin5atGH6w74Sz9tHpdWpMuvo,33768
-TPTBox/spine/snapshot2D/snapshot_templates.py,sha256=AYO_7OjxZVecQysaYGS4lZx_sARdzOi1JdKGd0iEXvA,15619
+TPTBox/spine/snapshot2D/snapshot_modular.py,sha256=CjN3Gu-KWT8X0nz4AOCQ1CWXOX3k3tD0Ks8EvQJ-xXU,32664
+TPTBox/spine/snapshot2D/snapshot_templates.py,sha256=JHr0QyjI2D25Mp3IjSHuaMxIOCmNQYHXWhs7n3SZmV0,15647
 TPTBox/spine/spinal_cord_segmentation/__count_segmented.py,sha256=IFdZVCzX-n5JbpbE3aXzbDPd-pJcfL4OBp8fv1Y9s7Q,2156
 TPTBox/spine/spinal_cord_segmentation/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 TPTBox/spine/spinal_cord_segmentation/seg_spinalcordtoolbox.py,sha256=4mk7zqsQ_pj9IZ9X2_gZea0N3REgaFbXESigKvFh6do,20055
 TPTBox/stitching/README.md,sha256=jIi2TKigB029La2YeJAec878E7EelU7YCWDcYr-dWXo,2256
 TPTBox/stitching/__init__.py,sha256=bWSBRWS7LstJjZxZI1m4OTezXXthdz2zoT2krOa8YwE,86
-TPTBox/stitching/__stitching_reg.py,sha256=SMKt5bihGXMyOU7-ua1alKfn2FPbA92llckLBToQ11w,11308
-TPTBox/stitching/__stitching_vertical.py,sha256=SngTW9YO0YpjDVVTuQVAtYhc5SkjfmNzBFDPPGgwfBo,10048
 TPTBox/stitching/stitching.jpg,sha256=zUSjKVWNqJdlod-uQvqd_gqvq3ZoLkpgZQaS3TkiJ5Q,326043
 TPTBox/stitching/stitching.py,sha256=0Dw7xGQXlZBorDUFjX0aZg_FTi7lYFsKQOgcwXJBeqk,22293
-TPTBox/stitching/stitching_tools.py,sha256=Jt03jN6CW3wD90dCxLnERiP041IOhrC-uR2dU28FzEI,4427
+TPTBox/stitching/stitching_tools.py,sha256=z_20cNhbmDMoOGEKxd0f7m3hVuS8QfyW_blIZHlID-Q,4469
 TPTBox/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 TPTBox/tests/sample_ct/sub-ct_label-22_ct.nii.gz,sha256=UeCdpLAPiiLBe_xV9juMIdKp9vHT5ARBQ_KeEnRKvsk,357914
 TPTBox/tests/sample_ct/sub-ct_seg-subreg_label-22_msk.nii.gz,sha256=r7f9-XcKxGDnylC10rE66y0T5E0XlCSkh1AEmxd-k7U,14259
 TPTBox/tests/sample_ct/sub-ct_seg-vert_label-22_msk.nii.gz,sha256=5VKf-0INg75pxfwPusS2mQvKoGelwJOfx9cFxHUn6h0,12844
 TPTBox/tests/sample_mri/sub-mri_label-6_T2w.nii.gz,sha256=lhSVtCzfwC6vUOs_Rx1wV5O4KAL-2wX18CqlNnUoz0Q,259999
 TPTBox/tests/sample_mri/sub-mri_seg-subreg_label-6_msk.nii.gz,sha256=NMej3czue1ApUdluDMwL90mP8_nEenvfzvtRp-PKJ4U,12846
 TPTBox/tests/sample_mri/sub-mri_seg-vert_label-6_msk.nii.gz,sha256=FO5yej9WVRLdvwydjOjZzhJ6zEPw0hvHdxwvy9NDAjo,12144
-TPTBox/tests/speedtest.py,sha256=_HlkDuREjFCWuwyReKnYpLSLxw5XRXlogWQj2A11r_w,1756
-TPTBox/tests/speedtest_cc3d.py,sha256=NWPxZ8tu46LgxsJpceKBbKsSUhdzIHkQC9yHRy9S8CE,2871
-TPTBox/tests/speedtest_morphological.py,sha256=NdQWYwN6CZSgixYhXMuXFZfOcmn2DRzw0oGuw0eKUg4,1045
+TPTBox/tests/speedtest.py,sha256=W0-mvUyZ94_e1jmErAoPqB4T_OsN3FJcnrcR11UAlO4,1742
+TPTBox/tests/speedtest_cc3d.py,sha256=TwLN-oZBJiiB3A8eGEdm1aSG25mRelcca3vRRecDtnQ,2887
 TPTBox/tests/test_utils.py,sha256=OqGYNIt3eyq3uNzx5ltcGpFrTSnqeonRIP3NAEDs7q8,11959
-tptbox-0.1.1.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-tptbox-0.1.1.dist-info/METADATA,sha256=sFvib_RbmiSHuoEacklY2sby2xBVqi69E4p4-wtSSJg,4337
-tptbox-0.1.1.dist-info/WHEEL,sha256=sP946D7jFCHeNz5Iq4fL4Lu-PrWrFsgfLXbbkciIZwg,88
-tptbox-0.1.1.dist-info/RECORD,,
+tptbox-0.1.2.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+tptbox-0.1.2.dist-info/METADATA,sha256=BnKo-6pZLbhMlrK28nY7iDwSwdo6QNGVP7z0EIWlYoE,8944
+tptbox-0.1.2.dist-info/WHEEL,sha256=sP946D7jFCHeNz5Iq4fL4Lu-PrWrFsgfLXbbkciIZwg,88
+tptbox-0.1.2.dist-info/RECORD,,
```

